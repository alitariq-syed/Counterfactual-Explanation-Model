{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-02T05:46:51.355891Z",
     "iopub.status.busy": "2020-12-02T05:46:51.355195Z",
     "iopub.status.idle": "2020-12-02T05:46:51.364899Z",
     "shell.execute_reply": "2020-12-02T05:46:51.365480Z"
    },
    "papermill": {
     "duration": 0.034974,
     "end_time": "2020-12-02T05:46:51.365646",
     "exception": false,
     "start_time": "2020-12-02T05:46:51.330672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:46:51.397975Z",
     "iopub.status.busy": "2020-12-02T05:46:51.397178Z",
     "iopub.status.idle": "2020-12-02T05:46:52.035175Z",
     "shell.execute_reply": "2020-12-02T05:46:52.034672Z"
    },
    "papermill": {
     "duration": 0.655323,
     "end_time": "2020-12-02T05:46:52.035282",
     "exception": false,
     "start_time": "2020-12-02T05:46:51.379959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-02T05:46:52.083545Z",
     "iopub.status.busy": "2020-12-02T05:46:52.082652Z",
     "iopub.status.idle": "2020-12-02T05:47:04.319825Z",
     "shell.execute_reply": "2020-12-02T05:47:04.319261Z"
    },
    "papermill": {
     "duration": 12.270234,
     "end_time": "2020-12-02T05:47:04.319946",
     "exception": false,
     "start_time": "2020-12-02T05:46:52.049712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p '/kaggle/temp/'\n",
    "# #!tar -xvzf '../input/200-bird-species-with-11788-images/CUB_200_2011.tgz' --directory '/kaggle/temp/'\n",
    "# !tar -xzf '../input/200-bird-species-with-11788-images/CUB_200_2011.tgz' --directory '/kaggle/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:04.355204Z",
     "iopub.status.busy": "2020-12-02T05:47:04.354566Z",
     "iopub.status.idle": "2020-12-02T05:47:04.357672Z",
     "shell.execute_reply": "2020-12-02T05:47:04.358826Z"
    },
    "papermill": {
     "duration": 0.024228,
     "end_time": "2020-12-02T05:47:04.358969",
     "exception": false,
     "start_time": "2020-12-02T05:47:04.334741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for dirname, _, filenames in os.walk('./'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:04.399315Z",
     "iopub.status.busy": "2020-12-02T05:47:04.398623Z",
     "iopub.status.idle": "2020-12-02T05:47:11.930448Z",
     "shell.execute_reply": "2020-12-02T05:47:11.929454Z"
    },
    "papermill": {
     "duration": 7.556338,
     "end_time": "2020-12-02T05:47:11.930596",
     "exception": false,
     "start_time": "2020-12-02T05:47:04.374258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.1.0\n",
      "...GPU set_memory_growth successfully set...\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import tensorflow as tf\n",
    "print('TF version: ', tf.__version__)\n",
    "\n",
    "#%% \n",
    "\"\"\"fix for issue: cuDNN failed to initialize\"\"\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('...GPU set_memory_growth successfully set...')\n",
    "\n",
    "else:\n",
    "    print('...GPU set_memory_growth not set...')\n",
    "\n",
    "#%%\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten,Softmax, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "#from tf_explain_modified.core.grad_cam import GradCAM\n",
    "import datetime\n",
    "from tqdm import tqdm # to monitor progress\n",
    "import argparse\n",
    "import os, sys\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "# from models10 import MySubClassModel\n",
    "# from codes.compute_filter_importance import save_filter_importance, test_filter_importance,test_filter_importance_in_code_method, plot_filter_importance,check_top_filter_importance,save_filter_importance_batch, check_histogram_top_filter_result\n",
    "# from codes.load_cxr_dataset import create_cxr_dataframes, load_cxr_dataset\n",
    "# from codes.support_functions import print_filter_classes_1, print_filter_classes_2, save_interpretable_parameters\n",
    "# from codes.find_filter_class import find_filter_class\n",
    "# from codes.train_counterfactual_net import train_counterfactual_net\n",
    "# from codes.support_functions import get_heatmap_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:11.989291Z",
     "iopub.status.busy": "2020-12-02T05:47:11.981291Z",
     "iopub.status.idle": "2020-12-02T05:47:11.993816Z",
     "shell.execute_reply": "2020-12-02T05:47:11.994331Z"
    },
    "papermill": {
     "duration": 0.046688,
     "end_time": "2020-12-02T05:47:11.994513",
     "exception": false,
     "start_time": "2020-12-02T05:47:11.947825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path:  ./trained_weights/resnet50/CUB200/standard\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "KAGGLE = True\n",
    "parser = argparse.ArgumentParser(description='Interpretable CNN')\n",
    "parser.add_argument('--interpretable',default = False)\n",
    "parser.add_argument('--full_standard',default = True)## dont add extra cnn layer to be comparable with interpretable model. Make completely standalone model\n",
    "parser.add_argument('--train_using_builtin_fit_method',default = True)#for training base model easily\n",
    "\n",
    "parser.add_argument('--create_counterfactual_combined' ,default = False)## create CF model for a pretrained base model or train a new base model\n",
    "\n",
    "parser.add_argument('--train_counterfactual_net' ,default = False)## \n",
    "parser.add_argument('--resume_counterfactual_net' ,default = False)## \n",
    "parser.add_argument('--test_counterfactual_net' ,default = False)## \n",
    "parser.add_argument('--load_counterfactual_net',default = True)\n",
    "\n",
    "\n",
    "parser.add_argument('--resume', default =False) # load saved weights for base model\n",
    "parser.add_argument('--pretrained', default = False) # load self-pretrained model\n",
    "\n",
    "parser.add_argument('--find_filter_class', default = False) # load retrained model and assign class to each filter by check mean activation per filter per class\n",
    "\n",
    "parser.add_argument('--filter_modified_directly', default = True)\n",
    "parser.add_argument('--loss_compute', default = True)#False = forward only\n",
    "parser.add_argument('--high_capacity_model', default = True)#\n",
    "parser.add_argument('--fixed_classes', default = True)#idea 2: fine tune from forward only with fixed classes\n",
    "parser.add_argument('--fixed_classes_reduce_loss', default = True)#False = forward only masked with fixed filter class. issue: 100% training accuracy but 10% testing acc\n",
    "\n",
    "parser.add_argument('--test_filter_importance', default = False)#for testing idea 2\n",
    "parser.add_argument('--save_filter_importance', default = False)#for testing idea 2\n",
    "parser.add_argument('--analyze_filter_importance', default = False)#for testing idea 2\n",
    "parser.add_argument('--save_filter_fmap', default = False)#save filter fmap as well\n",
    "parser.add_argument('--save_top_layer', default = True)#save top layer filter data only\n",
    "\n",
    "parser.add_argument('--visualize_fmaps', default = False)\n",
    "\n",
    "#base model parameters\n",
    "parser.add_argument('--dataset',default = 'CUB200')#mnist, cifar10, CUB200, #cxr1000, #catsvsdogs, #VOC2010\n",
    "parser.add_argument('--save_directory',default = './trained_weights/')\n",
    "parser.add_argument('--train',default = True)\n",
    "parser.add_argument('--test', default = True)\n",
    "parser.add_argument('--model',default = 'resnet50/')#myCNN, VGG16,resnet50\n",
    "parser.add_argument('--imagenet_weights',default = True) #use imageNet pretrained VGG\n",
    "\n",
    "parser.add_argument('--filter_category_method',default = 'own_reduce_loss')   # paper --> similar to paper implementation---assign filter with categories during training by accumulating batch-wise max activations\n",
    "                                                                    # own_reduce_loss --> our idea - pre-assign filter categories during forward pass over all the data, based on pretrained weights and feature maps\n",
    "\n",
    "\n",
    "#parser.add_argument('--test',default = True)\n",
    " \n",
    "if KAGGLE: args = parser.parse_known_args()[0] \n",
    "else: args = parser.parse_args()\n",
    "\n",
    "if args.interpretable:\n",
    "    if args.filter_category_method=='paper':\n",
    "        print('filter category assignment --> paper method')\n",
    "    else:\n",
    "        print('filter category assignment --> our idea')\n",
    "    weights_path = args.save_directory+args.model+args.dataset+'/interpretable/filter_category_method_'+str(args.filter_category_method)\n",
    "    log_path  = './logs/'+args.model+args.dataset+'/interpretable/filter_category_method_'+str(args.filter_category_method)\n",
    "    filter_data_path = './create_training_data/'+args.model+args.dataset+'/interpretable/filter_category_method_'+str(args.filter_category_method)\n",
    "else:\n",
    "    weights_path = args.save_directory+args.model+args.dataset+'/standard'\n",
    "    log_path  = './logs/'+args.model+args.dataset+'/standard'\n",
    "    filter_data_path = './create_training_data/'+args.model+args.dataset+'/standard' #directory for saving filter importance training data\n",
    "    \n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)    \n",
    "print('save_path: ',weights_path)\n",
    "\n",
    "parser.add_argument('--save_path',default = weights_path)\n",
    "if KAGGLE: args = parser.parse_known_args()[0] \n",
    "else: args = parser.parse_args()\n",
    "\n",
    "if args.resume:\n",
    "    print(\"resuming training\")\n",
    "#print(args)\n",
    "\n",
    "if args.model == 'VGG16/':\n",
    "    from tensorflow.keras.applications.vgg16 import VGG16,decode_predictions, preprocess_input\n",
    "elif args.model == 'resnet50/':\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50, decode_predictions, preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "#(for jupyterbook issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:12.045274Z",
     "iopub.status.busy": "2020-12-02T05:47:12.033564Z",
     "iopub.status.idle": "2020-12-02T05:47:12.051929Z",
     "shell.execute_reply": "2020-12-02T05:47:12.051404Z"
    },
    "papermill": {
     "duration": 0.041573,
     "end_time": "2020-12-02T05:47:12.052019",
     "exception": false,
     "start_time": "2020-12-02T05:47:12.010446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUB200-2011 dataset\n",
      "using official split\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "batch_size = 32\n",
    "if args.dataset == 'mnist':\n",
    "    num_classes=10\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.expand_dims(x_train,-1)\n",
    "    x_test = np.expand_dims(x_test,-1)\n",
    "    input_shape = (batch_size,28,28,1)\n",
    "    label_map = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "elif args.dataset == 'cifar10':\n",
    "    num_classes=10\n",
    "    print('cifar-10 dataset')\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    input_shape = (batch_size,32,32,3)\n",
    "    label_map = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "elif args.dataset == 'CUB200':\n",
    "    print('CUB200-2011 dataset')\n",
    "    num_classes=200\n",
    "    input_shape = (batch_size,224,224,3)\n",
    "    official_split=True\n",
    "    lab_system=True\n",
    "    if KAGGLE:\n",
    "        if official_split:\n",
    "            print('official split not established for training on KAGGLE')\n",
    "            sys.exit()\n",
    "        else:\n",
    "            base_path = '/kaggle/temp/CUB_200_2011'\n",
    "    if lab_system:\n",
    "        base_path ='D:/Ali Tariq/CUB_200_2011'\n",
    "    else:\n",
    "        base_path = 'G:/CUB_200_2011/CUB_200_2011'\n",
    "    \n",
    "    if official_split:\n",
    "        data_dir =base_path+'/train_test_split/train/'\n",
    "        data_dir_test =base_path+'/train_test_split/test/'\n",
    "        label_map = np.loadtxt(fname=base_path + '/classes.txt',dtype='str')\n",
    "        label_map = label_map[:,1]\n",
    "        print('using official split')\n",
    "    else:\n",
    "        data_dir =base_path+'/images/'\n",
    "        label_map = np.loadtxt(fname=base_path + '/classes.txt',dtype='str')\n",
    "        label_map = label_map[:,1]\n",
    "\n",
    "elif args.dataset == 'cxr1000':\n",
    "    print('CXR-1000 dataset')\n",
    "    num_classes=15\n",
    "    input_shape = (batch_size,224,224,3)\n",
    "    label_map, train_df, test_df, valid_df = create_cxr_dataframes()\n",
    "    all_labels = label_map\n",
    "elif args.dataset == 'catsvsdogs':\n",
    "    print('catsvsdogs dataset')\n",
    "    num_classes=2\n",
    "    input_shape = (batch_size,224,224,3)\n",
    "    lab_system = False\n",
    "    if lab_system:\n",
    "        data_dir ='D:/Ali Tariq/catsvsdogs/train/'\n",
    "        data_dir_test ='D:/Ali Tariq/catsvsdogs/test/'    \n",
    "    else:\n",
    "        data_dir ='G:/catsvsdogs/train/'\n",
    "        data_dir_test ='G:/catsvsdogs/test/'\n",
    "\n",
    "    label_map = ['cat',  'dog']\n",
    "\n",
    "elif args.dataset == 'VOC2010':\n",
    "    print('VOC2010-animals dataset')\n",
    "    num_classes=6\n",
    "    input_shape = (batch_size,224,224,3)\n",
    "    lab_system = False\n",
    "    if lab_system:\n",
    "        data_dir ='D:/Ali Tariq/VOCdevkit/VOC_animals/'\n",
    "        #data_dir_test ='D:/Ali Tariq/catsvsdogs/test/'    \n",
    "    else:\n",
    "        #data_dir ='G:/VOCdevkit/VOC_animals_one/'\n",
    "        data_dir ='G:/VOCdevkit/VOC_animals/'\n",
    "        #data_dir_test ='G:/catsvsdogs/test/'\n",
    "\n",
    "    label_map = ['bird',  'cat', 'cow', 'dog', 'horse', 'sheep']#['cat']#\n",
    "\n",
    "else:\n",
    "    print('unknown dataset')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:12.090645Z",
     "iopub.status.busy": "2020-12-02T05:47:12.089989Z",
     "iopub.status.idle": "2020-12-02T05:47:12.095324Z",
     "shell.execute_reply": "2020-12-02T05:47:12.095802Z"
    },
    "papermill": {
     "duration": 0.027688,
     "end_time": "2020-12-02T05:47:12.095909",
     "exception": false,
     "start_time": "2020-12-02T05:47:12.068221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['001.Black_footed_Albatross', '002.Laysan_Albatross',\n",
       "       '003.Sooty_Albatross', '004.Groove_billed_Ani',\n",
       "       '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet',\n",
       "       '008.Rhinoceros_Auklet', '009.Brewer_Blackbird',\n",
       "       '010.Red_winged_Blackbird', '011.Rusty_Blackbird',\n",
       "       '012.Yellow_headed_Blackbird', '013.Bobolink',\n",
       "       '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting',\n",
       "       '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird',\n",
       "       '020.Yellow_breasted_Chat', '021.Eastern_Towhee',\n",
       "       '022.Chuck_will_Widow', '023.Brandt_Cormorant',\n",
       "       '024.Red_faced_Cormorant', '025.Pelagic_Cormorant',\n",
       "       '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper',\n",
       "       '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo',\n",
       "       '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo',\n",
       "       '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch',\n",
       "       '036.Northern_Flicker', '037.Acadian_Flycatcher',\n",
       "       '038.Great_Crested_Flycatcher', '039.Least_Flycatcher',\n",
       "       '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher',\n",
       "       '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher',\n",
       "       '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall',\n",
       "       '047.American_Goldfinch', '048.European_Goldfinch',\n",
       "       '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe',\n",
       "       '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak',\n",
       "       '055.Evening_Grosbeak', '056.Pine_Grosbeak',\n",
       "       '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot',\n",
       "       '059.California_Gull', '060.Glaucous_winged_Gull',\n",
       "       '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull',\n",
       "       '064.Ring_billed_Gull', '065.Slaty_backed_Gull',\n",
       "       '066.Western_Gull', '067.Anna_Hummingbird',\n",
       "       '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird',\n",
       "       '070.Green_Violetear', '071.Long_tailed_Jaeger',\n",
       "       '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay',\n",
       "       '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird',\n",
       "       '078.Gray_Kingbird', '079.Belted_Kingfisher',\n",
       "       '080.Green_Kingfisher', '081.Pied_Kingfisher',\n",
       "       '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher',\n",
       "       '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon',\n",
       "       '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser',\n",
       "       '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk',\n",
       "       '093.Clark_Nutcracker', '094.White_breasted_Nuthatch',\n",
       "       '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole',\n",
       "       '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican',\n",
       "       '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis',\n",
       "       '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin',\n",
       "       '107.Common_Raven', '108.White_necked_Raven',\n",
       "       '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike',\n",
       "       '112.Great_Grey_Shrike', '113.Baird_Sparrow',\n",
       "       '114.Black_throated_Sparrow', '115.Brewer_Sparrow',\n",
       "       '116.Chipping_Sparrow', '117.Clay_colored_Sparrow',\n",
       "       '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow',\n",
       "       '121.Grasshopper_Sparrow', '122.Harris_Sparrow',\n",
       "       '123.Henslow_Sparrow', '124.Le_Conte_Sparrow',\n",
       "       '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow',\n",
       "       '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow',\n",
       "       '130.Tree_Sparrow', '131.Vesper_Sparrow',\n",
       "       '132.White_crowned_Sparrow', '133.White_throated_Sparrow',\n",
       "       '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow',\n",
       "       '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager',\n",
       "       '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern',\n",
       "       '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern',\n",
       "       '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee',\n",
       "       '149.Brown_Thrasher', '150.Sage_Thrasher',\n",
       "       '151.Black_capped_Vireo', '152.Blue_headed_Vireo',\n",
       "       '153.Philadelphia_Vireo', '154.Red_eyed_Vireo',\n",
       "       '155.Warbling_Vireo', '156.White_eyed_Vireo',\n",
       "       '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler',\n",
       "       '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler',\n",
       "       '161.Blue_winged_Warbler', '162.Canada_Warbler',\n",
       "       '163.Cape_May_Warbler', '164.Cerulean_Warbler',\n",
       "       '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler',\n",
       "       '167.Hooded_Warbler', '168.Kentucky_Warbler',\n",
       "       '169.Magnolia_Warbler', '170.Mourning_Warbler',\n",
       "       '171.Myrtle_Warbler', '172.Nashville_Warbler',\n",
       "       '173.Orange_crowned_Warbler', '174.Palm_Warbler',\n",
       "       '175.Pine_Warbler', '176.Prairie_Warbler',\n",
       "       '177.Prothonotary_Warbler', '178.Swainson_Warbler',\n",
       "       '179.Tennessee_Warbler', '180.Wilson_Warbler',\n",
       "       '181.Worm_eating_Warbler', '182.Yellow_Warbler',\n",
       "       '183.Northern_Waterthrush', '184.Louisiana_Waterthrush',\n",
       "       '185.Bohemian_Waxwing', '186.Cedar_Waxwing',\n",
       "       '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker',\n",
       "       '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker',\n",
       "       '191.Red_headed_Woodpecker', '192.Downy_Woodpecker',\n",
       "       '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren',\n",
       "       '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren',\n",
       "       '199.Winter_Wren', '200.Common_Yellowthroat'], dtype='<U34')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:12.150292Z",
     "iopub.status.busy": "2020-12-02T05:47:12.142828Z",
     "iopub.status.idle": "2020-12-02T05:47:13.338468Z",
     "shell.execute_reply": "2020-12-02T05:47:13.339591Z"
    },
    "papermill": {
     "duration": 1.227013,
     "end_time": "2020-12-02T05:47:13.339775",
     "exception": false,
     "start_time": "2020-12-02T05:47:12.112762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using imagenet_weights\n",
      "Found 5400 images belonging to 200 classes.\n",
      "Found 594 images belonging to 200 classes.\n",
      "Found 5400 images belonging to 200 classes.\n",
      "Found 594 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "if args.imagenet_weights:\n",
    "    print('using imagenet_weights')\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "                                        #rescale = 1./255)\n",
    "\n",
    "        train_gen = imgDataGen.flow(x_train, y_train, batch_size = batch_size,shuffle= False)\n",
    "        test_gen  = imgDataGen.flow(x_test, y_test, batch_size = batch_size,shuffle= False)\n",
    "    else:\n",
    "        augment = False\n",
    "        if not augment:\n",
    "            imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input, \n",
    "                                            #rescale = 1./255,\n",
    "                                            validation_split=0.1)\n",
    "        else:\n",
    "            imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input, \n",
    "                                        #rescale = 1./255,\n",
    "                                        validation_split=0.1,\n",
    "                                        \n",
    "                              height_shift_range= 0.2, \n",
    "                              width_shift_range=0.2, \n",
    "                              rotation_range=15, \n",
    "                              shear_range = 0.2,\n",
    "                              fill_mode = 'nearest',#''nearest#reflect\n",
    "                              zoom_range=0.2)\n",
    "        \n",
    "        train_gen = imgDataGen.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                subset='training',\n",
    "                                interpolation='nearest')#,\n",
    "                                #all classes for base model; binary classes for CF model\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_map)\n",
    "        test_gen  = imgDataGen.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                seed=None,\n",
    "                                subset='validation',\n",
    "                                interpolation='nearest')#,\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                               # classes = label_map)\n",
    "        \n",
    "        # for visualization, dont use preprocessed image\n",
    "        imgDataGen_nopreprocess = ImageDataGenerator(#preprocessing_function = preprocess_input, \n",
    "                                        rescale = 1./255,\n",
    "                                        validation_split=0.1)\n",
    "        \n",
    "        train_gen_nopreprocess = imgDataGen_nopreprocess.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                subset='training',\n",
    "                                interpolation='nearest'),\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_map)\n",
    "        test_gen_nopreprocess  = imgDataGen_nopreprocess.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                seed=None,\n",
    "                                subset='validation',\n",
    "                                interpolation='nearest'),\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_\n",
    "        if args.dataset == 'CUB200' and official_split:\n",
    "            #actual unseen test set\n",
    "            imgDataGen_official_split = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "            actual_test_gen  = imgDataGen_official_split.flow_from_directory(data_dir_test,\n",
    "                            target_size=(input_shape[1], input_shape[2]),\n",
    "                            color_mode='rgb',\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=None,\n",
    "                            #subset='validation',\n",
    "                            interpolation='nearest')\n",
    "            imgDataGen_official_split_nopreprocess = ImageDataGenerator(rescale = 1./255)\n",
    "            actual_test_gen_nopreprocess  = imgDataGen_official_split_nopreprocess.flow_from_directory(data_dir_test,\n",
    "                            target_size=(input_shape[1], input_shape[2]),\n",
    "                            color_mode='rgb',\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=None,\n",
    "                            #subset='validation',\n",
    "                            interpolation='nearest')\n",
    "elif args.dataset == 'cxr1000':\n",
    "    train_gen, test_gen, valid_gen = load_cxr_dataset(train_df, test_df, valid_df, all_labels, batch_size)\n",
    "    \n",
    "else:\n",
    "    print('not using imagenet_weights')\n",
    "\n",
    "    imgDataGen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    train_gen = imgDataGen.flow(x_train, y_train, batch_size = batch_size,shuffle= False)\n",
    "    test_gen  = imgDataGen.flow(x_test, y_test, batch_size = batch_size,shuffle= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:13.405065Z",
     "iopub.status.busy": "2020-12-02T05:47:13.404226Z",
     "iopub.status.idle": "2020-12-02T05:47:17.109487Z",
     "shell.execute_reply": "2020-12-02T05:47:17.109938Z"
    },
    "papermill": {
     "duration": 3.742196,
     "end_time": "2020-12-02T05:47:17.110077",
     "exception": false,
     "start_time": "2020-12-02T05:47:13.367881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VGG model\n",
      "using resnet50 imagenet weights for CUB200 dataset\n",
      "input_7\n",
      "conv1_pad\n",
      "conv1_conv\n",
      "conv1_bn\n",
      "conv1_relu\n",
      "pool1_pad\n",
      "pool1_pool\n",
      "conv2_block1_1_conv\n",
      "conv2_block1_1_bn\n",
      "conv2_block1_1_relu\n",
      "conv2_block1_2_conv\n",
      "conv2_block1_2_bn\n",
      "conv2_block1_2_relu\n",
      "conv2_block1_0_conv\n",
      "conv2_block1_3_conv\n",
      "conv2_block1_0_bn\n",
      "conv2_block1_3_bn\n",
      "conv2_block1_add\n",
      "conv2_block1_out\n",
      "conv2_block2_1_conv\n",
      "conv2_block2_1_bn\n",
      "conv2_block2_1_relu\n",
      "conv2_block2_2_conv\n",
      "conv2_block2_2_bn\n",
      "conv2_block2_2_relu\n",
      "conv2_block2_3_conv\n",
      "conv2_block2_3_bn\n",
      "conv2_block2_add\n",
      "conv2_block2_out\n",
      "conv2_block3_1_conv\n",
      "conv2_block3_1_bn\n",
      "conv2_block3_1_relu\n",
      "conv2_block3_2_conv\n",
      "conv2_block3_2_bn\n",
      "conv2_block3_2_relu\n",
      "conv2_block3_3_conv\n",
      "conv2_block3_3_bn\n",
      "conv2_block3_add\n",
      "conv2_block3_out\n",
      "conv3_block1_1_conv\n",
      "conv3_block1_1_bn\n",
      "conv3_block1_1_relu\n",
      "conv3_block1_2_conv\n",
      "conv3_block1_2_bn\n",
      "conv3_block1_2_relu\n",
      "conv3_block1_0_conv\n",
      "conv3_block1_3_conv\n",
      "conv3_block1_0_bn\n",
      "conv3_block1_3_bn\n",
      "conv3_block1_add\n",
      "conv3_block1_out\n",
      "conv3_block2_1_conv\n",
      "conv3_block2_1_bn\n",
      "conv3_block2_1_relu\n",
      "conv3_block2_2_conv\n",
      "conv3_block2_2_bn\n",
      "conv3_block2_2_relu\n",
      "conv3_block2_3_conv\n",
      "conv3_block2_3_bn\n",
      "conv3_block2_add\n",
      "conv3_block2_out\n",
      "conv3_block3_1_conv\n",
      "conv3_block3_1_bn\n",
      "conv3_block3_1_relu\n",
      "conv3_block3_2_conv\n",
      "conv3_block3_2_bn\n",
      "conv3_block3_2_relu\n",
      "conv3_block3_3_conv\n",
      "conv3_block3_3_bn\n",
      "conv3_block3_add\n",
      "conv3_block3_out\n",
      "conv3_block4_1_conv\n",
      "conv3_block4_1_bn\n",
      "conv3_block4_1_relu\n",
      "conv3_block4_2_conv\n",
      "conv3_block4_2_bn\n",
      "conv3_block4_2_relu\n",
      "conv3_block4_3_conv\n",
      "conv3_block4_3_bn\n",
      "conv3_block4_add\n",
      "conv3_block4_out\n",
      "conv4_block1_1_conv\n",
      "conv4_block1_1_bn\n",
      "conv4_block1_1_relu\n",
      "conv4_block1_2_conv\n",
      "conv4_block1_2_bn\n",
      "conv4_block1_2_relu\n",
      "conv4_block1_0_conv\n",
      "conv4_block1_3_conv\n",
      "conv4_block1_0_bn\n",
      "conv4_block1_3_bn\n",
      "conv4_block1_add\n",
      "conv4_block1_out\n",
      "conv4_block2_1_conv\n",
      "conv4_block2_1_bn\n",
      "conv4_block2_1_relu\n",
      "conv4_block2_2_conv\n",
      "conv4_block2_2_bn\n",
      "conv4_block2_2_relu\n",
      "conv4_block2_3_conv\n",
      "conv4_block2_3_bn\n",
      "conv4_block2_add\n",
      "conv4_block2_out\n",
      "conv4_block3_1_conv\n",
      "conv4_block3_1_bn\n",
      "conv4_block3_1_relu\n",
      "conv4_block3_2_conv\n",
      "conv4_block3_2_bn\n",
      "conv4_block3_2_relu\n",
      "conv4_block3_3_conv\n",
      "conv4_block3_3_bn\n",
      "conv4_block3_add\n",
      "conv4_block3_out\n",
      "conv4_block4_1_conv\n",
      "conv4_block4_1_bn\n",
      "conv4_block4_1_relu\n",
      "conv4_block4_2_conv\n",
      "conv4_block4_2_bn\n",
      "conv4_block4_2_relu\n",
      "conv4_block4_3_conv\n",
      "conv4_block4_3_bn\n",
      "conv4_block4_add\n",
      "conv4_block4_out\n",
      "conv4_block5_1_conv\n",
      "conv4_block5_1_bn\n",
      "conv4_block5_1_relu\n",
      "conv4_block5_2_conv\n",
      "conv4_block5_2_bn\n",
      "conv4_block5_2_relu\n",
      "conv4_block5_3_conv\n",
      "conv4_block5_3_bn\n",
      "conv4_block5_add\n",
      "conv4_block5_out\n",
      "conv4_block6_1_conv\n",
      "conv4_block6_1_bn\n",
      "conv4_block6_1_relu\n",
      "conv4_block6_2_conv\n",
      "conv4_block6_2_bn\n",
      "conv4_block6_2_relu\n",
      "conv4_block6_3_conv\n",
      "conv4_block6_3_bn\n",
      "conv4_block6_add\n",
      "conv4_block6_out\n",
      "conv5_block1_1_conv\n",
      "conv5_block1_1_bn\n",
      "conv5_block1_1_relu\n",
      "conv5_block1_2_conv\n",
      "conv5_block1_2_bn\n",
      "conv5_block1_2_relu\n",
      "conv5_block1_0_conv\n",
      "conv5_block1_3_conv\n",
      "conv5_block1_0_bn\n",
      "conv5_block1_3_bn\n",
      "conv5_block1_add\n",
      "conv5_block1_out\n",
      "conv5_block2_1_conv\n",
      "conv5_block2_1_bn\n",
      "conv5_block2_1_relu\n",
      "conv5_block2_2_conv\n",
      "conv5_block2_2_bn\n",
      "conv5_block2_2_relu\n",
      "conv5_block2_3_conv\n",
      "conv5_block2_3_bn\n",
      "conv5_block2_add\n",
      "conv5_block2_out\n",
      "conv5_block3_1_conv\n",
      "conv5_block3_1_bn\n",
      "conv5_block3_1_relu\n",
      "conv5_block3_2_conv\n",
      "conv5_block3_2_bn\n",
      "conv5_block3_2_relu\n",
      "conv5_block3_3_conv\n",
      "conv5_block3_3_bn\n",
      "conv5_block3_add\n",
      "conv5_block3_out\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "if args.imagenet_weights:\n",
    "    print('loading VGG model')\n",
    "    if args.dataset == 'cxr1000':\n",
    "        tr = 1\n",
    "        if tr:\n",
    "            print('using imagenet weights for CXR dataset')\n",
    "            vgg = VGG16(weights='imagenet',include_top = True)#top needed to get output dimensions at each layer\n",
    "                # EfficientNetB0(include_top=True,\n",
    "                #                weights=None,\n",
    "                #                input_shape=img_shape,\n",
    "                #                classes=len(all_labels),\n",
    "                #                classifier_activation='sigmoid')\n",
    "            base_model = tf.keras.Model(vgg.input,vgg.layers[-6].output)\n",
    "            #model.compile(optimizer = optimizers.RMSprop(), loss = 'binary_crossentropy',#adam #weighted_binary_crossentropy #lr=0.001/2#binary_crossentropy\n",
    "                                       #metrics = ['binary_accuracy'])#,tf.keras.metrics.AUC()])\n",
    "        else:\n",
    "            print('loading saved model - NOT IMPLEMENTED YET')\n",
    "            #model = load_model('../input/efnb0-saved-weights/xray_class_EfficientNetB4_15_class_CEL_heatmap_imagenet_pretrained_weights.05-0.1807.hdf5')\n",
    "        \n",
    "        #model.summary()\n",
    "    elif args.dataset == 'cifar10':\n",
    "        print('using imagenet weights for cifar10 dataset')\n",
    "        vgg = VGG16(weights='imagenet',include_top = False,input_shape=(32,32,3))#top needed to get output dimensions at each layer\n",
    "        freeze=True\n",
    "        if freeze:\n",
    "            for layer in vgg.layers:\n",
    "                layer.trainable = False\n",
    "        base_model = tf.keras.Model(vgg.input,vgg.layers[-2].output)\n",
    "    elif args.dataset == 'CUB200':\n",
    "        if args.model == 'VGG16/':\n",
    "           print('using VGG16 imagenet weights for CUB200 dataset')\n",
    "           vgg = VGG16(weights='imagenet',include_top = False,input_shape=(224,224,3))#top needed to get output dimensions at each layer\n",
    "           freeze=True\n",
    "           if freeze:\n",
    "               for layer in vgg.layers:\n",
    "                   print (layer.name)\n",
    "                   if layer.name == '----block5_conv3': continue\n",
    "                   else: layer.trainable = False\n",
    "           base_model = tf.keras.Model(vgg.input,vgg.layers[-2].output)\n",
    "        elif args.model == 'resnet50/':\n",
    "           print('using resnet50 imagenet weights for CUB200 dataset')\n",
    "           vgg = ResNet50(weights='imagenet',include_top = False,input_shape=(224,224,3))#top needed to get output dimensions at each layer\n",
    "           freeze=True\n",
    "           if freeze:\n",
    "               for layer in vgg.layers:\n",
    "                   print (layer.name)\n",
    "                   if layer.name == '----block5_conv3': continue\n",
    "                   else: layer.trainable = False\n",
    "           base_model = tf.keras.Model(vgg.input,vgg.output)\n",
    "    elif args.dataset == 'catsvsdogs':\n",
    "        print('using imagenet weights for catsvsdogs dataset')\n",
    "        vgg = VGG16(weights='imagenet',include_top = False,input_shape=(224,224,3))#top needed to get output dimensions at each layer\n",
    "        freeze=True\n",
    "        if freeze:\n",
    "            for layer in vgg.layers:\n",
    "                layer.trainable = False\n",
    "        base_model = tf.keras.Model(vgg.input,vgg.layers[-2].output)\n",
    "    elif args.dataset == 'VOC2010':\n",
    "        print('using imagenet weights for VOC2010-animals dataset')\n",
    "        vgg = VGG16(weights='imagenet',include_top = False,input_shape=(224,224,3))#top needed to get output dimensions at each layer\n",
    "        freeze=True\n",
    "        if freeze:\n",
    "            for layer in vgg.layers:\n",
    "                layer.trainable = False\n",
    "        base_model = tf.keras.Model(vgg.input,vgg.layers[-2].output)\n",
    "else:\n",
    "    #base_model = VGG16(weights=None,include_top = False)\n",
    "    base_model = MyFunctionalModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:17.180013Z",
     "iopub.status.busy": "2020-12-02T05:47:17.179015Z",
     "iopub.status.idle": "2020-12-02T05:47:17.289185Z",
     "shell.execute_reply": "2020-12-02T05:47:17.290340Z"
    },
    "papermill": {
     "duration": 0.156274,
     "end_time": "2020-12-02T05:47:17.290542",
     "exception": false,
     "start_time": "2020-12-02T05:47:17.134268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          409800      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,997,512\n",
      "Trainable params: 409,800\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#%% create base model\n",
    "if args.full_standard:\n",
    "    top_filters = base_model.output_shape[3] # flters in top conv layer (512 for VGG)\n",
    "    fmatrix = tf.keras.layers.Input(shape=(top_filters))\n",
    "    #flag = tf.keras.layers.Input(shape=(1))\n",
    "    \n",
    "    if args.model == 'VGG16/':\n",
    "        x =  MaxPool2D()(base_model.output)\n",
    "    elif args.model == 'resnet50/':\n",
    "        x =  base_model.output\n",
    "    mean_fmap = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "\n",
    "    \n",
    "    #modify base model (once it has been pre-trained separately) to be used with CF model later\n",
    "    if args.create_counterfactual_combined:\n",
    "        #modified_fmap = tf.cond(tf.reduce_sum(fmatrix)>511, lambda: mean_fmap, lambda: mean_fmap*fmatrix)#modified_fmap = mean_fmap*fmatrix\n",
    "        modified_fmap = mean_fmap*fmatrix\n",
    "        pre_softmax = Dense(num_classes,activation=None)(modified_fmap)\n",
    "        x = tf.keras.layers.Activation('softmax')(pre_softmax)\n",
    "        model = tf.keras.Model(inputs=[base_model.input, fmatrix], outputs= [x,base_model.output, mean_fmap, modified_fmap,pre_softmax],name='VGG_base_model')\n",
    "        default_fmatrix = tf.ones((train_gen.batch_size,base_model.output.shape[3]))\n",
    "    else:\n",
    "        x = tf.keras.layers.Dropout(0.5)(mean_fmap)\n",
    "        \n",
    "        #x = Dense(512,activation='relu')(x)\n",
    "        #x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #x = Dense(512,activation='relu')(x)\n",
    "\n",
    "        x = Dense(num_classes,activation='softmax')(x)\n",
    "        if args.train_using_builtin_fit_method:\n",
    "            model = tf.keras.Model(inputs=base_model.input, outputs= [x])#, base_model.output])\n",
    "        else:\n",
    "            model = tf.keras.Model(inputs=base_model.input, outputs= [x, base_model.output])\n",
    "else:\n",
    "    model = MySubClassModel(num_classes=num_classes, base_model=base_model, args=args)\n",
    "    #model = base_model\n",
    "    model(tf.zeros(input_shape))\n",
    "    #model.build(input_shape = input_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.load_weights('./trained_weights/VGG16/CUB200/standard/model.09-2.3280.hdf5')\n",
    "#load saved weights\n",
    "if args.resume:\n",
    "    #model.load_weights('./trained_weights/myCNN/cifar10/standard/model.hdf5')\n",
    "    #model.load_weights('./trained_weights/myCNN/cifar10/interpretable/filter_category_method_paper/from_pretrained_model.hdf5')\n",
    "    model.load_weights(filepath=weights_path+'/model.hdf5')\n",
    "    #model.load_weights('./trained_weights/myCNN/cifar10/interpretable/filter_category_method_paper/model.hdf5')\n",
    "\n",
    "    print(\"weights loaded\")\n",
    "if args.pretrained:\n",
    "    model.load_weights('./trained_weights/myCNN/cifar10/standard/model.hdf5')\n",
    "    print(\"pretrained weights loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T05:47:17.373894Z",
     "iopub.status.busy": "2020-12-02T05:47:17.372865Z",
     "iopub.status.idle": "2020-12-02T06:09:37.030831Z",
     "shell.execute_reply": "2020-12-02T06:09:37.029936Z"
    },
    "papermill": {
     "duration": 1339.700017,
     "end_time": "2020-12-02T06:09:37.030951",
     "exception": false,
     "start_time": "2020-12-02T05:47:17.330934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 169 steps, validate for 19 steps\n",
      "Epoch 1/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 5.8252 - accuracy: 0.0099\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.05219, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.01-4.8786.hdf5\n",
      "169/169 [==============================] - 61s 360ms/step - loss: 5.8233 - accuracy: 0.0100 - val_loss: 4.8786 - val_accuracy: 0.0522\n",
      "Epoch 2/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 4.9080 - accuracy: 0.0494\n",
      "Epoch 00002: val_accuracy improved from 0.05219 to 0.12795, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.02-4.2134.hdf5\n",
      "169/169 [==============================] - 46s 272ms/step - loss: 4.9053 - accuracy: 0.0494 - val_loss: 4.2134 - val_accuracy: 0.1279\n",
      "Epoch 3/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 4.2661 - accuracy: 0.1161\n",
      "Epoch 00003: val_accuracy improved from 0.12795 to 0.20707, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.03-3.7385.hdf5\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 4.2659 - accuracy: 0.1165 - val_loss: 3.7385 - val_accuracy: 0.2071\n",
      "Epoch 4/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 3.7393 - accuracy: 0.1913\n",
      "Epoch 00004: val_accuracy improved from 0.20707 to 0.26936, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.04-3.3987.hdf5\n",
      "169/169 [==============================] - 44s 261ms/step - loss: 3.7391 - accuracy: 0.1917 - val_loss: 3.3987 - val_accuracy: 0.2694\n",
      "Epoch 5/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 3.3189 - accuracy: 0.2569\n",
      "Epoch 00005: val_accuracy improved from 0.26936 to 0.30135, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.05-3.1332.hdf5\n",
      "169/169 [==============================] - 46s 271ms/step - loss: 3.3159 - accuracy: 0.2574 - val_loss: 3.1332 - val_accuracy: 0.3013\n",
      "Epoch 6/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 3.0244 - accuracy: 0.3141\n",
      "Epoch 00006: val_accuracy improved from 0.30135 to 0.31650, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.06-2.9637.hdf5\n",
      "169/169 [==============================] - 44s 260ms/step - loss: 3.0219 - accuracy: 0.3148 - val_loss: 2.9637 - val_accuracy: 0.3165\n",
      "Epoch 7/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 2.7279 - accuracy: 0.3761\n",
      "Epoch 00007: val_accuracy improved from 0.31650 to 0.33502, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.07-2.8042.hdf5\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 2.7269 - accuracy: 0.3769 - val_loss: 2.8042 - val_accuracy: 0.3350\n",
      "Epoch 8/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 2.5163 - accuracy: 0.4192\n",
      "Epoch 00008: val_accuracy improved from 0.33502 to 0.35017, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.08-2.6924.hdf5\n",
      "169/169 [==============================] - 44s 263ms/step - loss: 2.5181 - accuracy: 0.4185 - val_loss: 2.6924 - val_accuracy: 0.3502\n",
      "Epoch 9/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 2.2936 - accuracy: 0.4590\n",
      "Epoch 00009: val_accuracy improved from 0.35017 to 0.36364, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.09-2.5927.hdf5\n",
      "169/169 [==============================] - 44s 261ms/step - loss: 2.2950 - accuracy: 0.4583 - val_loss: 2.5927 - val_accuracy: 0.3636\n",
      "Epoch 10/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 2.1292 - accuracy: 0.5004\n",
      "Epoch 00010: val_accuracy improved from 0.36364 to 0.38047, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.10-2.5095.hdf5\n",
      "169/169 [==============================] - 45s 267ms/step - loss: 2.1300 - accuracy: 0.5000 - val_loss: 2.5095 - val_accuracy: 0.3805\n",
      "Epoch 11/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.9919 - accuracy: 0.5345\n",
      "Epoch 00011: val_accuracy improved from 0.38047 to 0.39899, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.11-2.4395.hdf5\n",
      "169/169 [==============================] - 45s 265ms/step - loss: 1.9884 - accuracy: 0.5359 - val_loss: 2.4395 - val_accuracy: 0.3990\n",
      "Epoch 12/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.8582 - accuracy: 0.5682\n",
      "Epoch 00012: val_accuracy did not improve from 0.39899\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 1.8567 - accuracy: 0.5680 - val_loss: 2.4086 - val_accuracy: 0.3956\n",
      "Epoch 13/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.7612 - accuracy: 0.5885 ETA: 0s - loss: 1.7626 - accuracy: 0.58\n",
      "Epoch 00013: val_accuracy improved from 0.39899 to 0.41919, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.13-2.3526.hdf5\n",
      "169/169 [==============================] - 44s 261ms/step - loss: 1.7640 - accuracy: 0.5876 - val_loss: 2.3526 - val_accuracy: 0.4192\n",
      "Epoch 14/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.6505 - accuracy: 0.6097\n",
      "Epoch 00014: val_accuracy did not improve from 0.41919\n",
      "169/169 [==============================] - 43s 257ms/step - loss: 1.6526 - accuracy: 0.6091 - val_loss: 2.2937 - val_accuracy: 0.4108\n",
      "Epoch 15/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.5550 - accuracy: 0.6319\n",
      "Epoch 00015: val_accuracy improved from 0.41919 to 0.42088, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.15-2.2702.hdf5\n",
      "169/169 [==============================] - 44s 262ms/step - loss: 1.5578 - accuracy: 0.6313 - val_loss: 2.2702 - val_accuracy: 0.4209\n",
      "Epoch 16/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.4847 - accuracy: 0.6513\n",
      "Epoch 00016: val_accuracy improved from 0.42088 to 0.42256, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.16-2.2519.hdf5\n",
      "169/169 [==============================] - 44s 261ms/step - loss: 1.4852 - accuracy: 0.6509 - val_loss: 2.2519 - val_accuracy: 0.4226\n",
      "Epoch 17/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.4197 - accuracy: 0.6652\n",
      "Epoch 00017: val_accuracy did not improve from 0.42256\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 1.4214 - accuracy: 0.6644 - val_loss: 2.2214 - val_accuracy: 0.4192\n",
      "Epoch 18/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.3455 - accuracy: 0.6896\n",
      "Epoch 00018: val_accuracy improved from 0.42256 to 0.43434, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.18-2.2004.hdf5\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 1.3472 - accuracy: 0.6891 - val_loss: 2.2004 - val_accuracy: 0.4343\n",
      "Epoch 19/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.2910 - accuracy: 0.7027\n",
      "Epoch 00019: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 43s 252ms/step - loss: 1.2925 - accuracy: 0.7028 - val_loss: 2.1819 - val_accuracy: 0.4125\n",
      "Epoch 20/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.2138 - accuracy: 0.7194\n",
      "Epoch 00020: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 43s 256ms/step - loss: 1.2168 - accuracy: 0.7183 - val_loss: 2.1637 - val_accuracy: 0.4175\n",
      "Epoch 21/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.1644 - accuracy: 0.7347\n",
      "Epoch 00021: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 43s 255ms/step - loss: 1.1648 - accuracy: 0.7344 - val_loss: 2.1551 - val_accuracy: 0.4141\n",
      "Epoch 22/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.1178 - accuracy: 0.7517\n",
      "Epoch 00022: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 43s 257ms/step - loss: 1.1170 - accuracy: 0.7517 - val_loss: 2.1168 - val_accuracy: 0.4276\n",
      "Epoch 23/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.0624 - accuracy: 0.7649\n",
      "Epoch 00023: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 44s 259ms/step - loss: 1.0626 - accuracy: 0.7648 - val_loss: 2.1327 - val_accuracy: 0.4276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 1.0402 - accuracy: 0.7681\n",
      "Epoch 00024: val_accuracy did not improve from 0.43434\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 1.0401 - accuracy: 0.7676 - val_loss: 2.0946 - val_accuracy: 0.4343\n",
      "Epoch 25/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.9875 - accuracy: 0.7854\n",
      "Epoch 00025: val_accuracy improved from 0.43434 to 0.43771, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.25-2.0990.hdf5\n",
      "169/169 [==============================] - 45s 265ms/step - loss: 0.9888 - accuracy: 0.7848 - val_loss: 2.0990 - val_accuracy: 0.4377\n",
      "Epoch 26/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.9704 - accuracy: 0.7882\n",
      "Epoch 00026: val_accuracy did not improve from 0.43771\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 0.9688 - accuracy: 0.7887 - val_loss: 2.0892 - val_accuracy: 0.4343\n",
      "Epoch 27/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.9423 - accuracy: 0.7880 ETA: 0s - loss: 0.9433 - accuracy: \n",
      "Epoch 00027: val_accuracy did not improve from 0.43771\n",
      "169/169 [==============================] - 43s 257ms/step - loss: 0.9412 - accuracy: 0.7883 - val_loss: 2.1015 - val_accuracy: 0.4377\n",
      "Epoch 28/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.8968 - accuracy: 0.8135\n",
      "Epoch 00028: val_accuracy improved from 0.43771 to 0.44444, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.28-2.0704.hdf5\n",
      "169/169 [==============================] - 44s 258ms/step - loss: 0.8961 - accuracy: 0.8139 - val_loss: 2.0704 - val_accuracy: 0.4444\n",
      "Epoch 29/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.8747 - accuracy: 0.8128\n",
      "Epoch 00029: val_accuracy did not improve from 0.44444\n",
      "169/169 [==============================] - 43s 255ms/step - loss: 0.8758 - accuracy: 0.8130 - val_loss: 2.0700 - val_accuracy: 0.4360\n",
      "Epoch 30/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.8394 - accuracy: 0.8251\n",
      "Epoch 00030: val_accuracy improved from 0.44444 to 0.44949, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.30-2.0485.hdf5\n",
      "169/169 [==============================] - 44s 259ms/step - loss: 0.8376 - accuracy: 0.8257 - val_loss: 2.0485 - val_accuracy: 0.4495\n",
      "Epoch 31/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.8097 - accuracy: 0.8303\n",
      "Epoch 00031: val_accuracy did not improve from 0.44949\n",
      "169/169 [==============================] - 43s 255ms/step - loss: 0.8090 - accuracy: 0.8307 - val_loss: 2.0370 - val_accuracy: 0.4461\n",
      "Epoch 32/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.8025 - accuracy: 0.8279\n",
      "Epoch 00032: val_accuracy did not improve from 0.44949\n",
      "169/169 [==============================] - 43s 255ms/step - loss: 0.8029 - accuracy: 0.8280 - val_loss: 2.0280 - val_accuracy: 0.4495\n",
      "Epoch 33/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.8404\n",
      "Epoch 00033: val_accuracy improved from 0.44949 to 0.45118, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.33-2.0319.hdf5\n",
      "169/169 [==============================] - 43s 257ms/step - loss: 0.7687 - accuracy: 0.8409 - val_loss: 2.0319 - val_accuracy: 0.4512\n",
      "Epoch 34/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.7387 - accuracy: 0.8471\n",
      "Epoch 00034: val_accuracy did not improve from 0.45118\n",
      "169/169 [==============================] - 43s 255ms/step - loss: 0.7380 - accuracy: 0.8474 - val_loss: 2.0426 - val_accuracy: 0.4461\n",
      "Epoch 35/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.7305 - accuracy: 0.8480\n",
      "Epoch 00035: val_accuracy did not improve from 0.45118\n",
      "169/169 [==============================] - 43s 253ms/step - loss: 0.7308 - accuracy: 0.8478 - val_loss: 2.0312 - val_accuracy: 0.4478\n",
      "Epoch 36/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.7064 - accuracy: 0.8534\n",
      "Epoch 00036: val_accuracy improved from 0.45118 to 0.45286, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.36-2.0200.hdf5\n",
      "169/169 [==============================] - 44s 261ms/step - loss: 0.7067 - accuracy: 0.8531 - val_loss: 2.0200 - val_accuracy: 0.4529\n",
      "Epoch 37/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.8594\n",
      "Epoch 00037: val_accuracy did not improve from 0.45286\n",
      "169/169 [==============================] - 44s 259ms/step - loss: 0.6924 - accuracy: 0.8594 - val_loss: 2.0213 - val_accuracy: 0.4478\n",
      "Epoch 38/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.8661\n",
      "Epoch 00038: val_accuracy improved from 0.45286 to 0.45623, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.38-2.0114.hdf5\n",
      "169/169 [==============================] - 44s 263ms/step - loss: 0.6756 - accuracy: 0.8663 - val_loss: 2.0114 - val_accuracy: 0.4562\n",
      "Epoch 39/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.8703\n",
      "Epoch 00039: val_accuracy did not improve from 0.45623\n",
      "169/169 [==============================] - 44s 257ms/step - loss: 0.6553 - accuracy: 0.8707 - val_loss: 2.0108 - val_accuracy: 0.4562\n",
      "Epoch 40/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6222 - accuracy: 0.8770\n",
      "Epoch 00040: val_accuracy did not improve from 0.45623\n",
      "169/169 [==============================] - 43s 256ms/step - loss: 0.6221 - accuracy: 0.8774 - val_loss: 2.0109 - val_accuracy: 0.4495\n",
      "Epoch 41/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5990 - accuracy: 0.8890\n",
      "Epoch 00041: val_accuracy did not improve from 0.45623\n",
      "169/169 [==============================] - 43s 257ms/step - loss: 0.5993 - accuracy: 0.8887 - val_loss: 2.0120 - val_accuracy: 0.4529\n",
      "Epoch 42/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.8927\n",
      "Epoch 00042: val_accuracy did not improve from 0.45623\n",
      "169/169 [==============================] - 43s 253ms/step - loss: 0.5943 - accuracy: 0.8926 - val_loss: 2.0100 - val_accuracy: 0.4562\n",
      "Epoch 43/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.8968\n",
      "Epoch 00043: val_accuracy did not improve from 0.45623\n",
      "169/169 [==============================] - 44s 259ms/step - loss: 0.5741 - accuracy: 0.8972 - val_loss: 2.0095 - val_accuracy: 0.4529\n",
      "Epoch 44/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.8959\n",
      "Epoch 00044: val_accuracy improved from 0.45623 to 0.46128, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.44-1.9954.hdf5\n",
      "169/169 [==============================] - 46s 271ms/step - loss: 0.5677 - accuracy: 0.8961 - val_loss: 1.9954 - val_accuracy: 0.4613\n",
      "Epoch 45/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.8957\n",
      "Epoch 00045: val_accuracy did not improve from 0.46128\n",
      "169/169 [==============================] - 47s 278ms/step - loss: 0.5653 - accuracy: 0.8959 - val_loss: 1.9995 - val_accuracy: 0.4545\n",
      "Epoch 46/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5519 - accuracy: 0.8955\n",
      "Epoch 00046: val_accuracy improved from 0.46128 to 0.46296, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.46-2.0092.hdf5\n",
      "169/169 [==============================] - 48s 284ms/step - loss: 0.5510 - accuracy: 0.8959 - val_loss: 2.0092 - val_accuracy: 0.4630\n",
      "Epoch 47/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.9009\n",
      "Epoch 00047: val_accuracy did not improve from 0.46296\n",
      "169/169 [==============================] - 46s 271ms/step - loss: 0.5285 - accuracy: 0.9007 - val_loss: 1.9844 - val_accuracy: 0.4613\n",
      "Epoch 48/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5192 - accuracy: 0.9083\n",
      "Epoch 00048: val_accuracy improved from 0.46296 to 0.46801, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.48-1.9844.hdf5\n",
      "169/169 [==============================] - 45s 266ms/step - loss: 0.5184 - accuracy: 0.9087 - val_loss: 1.9844 - val_accuracy: 0.4680\n",
      "Epoch 49/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.9011\n",
      "Epoch 00049: val_accuracy improved from 0.46801 to 0.46970, saving model to ./trained_weights/resnet50/CUB200/standard/model_transfer.49-1.9834.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 44s 259ms/step - loss: 0.5202 - accuracy: 0.9006 - val_loss: 1.9834 - val_accuracy: 0.4697\n",
      "Epoch 50/50\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.5018 - accuracy: 0.9094\n",
      "Epoch 00050: val_accuracy did not improve from 0.46970\n",
      "169/169 [==============================] - 43s 254ms/step - loss: 0.5018 - accuracy: 0.9096 - val_loss: 1.9842 - val_accuracy: 0.4680\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Trains for 5 epochs.\n",
    "if args.train_using_builtin_fit_method:\n",
    "\n",
    "    model.compile(optimizer=optimizers.SGD(lr=0.01/10, momentum = 0.9), \n",
    "                  loss=[categorical_crossentropy], \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    #%%\n",
    "    save_path=weights_path+'/model_transfer.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(save_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only = True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    #%%\n",
    "    history = model.fit(train_gen, epochs=50, verbose=1, callbacks=callbacks_list, validation_data=test_gen, shuffle=True)\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    #plt.style.available\n",
    "    #['fivethirtyeight',\n",
    "     #'seaborn-pastel',\n",
    "     #'seaborn-whitegrid',\n",
    "     #'ggplot',\n",
    "     #'grayscale']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(fname='model_accuracy_'+db+'.png')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(fname='model_loss_'+db+'.png')\n",
    "    \n",
    "    #%% stop execution\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(filepath=weights_path+\"/model_transfer_epoch_50.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.29073,
     "end_time": "2020-12-02T06:09:41.376678",
     "exception": false,
     "start_time": "2020-12-02T06:09:39.085948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Now fine tune all layers at small learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.0327,
     "end_time": "2020-12-02T06:09:49.916900",
     "exception": false,
     "start_time": "2020-12-02T06:09:47.884200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# enable augmentation for increasing regularization for fine-tuning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T06:09:55.113260Z",
     "iopub.status.busy": "2020-12-02T06:09:55.112341Z",
     "iopub.status.idle": "2020-12-02T06:09:56.192708Z",
     "shell.execute_reply": "2020-12-02T06:09:56.192155Z"
    },
    "papermill": {
     "duration": 3.152934,
     "end_time": "2020-12-02T06:09:56.192822",
     "exception": false,
     "start_time": "2020-12-02T06:09:53.039888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using imagenet_weights\n",
      "Found 5400 images belonging to 200 classes.\n",
      "Found 594 images belonging to 200 classes.\n",
      "Found 5400 images belonging to 200 classes.\n",
      "Found 594 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "if args.imagenet_weights:\n",
    "    print('using imagenet_weights')\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "                                        #rescale = 1./255)\n",
    "\n",
    "        train_gen = imgDataGen.flow(x_train, y_train, batch_size = batch_size,shuffle= False)\n",
    "        test_gen  = imgDataGen.flow(x_test, y_test, batch_size = batch_size,shuffle= False)\n",
    "    else:\n",
    "        augment = True\n",
    "        if not augment:\n",
    "            imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input, \n",
    "                                            #rescale = 1./255,\n",
    "                                            validation_split=0.1)\n",
    "        else:\n",
    "            imgDataGen = ImageDataGenerator(preprocessing_function = preprocess_input, \n",
    "                                        #rescale = 1./255,\n",
    "                                        validation_split=0.1,\n",
    "                                        \n",
    "                              height_shift_range= 0.2, \n",
    "                              width_shift_range=0.2, \n",
    "                              rotation_range=15, \n",
    "                              shear_range = 0.2,\n",
    "                              fill_mode = 'nearest',#''nearest#reflect\n",
    "                              zoom_range=0.2)\n",
    "        \n",
    "        train_gen = imgDataGen.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                subset='training',\n",
    "                                interpolation='nearest')#,\n",
    "                                #all classes for base model; binary classes for CF model\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_map)\n",
    "        test_gen  = imgDataGen.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                seed=None,\n",
    "                                subset='validation',\n",
    "                                interpolation='nearest')#,\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                               # classes = label_map)\n",
    "        \n",
    "        # for visualization, dont use preprocessed image\n",
    "        imgDataGen_nopreprocess = ImageDataGenerator(#preprocessing_function = preprocess_input, \n",
    "                                        rescale = 1./255,\n",
    "                                        validation_split=0.1)\n",
    "        \n",
    "        train_gen_nopreprocess = imgDataGen_nopreprocess.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                subset='training',\n",
    "                                interpolation='nearest'),\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_map)\n",
    "        test_gen_nopreprocess  = imgDataGen_nopreprocess.flow_from_directory(data_dir,\n",
    "                                target_size=(input_shape[1], input_shape[2]),\n",
    "                                color_mode='rgb',\n",
    "                                class_mode='categorical',\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                seed=None,\n",
    "                                subset='validation',\n",
    "                                interpolation='nearest'),\n",
    "                                #classes = ['cat', 'dog'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = ['cat'] if args.train_counterfactual_net else label_map)#['cat', 'dog'])\n",
    "                                #classes = label_map)\n",
    "        if args.dataset == 'CUB200' and official_split:\n",
    "            #actual unseen test set\n",
    "            imgDataGen_official_split = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "            actual_test_gen  = imgDataGen_official_split.flow_from_directory(data_dir_test,\n",
    "                            target_size=(input_shape[1], input_shape[2]),\n",
    "                            color_mode='rgb',\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=None,\n",
    "                            #subset='validation',\n",
    "                            interpolation='nearest')\n",
    "            imgDataGen_official_split_nopreprocess = ImageDataGenerator(rescale = 1./255)\n",
    "            actual_test_gen_nopreprocess  = imgDataGen_official_split_nopreprocess.flow_from_directory(data_dir_test,\n",
    "                            target_size=(input_shape[1], input_shape[2]),\n",
    "                            color_mode='rgb',\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=None,\n",
    "                            #subset='validation',\n",
    "                            interpolation='nearest')                            \n",
    "elif args.dataset == 'cxr1000':\n",
    "    train_gen, test_gen, valid_gen = load_cxr_dataset(train_df, test_df, valid_df, all_labels, batch_size)\n",
    "    \n",
    "else:\n",
    "    print('not using imagenet_weights')\n",
    "\n",
    "    imgDataGen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    train_gen = imgDataGen.flow(x_train, y_train, batch_size = batch_size,shuffle= False)\n",
    "    test_gen  = imgDataGen.flow(x_test, y_test, batch_size = batch_size,shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T06:10:00.229710Z",
     "iopub.status.busy": "2020-12-02T06:10:00.229009Z",
     "iopub.status.idle": "2020-12-02T09:00:35.976829Z",
     "shell.execute_reply": "2020-12-02T09:00:35.974177Z"
    },
    "papermill": {
     "duration": 10237.75517,
     "end_time": "2020-12-02T09:00:35.976976",
     "exception": false,
     "start_time": "2020-12-02T06:09:58.221806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          409800      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,997,512\n",
      "Trainable params: 23,944,392\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 169 steps, validate for 19 steps\n",
      "Epoch 1/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 1.1378 - accuracy: 0.6827\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53535, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.01-1.6051.hdf5\n",
      "169/169 [==============================] - 232s 1s/step - loss: 1.1372 - accuracy: 0.6826 - val_loss: 1.6051 - val_accuracy: 0.5354\n",
      "Epoch 2/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.9466 - accuracy: 0.7452\n",
      "Epoch 00002: val_accuracy improved from 0.53535 to 0.56229, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.02-1.5339.hdf5\n",
      "169/169 [==============================] - 209s 1s/step - loss: 0.9471 - accuracy: 0.7444 - val_loss: 1.5339 - val_accuracy: 0.5623\n",
      "Epoch 3/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.8798 - accuracy: 0.7604\n",
      "Epoch 00003: val_accuracy improved from 0.56229 to 0.57576, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.03-1.5178.hdf5\n",
      "169/169 [==============================] - 206s 1s/step - loss: 0.8828 - accuracy: 0.7600 - val_loss: 1.5178 - val_accuracy: 0.5758\n",
      "Epoch 4/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.7991 - accuracy: 0.7848\n",
      "Epoch 00004: val_accuracy improved from 0.57576 to 0.58418, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.04-1.4318.hdf5\n",
      "169/169 [==============================] - 207s 1s/step - loss: 0.7988 - accuracy: 0.7848 - val_loss: 1.4318 - val_accuracy: 0.5842\n",
      "Epoch 5/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.7668 - accuracy: 0.7901\n",
      "Epoch 00005: val_accuracy improved from 0.58418 to 0.58754, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.05-1.4529.hdf5\n",
      "169/169 [==============================] - 218s 1s/step - loss: 0.7673 - accuracy: 0.7902 - val_loss: 1.4529 - val_accuracy: 0.5875\n",
      "Epoch 6/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.6960 - accuracy: 0.8184\n",
      "Epoch 00006: val_accuracy improved from 0.58754 to 0.59764, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.06-1.4229.hdf5\n",
      "169/169 [==============================] - 213s 1s/step - loss: 0.6975 - accuracy: 0.8183 - val_loss: 1.4229 - val_accuracy: 0.5976\n",
      "Epoch 7/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.6819 - accuracy: 0.8132\n",
      "Epoch 00007: val_accuracy improved from 0.59764 to 0.61616, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.07-1.3620.hdf5\n",
      "169/169 [==============================] - 206s 1s/step - loss: 0.6831 - accuracy: 0.8130 - val_loss: 1.3620 - val_accuracy: 0.6162\n",
      "Epoch 8/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.6359 - accuracy: 0.8314\n",
      "Epoch 00008: val_accuracy did not improve from 0.61616\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.6356 - accuracy: 0.8311 - val_loss: 1.4547 - val_accuracy: 0.5808\n",
      "Epoch 9/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.5932 - accuracy: 0.8431\n",
      "Epoch 00009: val_accuracy improved from 0.61616 to 0.62458, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.09-1.3741.hdf5\n",
      "169/169 [==============================] - 207s 1s/step - loss: 0.5932 - accuracy: 0.8430 - val_loss: 1.3741 - val_accuracy: 0.6246\n",
      "Epoch 10/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.5914 - accuracy: 0.8389\n",
      "Epoch 00010: val_accuracy did not improve from 0.62458\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.5918 - accuracy: 0.8387 - val_loss: 1.4803 - val_accuracy: 0.5825\n",
      "Epoch 11/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.5555 - accuracy: 0.8553\n",
      "Epoch 00011: val_accuracy did not improve from 0.62458\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.5555 - accuracy: 0.8554 - val_loss: 1.3262 - val_accuracy: 0.5909\n",
      "Epoch 12/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.5386 - accuracy: 0.8571\n",
      "Epoch 00012: val_accuracy did not improve from 0.62458\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.5418 - accuracy: 0.8559 - val_loss: 1.3173 - val_accuracy: 0.6212\n",
      "Epoch 13/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.5021 - accuracy: 0.8700\n",
      "Epoch 00013: val_accuracy did not improve from 0.62458\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.5037 - accuracy: 0.8691 - val_loss: 1.3372 - val_accuracy: 0.6128\n",
      "Epoch 14/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4911 - accuracy: 0.8715\n",
      "Epoch 00014: val_accuracy did not improve from 0.62458\n",
      "169/169 [==============================] - 205s 1s/step - loss: 0.4907 - accuracy: 0.8717 - val_loss: 1.3058 - val_accuracy: 0.6246\n",
      "Epoch 15/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4728 - accuracy: 0.8808\n",
      "Epoch 00015: val_accuracy improved from 0.62458 to 0.64310, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.15-1.2671.hdf5\n",
      "169/169 [==============================] - 205s 1s/step - loss: 0.4729 - accuracy: 0.8809 - val_loss: 1.2671 - val_accuracy: 0.6431\n",
      "Epoch 16/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4434 - accuracy: 0.8893\n",
      "Epoch 00016: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.4424 - accuracy: 0.8900 - val_loss: 1.2890 - val_accuracy: 0.6347\n",
      "Epoch 17/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4507 - accuracy: 0.8845\n",
      "Epoch 00017: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.4508 - accuracy: 0.8839 - val_loss: 1.3326 - val_accuracy: 0.6246\n",
      "Epoch 18/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4198 - accuracy: 0.8959\n",
      "Epoch 00018: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 206s 1s/step - loss: 0.4188 - accuracy: 0.8963 - val_loss: 1.3264 - val_accuracy: 0.6027\n",
      "Epoch 19/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.4108 - accuracy: 0.8959\n",
      "Epoch 00019: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 223s 1s/step - loss: 0.4105 - accuracy: 0.8959 - val_loss: 1.3135 - val_accuracy: 0.6330\n",
      "Epoch 20/150\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.4058 - accuracy: 0.8979\n",
      "Epoch 00020: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 184s 1s/step - loss: 0.4051 - accuracy: 0.8981 - val_loss: 1.2839 - val_accuracy: 0.6195\n",
      "Epoch 21/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3854 - accuracy: 0.9011\n",
      "Epoch 00021: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.3851 - accuracy: 0.9011 - val_loss: 1.3204 - val_accuracy: 0.6296\n",
      "Epoch 22/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3641 - accuracy: 0.9132\n",
      "Epoch 00022: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.3639 - accuracy: 0.9133 - val_loss: 1.3124 - val_accuracy: 0.6178\n",
      "Epoch 23/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3647 - accuracy: 0.9110\n",
      "Epoch 00023: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.3645 - accuracy: 0.9109 - val_loss: 1.3065 - val_accuracy: 0.6162\n",
      "Epoch 24/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3653 - accuracy: 0.9078\n",
      "Epoch 00024: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.3649 - accuracy: 0.9080 - val_loss: 1.2176 - val_accuracy: 0.6380\n",
      "Epoch 25/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3416 - accuracy: 0.9182\n",
      "Epoch 00025: val_accuracy did not improve from 0.64310\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.3408 - accuracy: 0.9185 - val_loss: 1.2757 - val_accuracy: 0.6296\n",
      "Epoch 26/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3289 - accuracy: 0.9195\n",
      "Epoch 00026: val_accuracy improved from 0.64310 to 0.64478, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.26-1.2529.hdf5\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.3284 - accuracy: 0.9198 - val_loss: 1.2529 - val_accuracy: 0.6448\n",
      "Epoch 27/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3246 - accuracy: 0.9240\n",
      "Epoch 00027: val_accuracy did not improve from 0.64478\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.3243 - accuracy: 0.9241 - val_loss: 1.2755 - val_accuracy: 0.6212\n",
      "Epoch 28/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.3150 - accuracy: 0.9270\n",
      "Epoch 00028: val_accuracy did not improve from 0.64478\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.3146 - accuracy: 0.9272 - val_loss: 1.2887 - val_accuracy: 0.6431\n",
      "Epoch 29/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2972 - accuracy: 0.9341\n",
      "Epoch 00029: val_accuracy improved from 0.64478 to 0.66330, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.29-1.2229.hdf5\n",
      "169/169 [==============================] - 210s 1s/step - loss: 0.2977 - accuracy: 0.9341 - val_loss: 1.2229 - val_accuracy: 0.6633\n",
      "Epoch 30/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2836 - accuracy: 0.9378\n",
      "Epoch 00030: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.2836 - accuracy: 0.9372 - val_loss: 1.2701 - val_accuracy: 0.6313\n",
      "Epoch 31/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2883 - accuracy: 0.9341\n",
      "Epoch 00031: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.2884 - accuracy: 0.9337 - val_loss: 1.2196 - val_accuracy: 0.6498\n",
      "Epoch 32/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2711 - accuracy: 0.9387\n",
      "Epoch 00032: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 220s 1s/step - loss: 0.2710 - accuracy: 0.9385 - val_loss: 1.2359 - val_accuracy: 0.6448\n",
      "Epoch 33/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2710 - accuracy: 0.9411\n",
      "Epoch 00033: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.2707 - accuracy: 0.9413 - val_loss: 1.2483 - val_accuracy: 0.6330\n",
      "Epoch 34/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2507 - accuracy: 0.9458\n",
      "Epoch 00034: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.2504 - accuracy: 0.9461 - val_loss: 1.2477 - val_accuracy: 0.6347\n",
      "Epoch 35/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2528 - accuracy: 0.9437\n",
      "Epoch 00035: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.2532 - accuracy: 0.9435 - val_loss: 1.2367 - val_accuracy: 0.6380\n",
      "Epoch 36/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2475 - accuracy: 0.9469\n",
      "Epoch 00036: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 211s 1s/step - loss: 0.2469 - accuracy: 0.9469 - val_loss: 1.2927 - val_accuracy: 0.6431\n",
      "Epoch 37/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2387 - accuracy: 0.9508\n",
      "Epoch 00037: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 217s 1s/step - loss: 0.2384 - accuracy: 0.9509 - val_loss: 1.2524 - val_accuracy: 0.6465\n",
      "Epoch 38/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2324 - accuracy: 0.9518\n",
      "Epoch 00038: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 216s 1s/step - loss: 0.2319 - accuracy: 0.9519 - val_loss: 1.2642 - val_accuracy: 0.6498\n",
      "Epoch 39/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2354 - accuracy: 0.9486\n",
      "Epoch 00039: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 214s 1s/step - loss: 0.2352 - accuracy: 0.9489 - val_loss: 1.2728 - val_accuracy: 0.6448\n",
      "Epoch 40/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2209 - accuracy: 0.9542\n",
      "Epoch 00040: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.2205 - accuracy: 0.9543 - val_loss: 1.2598 - val_accuracy: 0.6414\n",
      "Epoch 41/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2203 - accuracy: 0.9553\n",
      "Epoch 00041: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.2200 - accuracy: 0.9554 - val_loss: 1.2218 - val_accuracy: 0.6364\n",
      "Epoch 42/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2270 - accuracy: 0.9508\n",
      "Epoch 00042: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.2266 - accuracy: 0.9511 - val_loss: 1.2329 - val_accuracy: 0.6566\n",
      "Epoch 43/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2146 - accuracy: 0.9557\n",
      "Epoch 00043: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.2138 - accuracy: 0.9559 - val_loss: 1.2723 - val_accuracy: 0.6549\n",
      "Epoch 44/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1950 - accuracy: 0.9609\n",
      "Epoch 00044: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1950 - accuracy: 0.9607 - val_loss: 1.2491 - val_accuracy: 0.6465\n",
      "Epoch 45/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.2027 - accuracy: 0.9581\n",
      "Epoch 00045: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.2024 - accuracy: 0.9581 - val_loss: 1.2125 - val_accuracy: 0.6498\n",
      "Epoch 46/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1967 - accuracy: 0.9629\n",
      "Epoch 00046: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 222s 1s/step - loss: 0.1965 - accuracy: 0.9630 - val_loss: 1.2482 - val_accuracy: 0.6448\n",
      "Epoch 47/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1847 - accuracy: 0.9642\n",
      "Epoch 00047: val_accuracy did not improve from 0.66330\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.1846 - accuracy: 0.9643 - val_loss: 1.2274 - val_accuracy: 0.6532\n",
      "Epoch 48/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1782 - accuracy: 0.9678\n",
      "Epoch 00048: val_accuracy improved from 0.66330 to 0.66835, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.48-1.2039.hdf5\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1781 - accuracy: 0.9678 - val_loss: 1.2039 - val_accuracy: 0.6684\n",
      "Epoch 49/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1863 - accuracy: 0.9639\n",
      "Epoch 00049: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1862 - accuracy: 0.9639 - val_loss: 1.2336 - val_accuracy: 0.6582\n",
      "Epoch 50/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1843 - accuracy: 0.9648\n",
      "Epoch 00050: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1838 - accuracy: 0.9648 - val_loss: 1.2354 - val_accuracy: 0.6380\n",
      "Epoch 51/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1755 - accuracy: 0.9670\n",
      "Epoch 00051: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.1761 - accuracy: 0.9672 - val_loss: 1.2077 - val_accuracy: 0.6448\n",
      "Epoch 52/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1775 - accuracy: 0.9640\n",
      "Epoch 00052: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1778 - accuracy: 0.9641 - val_loss: 1.2153 - val_accuracy: 0.6465\n",
      "Epoch 53/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1731 - accuracy: 0.9659\n",
      "Epoch 00053: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 205s 1s/step - loss: 0.1724 - accuracy: 0.9661 - val_loss: 1.2410 - val_accuracy: 0.6515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1665 - accuracy: 0.9693\n",
      "Epoch 00054: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.1667 - accuracy: 0.9689 - val_loss: 1.1859 - val_accuracy: 0.6582\n",
      "Epoch 55/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1638 - accuracy: 0.9672\n",
      "Epoch 00055: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.1637 - accuracy: 0.9672 - val_loss: 1.2212 - val_accuracy: 0.6667\n",
      "Epoch 56/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1558 - accuracy: 0.9762\n",
      "Epoch 00056: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.1554 - accuracy: 0.9763 - val_loss: 1.2675 - val_accuracy: 0.6566\n",
      "Epoch 57/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1506 - accuracy: 0.9724\n",
      "Epoch 00057: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 205s 1s/step - loss: 0.1513 - accuracy: 0.9722 - val_loss: 1.2348 - val_accuracy: 0.6515\n",
      "Epoch 58/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1527 - accuracy: 0.9739\n",
      "Epoch 00058: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 205s 1s/step - loss: 0.1524 - accuracy: 0.9741 - val_loss: 1.2417 - val_accuracy: 0.6347\n",
      "Epoch 59/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1466 - accuracy: 0.9750\n",
      "Epoch 00059: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 208s 1s/step - loss: 0.1469 - accuracy: 0.9750 - val_loss: 1.2478 - val_accuracy: 0.6549\n",
      "Epoch 60/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1499 - accuracy: 0.9715\n",
      "Epoch 00060: val_accuracy did not improve from 0.66835\n",
      "169/169 [==============================] - 195s 1s/step - loss: 0.1501 - accuracy: 0.9715 - val_loss: 1.2645 - val_accuracy: 0.6380\n",
      "Epoch 61/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1454 - accuracy: 0.9706\n",
      "Epoch 00061: val_accuracy improved from 0.66835 to 0.67003, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.61-1.2219.hdf5\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.1450 - accuracy: 0.9707 - val_loss: 1.2219 - val_accuracy: 0.6700\n",
      "Epoch 62/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1383 - accuracy: 0.9780\n",
      "Epoch 00062: val_accuracy did not improve from 0.67003\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.1386 - accuracy: 0.9780 - val_loss: 1.2771 - val_accuracy: 0.6431\n",
      "Epoch 63/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1444 - accuracy: 0.9734\n",
      "Epoch 00063: val_accuracy did not improve from 0.67003\n",
      "169/169 [==============================] - 194s 1s/step - loss: 0.1440 - accuracy: 0.9735 - val_loss: 1.2494 - val_accuracy: 0.6515\n",
      "Epoch 64/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1352 - accuracy: 0.9743\n",
      "Epoch 00064: val_accuracy did not improve from 0.67003\n",
      "169/169 [==============================] - 196s 1s/step - loss: 0.1350 - accuracy: 0.9744 - val_loss: 1.2294 - val_accuracy: 0.6414\n",
      "Epoch 65/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1350 - accuracy: 0.9786\n",
      "Epoch 00065: val_accuracy improved from 0.67003 to 0.67172, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.65-1.1860.hdf5\n",
      "169/169 [==============================] - 193s 1s/step - loss: 0.1346 - accuracy: 0.9787 - val_loss: 1.1860 - val_accuracy: 0.6717\n",
      "Epoch 66/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1331 - accuracy: 0.9767\n",
      "Epoch 00066: val_accuracy improved from 0.67172 to 0.67340, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.66-1.2176.hdf5\n",
      "169/169 [==============================] - 192s 1s/step - loss: 0.1326 - accuracy: 0.9769 - val_loss: 1.2176 - val_accuracy: 0.6734\n",
      "Epoch 67/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1297 - accuracy: 0.9763\n",
      "Epoch 00067: val_accuracy did not improve from 0.67340\n",
      "169/169 [==============================] - 192s 1s/step - loss: 0.1297 - accuracy: 0.9763 - val_loss: 1.2000 - val_accuracy: 0.6599\n",
      "Epoch 68/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1233 - accuracy: 0.9804\n",
      "Epoch 00068: val_accuracy did not improve from 0.67340\n",
      "169/169 [==============================] - 196s 1s/step - loss: 0.1231 - accuracy: 0.9806 - val_loss: 1.2650 - val_accuracy: 0.6465\n",
      "Epoch 69/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1246 - accuracy: 0.9804\n",
      "Epoch 00069: val_accuracy did not improve from 0.67340\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.1244 - accuracy: 0.9806 - val_loss: 1.2472 - val_accuracy: 0.6566\n",
      "Epoch 70/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1294 - accuracy: 0.9784\n",
      "Epoch 00070: val_accuracy improved from 0.67340 to 0.67508, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.70-1.2162.hdf5\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.1297 - accuracy: 0.9781 - val_loss: 1.2162 - val_accuracy: 0.6751\n",
      "Epoch 71/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1208 - accuracy: 0.9784\n",
      "Epoch 00071: val_accuracy did not improve from 0.67508\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.1208 - accuracy: 0.9785 - val_loss: 1.2271 - val_accuracy: 0.6717\n",
      "Epoch 72/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1187 - accuracy: 0.9808\n",
      "Epoch 00072: val_accuracy improved from 0.67508 to 0.68519, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.72-1.1990.hdf5\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.1189 - accuracy: 0.9806 - val_loss: 1.1990 - val_accuracy: 0.6852\n",
      "Epoch 73/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1137 - accuracy: 0.9847\n",
      "Epoch 00073: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.1136 - accuracy: 0.9846 - val_loss: 1.1919 - val_accuracy: 0.6599\n",
      "Epoch 74/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1187 - accuracy: 0.9797\n",
      "Epoch 00074: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.1188 - accuracy: 0.9796 - val_loss: 1.2088 - val_accuracy: 0.6566\n",
      "Epoch 75/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1119 - accuracy: 0.9817\n",
      "Epoch 00075: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 196s 1s/step - loss: 0.1117 - accuracy: 0.9819 - val_loss: 1.2216 - val_accuracy: 0.6700\n",
      "Epoch 76/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1128 - accuracy: 0.9827\n",
      "Epoch 00076: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 195s 1s/step - loss: 0.1129 - accuracy: 0.9826 - val_loss: 1.2000 - val_accuracy: 0.6667\n",
      "Epoch 77/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1173 - accuracy: 0.9801\n",
      "Epoch 00077: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.1174 - accuracy: 0.9802 - val_loss: 1.2378 - val_accuracy: 0.6380\n",
      "Epoch 78/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1034 - accuracy: 0.9885\n",
      "Epoch 00078: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.1032 - accuracy: 0.9885 - val_loss: 1.1994 - val_accuracy: 0.6717\n",
      "Epoch 79/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1032 - accuracy: 0.9819\n",
      "Epoch 00079: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.1030 - accuracy: 0.9819 - val_loss: 1.2231 - val_accuracy: 0.6616\n",
      "Epoch 80/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1070 - accuracy: 0.9842\n",
      "Epoch 00080: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.1069 - accuracy: 0.9843 - val_loss: 1.2503 - val_accuracy: 0.6431\n",
      "Epoch 81/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0985 - accuracy: 0.9857\n",
      "Epoch 00081: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0981 - accuracy: 0.9857 - val_loss: 1.2156 - val_accuracy: 0.6616\n",
      "Epoch 82/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1037 - accuracy: 0.9836\n",
      "Epoch 00082: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.1038 - accuracy: 0.9837 - val_loss: 1.1998 - val_accuracy: 0.6751\n",
      "Epoch 83/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.1004 - accuracy: 0.9860\n",
      "Epoch 00083: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.1002 - accuracy: 0.9861 - val_loss: 1.2148 - val_accuracy: 0.6633\n",
      "Epoch 84/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0926 - accuracy: 0.9881\n",
      "Epoch 00084: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0928 - accuracy: 0.9880 - val_loss: 1.2132 - val_accuracy: 0.6684\n",
      "Epoch 85/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0970 - accuracy: 0.9857\n",
      "Epoch 00085: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0971 - accuracy: 0.9856 - val_loss: 1.2237 - val_accuracy: 0.6751\n",
      "Epoch 86/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0969 - accuracy: 0.9857\n",
      "Epoch 00086: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0967 - accuracy: 0.9857 - val_loss: 1.2278 - val_accuracy: 0.6599\n",
      "Epoch 87/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0946 - accuracy: 0.9864\n",
      "Epoch 00087: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0944 - accuracy: 0.9865 - val_loss: 1.2331 - val_accuracy: 0.6717\n",
      "Epoch 88/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0967 - accuracy: 0.9829\n",
      "Epoch 00088: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0965 - accuracy: 0.9830 - val_loss: 1.2128 - val_accuracy: 0.6549\n",
      "Epoch 89/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0922 - accuracy: 0.9855\n",
      "Epoch 00089: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0919 - accuracy: 0.9856 - val_loss: 1.2270 - val_accuracy: 0.6582\n",
      "Epoch 90/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0926 - accuracy: 0.9858\n",
      "Epoch 00090: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 222s 1s/step - loss: 0.0924 - accuracy: 0.9859 - val_loss: 1.2586 - val_accuracy: 0.6650\n",
      "Epoch 91/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0944 - accuracy: 0.9840\n",
      "Epoch 00091: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0947 - accuracy: 0.9839 - val_loss: 1.2133 - val_accuracy: 0.6700\n",
      "Epoch 92/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0923 - accuracy: 0.9875\n",
      "Epoch 00092: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0922 - accuracy: 0.9876 - val_loss: 1.2572 - val_accuracy: 0.6465\n",
      "Epoch 93/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0876 - accuracy: 0.9894\n",
      "Epoch 00093: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0874 - accuracy: 0.9894 - val_loss: 1.1924 - val_accuracy: 0.6751\n",
      "Epoch 94/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0813 - accuracy: 0.9911\n",
      "Epoch 00094: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0811 - accuracy: 0.9911 - val_loss: 1.1599 - val_accuracy: 0.6801\n",
      "Epoch 95/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0864 - accuracy: 0.9871\n",
      "Epoch 00095: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0864 - accuracy: 0.9872 - val_loss: 1.2328 - val_accuracy: 0.6582\n",
      "Epoch 96/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0827 - accuracy: 0.9875\n",
      "Epoch 00096: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0825 - accuracy: 0.9876 - val_loss: 1.2100 - val_accuracy: 0.6801\n",
      "Epoch 97/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0856 - accuracy: 0.9871\n",
      "Epoch 00097: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0854 - accuracy: 0.9872 - val_loss: 1.2114 - val_accuracy: 0.6768\n",
      "Epoch 98/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0818 - accuracy: 0.9877\n",
      "Epoch 00098: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0819 - accuracy: 0.9878 - val_loss: 1.2371 - val_accuracy: 0.6650\n",
      "Epoch 99/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0788 - accuracy: 0.9879\n",
      "Epoch 00099: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0787 - accuracy: 0.9878 - val_loss: 1.2274 - val_accuracy: 0.6667\n",
      "Epoch 100/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0730 - accuracy: 0.9916\n",
      "Epoch 00100: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0728 - accuracy: 0.9917 - val_loss: 1.2466 - val_accuracy: 0.6599\n",
      "Epoch 101/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0777 - accuracy: 0.9905\n",
      "Epoch 00101: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0777 - accuracy: 0.9906 - val_loss: 1.2286 - val_accuracy: 0.6684\n",
      "Epoch 102/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0755 - accuracy: 0.9914\n",
      "Epoch 00102: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0757 - accuracy: 0.9913 - val_loss: 1.2647 - val_accuracy: 0.6582\n",
      "Epoch 103/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0838 - accuracy: 0.9879\n",
      "Epoch 00103: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0841 - accuracy: 0.9876 - val_loss: 1.2199 - val_accuracy: 0.6801\n",
      "Epoch 104/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0737 - accuracy: 0.9927\n",
      "Epoch 00104: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 214s 1s/step - loss: 0.0741 - accuracy: 0.9926 - val_loss: 1.2360 - val_accuracy: 0.6768\n",
      "Epoch 105/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0806 - accuracy: 0.9890\n",
      "Epoch 00105: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0804 - accuracy: 0.9891 - val_loss: 1.2162 - val_accuracy: 0.6768\n",
      "Epoch 106/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0798 - accuracy: 0.9888\n",
      "Epoch 00106: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.0799 - accuracy: 0.9887 - val_loss: 1.2356 - val_accuracy: 0.6684\n",
      "Epoch 107/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0741 - accuracy: 0.9909\n",
      "Epoch 00107: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0743 - accuracy: 0.9907 - val_loss: 1.1902 - val_accuracy: 0.6751\n",
      "Epoch 108/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0737 - accuracy: 0.9899\n",
      "Epoch 00108: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0734 - accuracy: 0.9900 - val_loss: 1.2584 - val_accuracy: 0.6633\n",
      "Epoch 109/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0713 - accuracy: 0.9911\n",
      "Epoch 00109: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 204s 1s/step - loss: 0.0713 - accuracy: 0.9911 - val_loss: 1.1926 - val_accuracy: 0.6566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0739 - accuracy: 0.9881\n",
      "Epoch 00110: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0738 - accuracy: 0.9881 - val_loss: 1.2059 - val_accuracy: 0.6650\n",
      "Epoch 111/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0628 - accuracy: 0.9924\n",
      "Epoch 00111: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0627 - accuracy: 0.9924 - val_loss: 1.2219 - val_accuracy: 0.6650\n",
      "Epoch 112/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0697 - accuracy: 0.9916\n",
      "Epoch 00112: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0695 - accuracy: 0.9917 - val_loss: 1.2646 - val_accuracy: 0.6498\n",
      "Epoch 113/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0647 - accuracy: 0.9916\n",
      "Epoch 00113: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 197s 1s/step - loss: 0.0648 - accuracy: 0.9917 - val_loss: 1.1825 - val_accuracy: 0.6734\n",
      "Epoch 114/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0695 - accuracy: 0.9925\n",
      "Epoch 00114: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0693 - accuracy: 0.9926 - val_loss: 1.2455 - val_accuracy: 0.6801\n",
      "Epoch 115/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0686 - accuracy: 0.9905\n",
      "Epoch 00115: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0686 - accuracy: 0.9906 - val_loss: 1.2223 - val_accuracy: 0.6650\n",
      "Epoch 116/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0606 - accuracy: 0.9925\n",
      "Epoch 00116: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0609 - accuracy: 0.9926 - val_loss: 1.2155 - val_accuracy: 0.6515\n",
      "Epoch 117/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0653 - accuracy: 0.9931\n",
      "Epoch 00117: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0653 - accuracy: 0.9931 - val_loss: 1.2275 - val_accuracy: 0.6633\n",
      "Epoch 118/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0663 - accuracy: 0.9922\n",
      "Epoch 00118: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0662 - accuracy: 0.9922 - val_loss: 1.2015 - val_accuracy: 0.6852\n",
      "Epoch 119/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0637 - accuracy: 0.9924\n",
      "Epoch 00119: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0637 - accuracy: 0.9922 - val_loss: 1.2120 - val_accuracy: 0.6835\n",
      "Epoch 120/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0635 - accuracy: 0.9916\n",
      "Epoch 00120: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0636 - accuracy: 0.9915 - val_loss: 1.2312 - val_accuracy: 0.6768\n",
      "Epoch 121/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0610 - accuracy: 0.9916\n",
      "Epoch 00121: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0610 - accuracy: 0.9915 - val_loss: 1.1950 - val_accuracy: 0.6768\n",
      "Epoch 122/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0575 - accuracy: 0.9929\n",
      "Epoch 00122: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0575 - accuracy: 0.9930 - val_loss: 1.2087 - val_accuracy: 0.6751\n",
      "Epoch 123/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0633 - accuracy: 0.9929\n",
      "Epoch 00123: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0633 - accuracy: 0.9930 - val_loss: 1.1912 - val_accuracy: 0.6768\n",
      "Epoch 124/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0619 - accuracy: 0.9931\n",
      "Epoch 00124: val_accuracy did not improve from 0.68519\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0617 - accuracy: 0.9931 - val_loss: 1.2608 - val_accuracy: 0.6650\n",
      "Epoch 125/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0606 - accuracy: 0.9939\n",
      "Epoch 00125: val_accuracy improved from 0.68519 to 0.69024, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.125-1.2138.hdf5\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0604 - accuracy: 0.9939 - val_loss: 1.2138 - val_accuracy: 0.6902\n",
      "Epoch 126/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0601 - accuracy: 0.9929\n",
      "Epoch 00126: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 198s 1s/step - loss: 0.0604 - accuracy: 0.9928 - val_loss: 1.2674 - val_accuracy: 0.6599\n",
      "Epoch 127/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0579 - accuracy: 0.9935\n",
      "Epoch 00127: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0583 - accuracy: 0.9933 - val_loss: 1.2112 - val_accuracy: 0.6566\n",
      "Epoch 128/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0611 - accuracy: 0.9920\n",
      "Epoch 00128: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0610 - accuracy: 0.9920 - val_loss: 1.2527 - val_accuracy: 0.6633\n",
      "Epoch 129/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0563 - accuracy: 0.9924\n",
      "Epoch 00129: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0563 - accuracy: 0.9922 - val_loss: 1.1960 - val_accuracy: 0.6886\n",
      "Epoch 130/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0560 - accuracy: 0.9948\n",
      "Epoch 00130: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0559 - accuracy: 0.9948 - val_loss: 1.1805 - val_accuracy: 0.6633\n",
      "Epoch 131/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0560 - accuracy: 0.9948\n",
      "Epoch 00131: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0561 - accuracy: 0.9948 - val_loss: 1.2321 - val_accuracy: 0.6785\n",
      "Epoch 132/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0500 - accuracy: 0.9952\n",
      "Epoch 00132: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 209s 1s/step - loss: 0.0500 - accuracy: 0.9952 - val_loss: 1.1785 - val_accuracy: 0.6667\n",
      "Epoch 133/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0558 - accuracy: 0.9931\n",
      "Epoch 00133: val_accuracy did not improve from 0.69024\n",
      "169/169 [==============================] - 210s 1s/step - loss: 0.0559 - accuracy: 0.9930 - val_loss: 1.1710 - val_accuracy: 0.6751\n",
      "Epoch 134/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0558 - accuracy: 0.9935\n",
      "Epoch 00134: val_accuracy improved from 0.69024 to 0.70034, saving model to ./trained_weights/resnet50/CUB200/standard/model_fine_tune.134-1.2162.hdf5\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0558 - accuracy: 0.9935 - val_loss: 1.2162 - val_accuracy: 0.7003\n",
      "Epoch 135/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0531 - accuracy: 0.9939\n",
      "Epoch 00135: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0533 - accuracy: 0.9937 - val_loss: 1.2144 - val_accuracy: 0.6768\n",
      "Epoch 136/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0510 - accuracy: 0.9953\n",
      "Epoch 00136: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0511 - accuracy: 0.9952 - val_loss: 1.2590 - val_accuracy: 0.6667\n",
      "Epoch 137/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0541 - accuracy: 0.9933\n",
      "Epoch 00137: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0540 - accuracy: 0.9933 - val_loss: 1.2577 - val_accuracy: 0.6734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0534 - accuracy: 0.9935\n",
      "Epoch 00138: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0535 - accuracy: 0.9935 - val_loss: 1.2091 - val_accuracy: 0.6785\n",
      "Epoch 139/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0525 - accuracy: 0.9939\n",
      "Epoch 00139: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0524 - accuracy: 0.9939 - val_loss: 1.1800 - val_accuracy: 0.6785\n",
      "Epoch 140/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0520 - accuracy: 0.9935\n",
      "Epoch 00140: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0521 - accuracy: 0.9935 - val_loss: 1.2029 - val_accuracy: 0.6700\n",
      "Epoch 141/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0489 - accuracy: 0.9953\n",
      "Epoch 00141: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0489 - accuracy: 0.9954 - val_loss: 1.2236 - val_accuracy: 0.6818\n",
      "Epoch 142/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0523 - accuracy: 0.9931\n",
      "Epoch 00142: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 200s 1s/step - loss: 0.0523 - accuracy: 0.9931 - val_loss: 1.2409 - val_accuracy: 0.6650\n",
      "Epoch 143/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0498 - accuracy: 0.9944\n",
      "Epoch 00143: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0499 - accuracy: 0.9943 - val_loss: 1.1445 - val_accuracy: 0.6953\n",
      "Epoch 144/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0505 - accuracy: 0.9948\n",
      "Epoch 00144: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 199s 1s/step - loss: 0.0504 - accuracy: 0.9948 - val_loss: 1.2264 - val_accuracy: 0.6717\n",
      "Epoch 145/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0485 - accuracy: 0.9959\n",
      "Epoch 00145: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0484 - accuracy: 0.9959 - val_loss: 1.2250 - val_accuracy: 0.6650\n",
      "Epoch 146/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0468 - accuracy: 0.9946\n",
      "Epoch 00146: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 201s 1s/step - loss: 0.0468 - accuracy: 0.9946 - val_loss: 1.2659 - val_accuracy: 0.6650\n",
      "Epoch 147/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0500 - accuracy: 0.9952\n",
      "Epoch 00147: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 207s 1s/step - loss: 0.0500 - accuracy: 0.9952 - val_loss: 1.1751 - val_accuracy: 0.6751\n",
      "Epoch 148/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0485 - accuracy: 0.9942\n",
      "Epoch 00148: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 203s 1s/step - loss: 0.0488 - accuracy: 0.9941 - val_loss: 1.1857 - val_accuracy: 0.6970\n",
      "Epoch 149/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0447 - accuracy: 0.9959\n",
      "Epoch 00149: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0445 - accuracy: 0.9959 - val_loss: 1.2445 - val_accuracy: 0.6633\n",
      "Epoch 150/150\n",
      "168/169 [============================>.] - ETA: 1s - loss: 0.0463 - accuracy: 0.9948\n",
      "Epoch 00150: val_accuracy did not improve from 0.70034\n",
      "169/169 [==============================] - 202s 1s/step - loss: 0.0463 - accuracy: 0.9948 - val_loss: 1.2022 - val_accuracy: 0.6717\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdaWBU5dnw8f9syUz2fd8gkLAT2VQUQRQVFVxQUSsuuLVWH1+0VdvXWmur9dG2Pq19tL6tRSsWUXGBKi6AyiqyBhJIgED2fZ8ts573Q5KBACEBkkxmcv2+kDlz5pzrmoS55l7OfVSKoigIIYQQwmeovR2AEEIIIc6MFG8hhBDCx0jxFkIIIXyMFG8hhBDCx0jxFkIIIXyMFG8hhBDCx0jxFsJLysvLyc7O5o477jjpuaeeeors7GwaGxvP6JgPPvggH3300Wn32bZtG9dee223zzscDi6++GLuu+++Mzq3EGLgSPEWwosCAwM5evQoFRUVnm0Wi4Vdu3Z5Laavv/6aUaNGkZeXR1FRkdfiEEJ0T4q3EF6k0WiYO3cuq1ev9mz76quvuOyyy7rst2LFCq699lrmz5/P4sWLOXr0KAA1NTXcc889XHPNNdx///3U1dV5XlNUVMTixYu58cYbue666/jwww97FdPy5cu57LLLuPrqq3n77be7PPfhhx9yzTXXMG/ePO68806qqqq63X5iC//4x6+++ir33nsv8+bN42c/+xn19fU89NBDLFy4kNmzZ7No0SIaGhoAOHr0KIsWLfIc//PPP2fnzp3MmjULt9sNgNVq5cILLzzjngohfJYihPCKsrIyJScnR9m3b59y1VVXebbfddddSmFhoZKVlaU0NDQoW7ZsUS6//HKloaFBURRFWblypTJ37lzF7XYrDz30kPLKK68oiqIoxcXFSk5OjrJy5UrF4XAoV199tZKXl6coiqK0trYqc+fOVXbv3q18//33yjXXXHPKmA4dOqSMHTtWaWxsVHJzc5UJEyYojY2NiqIoyoEDB5Tzzz9fqaysVBRFUZYuXar86le/6nb7iec5/vFf/vIX5corr1QcDoeiKIry1ltvKW+88YaiKIridruV++67T3nzzTcVRVGU66+/Xlm2bJmiKIpSWVmpXHbZZYrRaFTmz5+vfPvtt4qiKMoHH3ygLFmy5Jx+H0L4Eq23vzwIMdSNGzcOjUZDXl4e0dHRmM1msrKyPM9v3LiRq6++mqioKABuvPFGnn/+ecrLy9myZQtPPvkkAOnp6Zx//vkAFBcXU1payi9/+UvPcdra2ti/fz+ZmZndxrJ8+XIuvfRSIiMjiYyMJCUlhffff58HH3yQrVu3cvHFF5OYmAjA3XffDcDSpUtPuX3btm2nzTsnJwettv0j6K677mLHjh0sXbqU4uJiDh06xMSJE2lubqagoICbb74ZgMTERNauXQvAj370I95//31mzpzJihUreOKJJ3p+s4XwE1K8hRgE5s+fz6pVq4iKiuK6667r8lxn1/DxFEXB6XSiUqlQjrs9QWcxdLlchIaG8umnn3qeq6+vJzQ0lD179pwyBovFwqeffkpAQACzZ88GwGQysWzZMhYvXoxGo0GlUnn2b2tro6KiotvtJ8bmcDi6nC8oKMjz88svv8zevXtZsGAB559/Pk6nE0VRPPkcf/wjR46QlJTEvHnz+NOf/sT333+PxWJh6tSpp8xLCH8kY95CDALXXXcdX3zxBZ9//vlJM8FnzJjB559/7hnPXblyJREREaSnpzNjxgxWrFgBQGVlpae1O2zYMPR6vad4V1VVce2115KXl9dtDKtXryYiIoKNGzeyfv161q9fz9q1a7FYLHzxxRecf/75bN26ldraWgDee+89Xn755W63R0VFUVlZSUNDA4qi8Nlnn3V77k2bNnHXXXdx/fXXEx0dzZYtW3C5XISEhDB27Fg++eQTTx633XYbRqMRg8HA/Pnz+eUvf8mtt956Nm+7ED5LWt5CDALx8fFkZmYSGhpKREREl+cuuugi7r77bu666y7cbjdRUVG88cYbqNVqfv3rX/OLX/yCuXPnkpCQwKhRowAICAjgtdde4/nnn+cf//gHTqeTRx99lMmTJ3fbnb18+XLuueceNBqNZ1tYWBiLFi3irbfeYuXKlfz85z/3XEIWGxvLCy+8QHx8fLfbb731VhYsWEBsbCyzZs1i3759pzz3T3/6U1566SX+/Oc/o9PpmDRpEqWlpQD88Y9/5De/+Q3vvPMOKpWK559/ntjYWKB9COH999/n+uuvP4d3Xwjfo1IUuSWoEML3KIrC3//+dyoqKvjNb37j7XCEGFDS8hZC+KTLLruMuLg4XnvtNW+HIsSAk5a3EEII4WNkwpoQQgjhY6R4CyGEED5GircQQgjhY3xmwlpdnbFPjxcZGURTk6VPjzlY+Gtu/poX+G9u/poX+G9u/poX+GZusbGhp9w+ZFveWq2m5518lL/m5q95gf/m5q95gf/m5q95gX/lNmSLtxBCCOGrpHgLIYQQPkaKtxBCCOFjpHgLIYQQPkaKtxBCCOFjpHgLIYQQPqZfi3dubi6LFi06afv69etZsGABCxcu5P333+/PEIQQQgi/02+LtPz9739n1apVGAyGLtsdDge///3v+fDDDzEYDNx2221ceumlnvvz+pJXX32FwsIDNDY20NbWRlJSMhERkfzud/992tcdOlTIpk0buOee+wcoUiGEEP6k34p3Wloar776Kk888USX7UVFRaSlpREeHg7A5MmT2bFjB3Pnzu2vUPrNI48sAeDzz1dTUlLMT37ySK9eN3JkNiNHZvdnaEIIIfxYvxXvK6+8kvLy8pO2m0wmQkOPLfcWHByMyWTq8XiRkUGnXR3nn6vz2ZxbcXbBduOiicksnje2x/1CQ/UEBQUQGxvKtm3b+MMf/oBOp+OWW25Br9fz7rvvevb985//zKFDh3jvvfd45ZVXuOKKK5g0aRJHjx4lOjqaV199FY3m3FcB6m5JPV/nr3mB/+bmr3mB/+bmr3mB/+Q24Gubh4SEYDabPY/NZnOXYt6dntajtVrsuFy9vzW5RqPqcX+rxd6rNdWNxjYsHfs2N1swm628/fY/AfjXv/7JCy/8Cb1ez0svPc+aNWuJiYnFZnNQV2ekrKyMP/3pf4mPT+AnP1nMhg3bGDdufK/zOJXY2NA+Xwt+MPDXvMB/c/PXvMB/c/NGXoqioABqlarfzmGzu0hMCKOx0dztPi63m9IaE0erWomLNJCdGoGuo9F4qhgbW9vILWogNkJPZlI4hsC+L6ndfdkY8OKdmZlJSUkJzc3NBAUFsWPHDu69995zPu4ts0dwy+wRvd6/P/9A09LSPT9HRkbxu9/9mqCgIEpKihk3bkKXfcPDI4iPTwAgLi4eu93WLzEJIURfcbrc1DZZsTlcpMaFoNX0bu5zSbWRTzcdpbG1DQC3omBuc2K0ONBqVMzKSebKaamEhwQC4HC6abM7sTvcNBlt7D5Ux86DdVjanJw/Jp5ZOUkkxQTjcLqx2JxYbU4sNidOp5tgvY6QIB2lNUa+21NJ7uEGYiL0XDE1lRkTErG0OTlY3kJpjZGGljbqW9soqzVhs7s88QZo1aQlhGK2OmhoaUOlUpGZHMaI5HBKqo3sPdKA0tEGVKtUZKdF8OhNEwjQ9f8a6gNWvFevXo3FYmHhwoU89dRT3HvvvSiKwoIFC4iPjx+oMAaEWt3+zcxkMvHmm2+wcuV/AFiy5KcoStfWvqofv2kKIbyjoKQJu9PFhMyYcz5WbbOV0moj8VFBJEQZPC1Bt1uhqsFMcbURu8PF1NHxhBh0ALSa7WzNr6aqwUxDqw2z1UFidDAZCaHERxk8nzthQQEkRAURGHCs2DhdbvKLG9l1sI7aRgtOl4LLreByu3G5FGwOF/Utbbjc7Z9lAVo1w5PCMARqaWhto9lkJzkmmHHDo8hKiQDA5nCxJa+arXnVKIA+QEN7CCqC9VpS44JpbLXxxQ+lrN1ZTkpsMI1GG61m+0nvR6BOQ6BOzbqd5azbWY5GrfLEcjrJMcHUNVtZ9tVB3v/mMHaHu8vzKhUkRAWRnRrBsMQwqhos7DvawOHyFoL1WhKig3A43ewvbmJ/cRMAwxJDuXBsAs0mOwfLmjFa7L2KpS/0a/FOSUnxXAo2b948z/bZs2cze/bs/jz1oBAcHMz48RNZvPgODAYDoaGh1NfXkZiY5O3QhBgyFEWhpMbIDwdqGZ0eyfjh0Wd8jDa7k3e+LORwRQvDk8LJSgknOETPoeJGWsw2Lp2Uwuj0SAB2H6rjfz/Kw60oXDIxiR/NGekpuJ1Ka4xU1JuJjwwiMToIo8XOwbIWyutMnDcyhuy09mPlH23krx/tw+Zobw2qVBDQcSyny92lUKxYf5jp4xJwuRW25tfgdB0rTlqNiuJqI1vzq0+ZX1RYoOe4RqsDs9XR5Xm1SoVGo0KjVqHTqslIDCUxOhitRs3h8mYKS5tRaC/koUEBHChp4kBJ00nnSY0LYeHsEYzJiDrpOYfTzeZ9VazZVkJZrYnoMD1JaREEG3QEaDUEBWoZMyySsRlRqNUqcg/Xs3FvFWarA0OgFkOgliB9+79ajRpzmwOTxUGIQcfFExIZlhiGTh/A8i8OsKOglvioILJSwxmeGEZshIGI0MCTehBuYQROl7vL9laLnSMVrUSFBZIW773xc5VyYlNwkOrrLm5/Ha8C/83NX/MC38zNrSi4XAo6bfddpt7My2ix831+DRv3VlFe1z4pVgXcfOkIrpyW2uter8bWNv7y4V5Ka00EaNXYne6T9lGrVNwyewTp8SH8cUUuajXERhioqDOTFhfCnKmphAbpsDvcrN9VTkFp82nPOTMnicykcN7+ogCVSsVV56ditDioqjdj6zi/WqUiKSaIjIQwHM7249a3tHdHx0UYuHxKCqMzoogJ06PTqalptFBcbfR0WQM0GW1U1pupabJ6vggE63WMSY9kUnYsI5LD0GjUPY5FW9ocON0KoQYdKpWKFrOd/KMNFFcb0WnUBOo0JEQHMSU7ztMz2Z3+HP/2xf9n3Y15S/H2Q/6am7/mBb6Z21trCth1sI6n75pCXIThlPt0l5elzcn3+6vZX9zEhMxoLhqfgEbdN2tGVdSZ+GTTUfYcqsflVtCoVUwcEcOEzGg+2XiEZpOd6eMSGJ4UhsniwOFqHx8NDdLhcLppaG2jsdWGy91eJAtLm2kx25mZk8SP5mRR12zlcHkLkZFBhASosdpc/L9V+bSY7ahVKlQqePSmCWSlRvDvtQfZkFt1UoxjMiKZmBlDXbOVqkYLhgANWakRRIfr+WjDESrq2idV6QM0PLJggqdVfzput8K+Iw2oVCrGDYvqsUh2xxf/FnvLF3OT4n0CX/wl9pa/5uaveYHv5VZc3cpzb+0AYGRKOE/ePqlLsXArCkcrWzHZXQyPDyE0KACA+hYr/9lSwvf7q7uMOcZHBXH55BQ0GhV2u4tGo42qBgsNrW1Mzopl/sUZnuJe32LlQEkTJosDk9VBUkww00bHodNq2FFQy5ufHcDmcJEcG8yM8YlcMDaBsOD28zcZbfzlw72U1PT+vdaoVdx86QjmTEnp0lo//nfWZLTx14/2UVzdyoPzxzJt9LF5PAfLmqlutGC02HE43UzJjiMlLqTb8zldbtZ8X8LeogZ+dEUWGQlhvY61L/ja3+KZ8MXcpHifwBd/ib3lr7n5a17g3dxKa4x8vOEIV0xNZfQpxiJPpCgKLy/fTUFpM2nxIZTWmLh5ViZzL0invsXKVz+UsaOwlmZT+2QjnVbNhWMT0GpUfLenEpdbISZcz8ycJMYPj+bbPZVszK085UQfrUaN0+UmOzWCO6/KZuPeKtbuKMN5wmWeYUE6stMi2V5QS6BOw+JrRjMlO/aUXeM2u4tdB+tQq1WEBOnQdYyPds54jg7TExWmJ6BjOCBApznlJUAn/s5cbjcmi8MzU9pXyf+zwUWK9wl88ZfYW/6am7/mBd7LLf9oI//78T7a7C4CdGp+fut5ZCa3r35otTkpqTZS1WCmttnKiORwJmXFknu4gb+s3MuEzGjuu3YMv/rHNkxWBxeOTWBrfjUut0KwXkvOyBgyUyNZs+Uodc3t46yxEXqunzGc80fHd2mp1zZbKSxpQqtVE6DVEB4cQGJMECpU/PPzA+w6WOfZNzoskCunpRETbsAQqGFvUQPf7anEYnMSF2Hg4QXjSYntvmXbV/z179Ff8wLfzE2K9wl88ZfYW/6am7/mBX2XW0m1keLqVqD9MsSxGVFEh+tP2s/tVtiwt5J3vzqISqXi8skpfLW9DEOghodvHE/e0UbW7yrHanN1eV16fChWm5O6FivP3Xs+yTHB7C1q4H8+yG3PI0LPdRcPY9roeLQaNbGxodTUtLL3SAN2h4tJWbG9via4k6IofL2jnHU7y7hkYhJzpqSedB1tm91J/tFGRqdHEqTXndHxz5a//j36a17gm7kNmkVahBBnr6zWRIhBR2ToyV2zW/Kq+OdnBbiP+z6u1aiYmZPMNRemo9OqMVoc7CtqYO3OMuqa2wjWa3lkQfvkquTYYN787AD//e/dQHtX9MxpySTHBhMVGsiGvVVs218DwKycJJJjggGYkBnNvdeMxuVWmD4u4aTirFaryBlx9tc7q1QqrpiayhVTU7vdRx+gZXJ23FmfQwhfI8X7HJztXcU6VVVVcuRIERddNKOfIxW+oL7FytLPC7h4fCIXjks46fl1O8t59+uDqFQwMTOGS3KSSI8PJTwkgHU7ylm+7hBBgVpumpVJYIAGs9XBV9vLPItZHE+nVTMzJ4m5F6R7ZopfND4Rm8PFt7sruGRiEjMmJhF4XAt3dEYUc89PI/dwPZdNTulyvIvGJ/bDOyKE6I4U73NwtncV67Rjxw9UVVVK8fYT2wtqWb+znDuuyCL5FGOubkXB4XB3Wc2qk6IovP1FoWdxC6vdyexJKZ7nPt10lFWbiwkLDiAyNJA9h+vZc7geaG9dO10K4SEBPH5LTpeZzLPOS2bj3ip2dEzkCg3SkRAdxMXjEz0zwI83e1KK57ynkhYf6tWFKYQQ7fymeH90+D/srt3X6/17s6TeeXHjuXHEtWccy2uv/Zl9+/bidru5/fZFzJw5mw8+eI+vvlqDWq0mJ2cS9933Y/79739ht9sZN24C06dffMbnEYOHze5i2VeFGC0OXli2i0duHM+o9EiqGy1s3ldFUUULJTUm2mxO5kxN5aZZmV26l7/ZWU7+0UYyk8OobWpfwrHJaEOnUVNY1syBkiZiwvX87NYc4iKDKK5uZXtBLXXNbTS0tKEP0HDX3FEnXW+t1ai59LxkLj0veaDfEiFEP/Kb4j1YbNq0gbq6Ol5//U1stjYeeOBupk49n88/X8WTTz5NdvZoPv74Q9RqNbfffidVVZVSuP3A+l3lGC0OxmREUljazJ/e30NGYhiHy1uA9pW94qOCCNZr+Wp7GUcqW/nxdWOJCtPTarHzj0/zCNCpeXDeWBwuNy8v381nW0s8xx+WGMrDN07wjHVnJIQN+PW/QojBw2+K940jrj2jVnJ/zTo8cuQwBw7s5+GHHwDA5XJRXV3F008/x/Ll71BdXcX48RNPukGJ8F1Wm5M120oxBGp56PpxlFQb+evH+zhc3sKotAhm5iQzITMaQ6AWq83J218U8MOBWp7821ZiIwyoVO1Led562UhiOlrOT985hZ0H64iNMJCREEqEj187LIToW35TvAeL9PQMpkyZxs9+9hQul4u33voHiYnJvPHGX3niif9LQEAAjz76E/bvz0OlUkkR90FOl5vtB2rRadVMHBHNup3lmKwOrp8xjCC9jtEZUfzuvgtwOF3ERQZ1ea0hUMuD88cyKj2SzfuqqKq3YLE5GZUeyeXHTQKLCtMzZ0r3s6uFEEObFO8+dskll7J79y4eeug+rFYLs2ZdhsFgICNjGPfdt4iIiEji4uIZNWoMAQEBvPvu24wcmc3s2Zd7O/QhY0dBLV9tL+OaC9OZeIpLmFotdrbmVTN2WNRJi30cKGni3a8PUlnfvva0IVCLu2NRkuOL7aku5eqkUrXft3hWTjKKomC0OEhLiaC5ydJHGQoh/J0s0uKH/DW3c83L6XLzwTdFfL2jDGi/veLCS0cwZ2r7HabMbQ6++qGMr3aUYbO70GpUXD9jOFdNS+NwRQtf/lDK7kP1qGi/65NBr+X7/BqajDZuvjSTueeney23wcpf8wL/zc1f8wLfzE0WaRFDkqIoVNSZ2Xe0gW35NZTWmkiMDmL+RcN4b/0h3lt/mD2H62kx26lusKAAYcEBzJmSwsbcKj78togvtpVi6ri/cWZyGLdfnsWwxPbJYgtmZlLTaCEhKug0UQghRN+S4i38VpvdyV8+3Nvl3skXjo1n0ZXZ6AO0jEwJ9zwfGKBhZGoEEzOjmT0phcAADVdMTWPZV4VsL6hlclYsc6amMjIlvMvNLtQqFYnRwd5ITwgxhEnxFn6hyWjjk41HGD88msnZsTicbk9hHp0eyUXjExg7LJrw4GMLk0SF6Xnm7qk0GW1EhgWiPuEOVCEGHT++bhz3XuNGp+2be00LIURfkOItfF6r2c4f3ttNVYOFjXurGJESToBWTUFpM5OyYvnJ9WM994I+kVqtOuWNO44nhVsIMdhI8RY+zWR18McVe6hqsDDrvGRazXbP7SPHD4/mwfndF24hhPBVUrzFoNdssnGwrBl3QR0TMiIJ0rf/2VptTl55fw9ltSYuPS+ZO67IQqVSUVjaxKHyFq6YmiqtZiGEX5LiLQathpY2Xv1oL6U1Js+2tLgQlizMQa/T8OcPcjlaZeSicQn8qKNwA2SnRZKdFumtsIUQot9J8RZepSgKDa1tlFQbcbjcTMmOQ6tRY7U5+Z8Pc6moMzMmI5LR6ZG0Wp18/UMpv1+2k6jQQA6WtzBtdBz3XD36pMlmQgjhz6R4C6/Zc6iet78soMVk92xbE1fKnVdm88mmo1TUmblsUgq3zxmJSqUiJiaEQK2K/2wpobbJSs6IGO67dgxqtRRuIcTQIsVbeMU3uytY9lUhOo2aKaPiyEgIpbrBwqZ9VTz/zk4AJmRGc+vlIzzd4SqVihsvySQm3EB5nYmbT7itphBCDBVSvEW/crndXWZ7u90KH204wufflxAapOPRmyYyPOnYrS0vGBvPO18dJESv7Xam+CUTkwYkdiGEGKykeIt+oSgKH37bvo74rPOSmX/RMOwOF39fvZ/CsmbiIg08dsvEk+66NSYjihfuPx+gy0pmQgghjpHiLfrFJxuPsmZbKSoVrN1RzpZ91ahUYG5zMjkrlrvmjiLEoDvla6VoCyHE6UnxFn3us63FrN5STFyEgZ/dmsPOg3Ws2lyM0+nmzquymTkxSQq0EEKcAyneos/UN1tZvu4Quw/VExUWyM9uyyEm3MCV09KYMSEJh8vdZW1xIYQQZ0eKtzhniqLw5Q9lfLLxCHanm6zUCBZfM5qYcINnn85V0YQQQpw7+UQVZ+SHAzV8u7uCqy9IZ9zwaABWby7mk01HCQsO4K65I7hgTLx0iwshRD+S4i16xWixs+yrg2wvqAWgsLSZa6dnoA/Q8Mmmo8SE6/nFHZOJDA30cqRCCOH/pHiLHjUZbfz27e00m+yMSAnnyqmprFh/mNVbigGIDA3k57edJ4VbCCEGiBRvcVqKovCvLwpoNtm5dno61188HLVaxaj0SN5aU0BZrYlHb5pAbISh54MJIYToE1K8xWlt3ldNblEDYzIiuWHGcM9YdrBex09vGO/l6IQQYmiShaFFtxpb21i+7iD6AA33zB0tk9CEEGKQkOItTqm+xcprn+Rhtbm49bKRRIfrvR2SEEKIDtJtLrpQFIVNe6tYvu4QbXYXF4yNZ8aERG+HJYQQ4jhSvIWH0+Vm6ecFbM2vxhCo4d5rRjN9XIJ0lwshxCAjxVsAYHO4eP2TPPYWNTAsMYyHrh8nXeVCCDFISfEe4uwOF4VlzazeUszh8hbGDovipzeMQx8gfxpCCDFYySf0EOVWFP71RSFb86txON0ATB0Vx/3zxqDVyDxGIYQYzKR4D1H7jzayIbeSmHA9U0fFMW54NKPSImR8WwghfIAU7yHqqx1lAPz0hvGkJ4R6ORohhBBnQvpH/ZTd4eJfXxayflc5Tpe7y3OV9WbyjjSSlRIuhVsIIXyQtLz91Pf722/dCfDFtlKuu3gYF45LQK1SsXZnOQBzpqZ6M0QhhBBnSVrefmrzvipUwCUTk2g22XjzswP897u7OFzewpZ9VcSE6zlvZKy3wxRCCHEWpOXthyrrTBwqb2F0eiR3zx3F/IsyeG/dIXYU1vHCsp0AzJ6Uglotk9OEEMIXScvbD63rmIx2cceyplFheh66YTwP3zieiJAAQoN0XDJRljwVQghfJS1vP+N2K6zfXoohUMOkrK7d4pOyYhk/PAq7002QXuelCIUQQpyrfmt5u91unnnmGRYuXMiiRYsoKSnp8vwnn3zCvHnzuP322/nggw/6K4whZ39JI/UtbUwdFU+gTnPS8zqthmAp3EII4dP6rXivXbsWu93OihUrePzxx3nxxRc9zzU2NvLnP/+Zd955h2XLlrF69WrKy8v7K5QhQ1EUvttdCRzrMhdCCOF/+q1479y5kxkzZgCQk5NDXl6e57ny8nJGjRpFREQEarWa8ePHk5ub21+hDAm1TRb+8N4edh6sIyMxjMykMG+HJIQQop/025i3yWQiJCTE81ij0eB0OtFqtaSnp3P48GHq6+sJDg5m69atZGRknPZ4kZFBaLUndwOfi9hY31+gpNVsZ9XGIj7+tgi7w8XUMfE8tGAiMREGb4fWL/zhd9Ydf83NX/MC/83NX/MC/8mt34p3SEgIZrPZ89jtdqPVtp8uPDycX/ziFzzyyCMkJCQwduxYIiMjT3u8piZLn8YXGxtKXZ2xT4/Z39yKwscbjtBsshEaFIDd4WLzvmpsDhdhQToWXz2KqaPiiIkw+FxuveGLv7Pe8tfc/DUv8N/c/DUv8M3cuvuy0W/Fe9KkSXzzzTdcffXV7Nmzh6ysLM9zTqeT3Nxc3n33XZxOJ/fcc2KQzdYAACAASURBVA9Llizpr1D8RmFpM59t7TrxLyIkgBsuGc7MiUkEBvRtz4QQQojBqd+K95w5c9i8eTO33noriqLwwgsvsHr1aiwWCwsXLkSn03HjjTcSGBjIPffcQ1RUVH+F4je27KsC4MfXjSU6TI/d4WJESgQ6rVyuL4QQQ0m/FW+1Ws1zzz3XZVtmZqbn54cffpiHH364v07vd9rsTnYU1hETrmfKqDjUcutOIYQYsqTJNkhtza/mF29spaLOBMDOwjpsDhfTO24uIoQQYuiS4j0I2R0u3v/mMDVNVl77JI82u5PNHV3m08fL9dtCCDHUSfEehL7bU0mLyU5kaCBVDRZe/ySfgtJmslLCifPTS8CEEEL0nhTvQcbucPH59yUEBmj41V1TGJYYxr4jDYC0uoUQQrST4j3IfLu7ghazncsnpxAREshPrh9LsF5LoE7D1FFx3g5PCCHEICB3FRtEbA4Xn28rRR+g4cppaQDEhBv45aLJ2BwuDIHy6xJCCCHFe1D5dncFrWY7105PJ8Rw7M5fidHBXoxKCCHEYCPd5oOEze5izfclGAI1XDE1zdvhCCGEGMSkeA8S3+yuoNXi4PLJqV1a3UIIIcSJpHgPAja7izXbOlrd01K9HY4QQohBTor3ILB+dzlGi4M5U1IJ1kurWwghxOnJhDUvsjtcfLm9jM+2FmMI1HLFVGl1CyGE6JkUby8pLG3i7//ZT2OrjdAgHXddNYogaXULIYToBSneXqAoCv/6spAWk52rL0jnmgvT5RpuIYQQvSYVwwvKak1UNViYkh3LTbMye36BEEIIcRyZsOYF3++vAeD8MQlejkQIIYQvkuI9wNyKwrb9NRgCtUzIjPJ2OEIIIXyQFO8BdqismSajjclZsei0Gm+HI4QQwgdJ8R5g2w7UAnD+2HgvRyKEEMJXSfEeQE6Xm+0HaggLDmB0WqS3wxFCCOGjZLZ5P1MUhXU7yympMVLdaMHc5uTyKSmo1SpvhyaEEMJHSfHuZ0WVrfx77SHP4+iwQGbmJHsxIiGEEL5Oinc/yzvSAMDdc0dx4dh4maQmhBDinMmYdz/LP9qIWqViSnacFG4hhBB9Qop3PzK3OThS1crw5DCC9NLJIYQQom9I8e5H+4ubUBQYN0wWYxFCCNF3pHj3o87x7nHDor0ciRBCCH8ixbsPuRUFk9UBtF8ilne0kWC9loyEUC9HJoQQwp/IQGwf+vCbIr7cXsqdV2YzIiWCJqONaaPj5JpuIYQQfUqKdx+x2V18l1uBosDbXxQyPCkMgLEy3i2EEKKPSbd5H9lRWIvV5uLCsfFEhQVypLIVgLEZUryFEEL0LWl595ENuZWogBtmDEelUvE/H+YSatARFab3dmhCCCH8jBTvPlDVYOZQeQtjMiKJiTAA8Jt7poEMdQshhOgHUrz7wMa9VQBcMjHJs00mqQkhhOgvMuZ9jpwuN1v2VRGs13LeyFhvhyOEEGIIkOJ9jvKPNtJqcXDhuAR0Wnk7hRBC9D+pNucov7gRgEnS6hZCCDFApHifo4KSZnRaNZnJYd4ORQghxBAhxfscGC12yutMjEgOl9t9CiGEGDBSvM9BYWkzAKPSIrwciRBCiKFEivc5KChtAmBUeqSXIxFCCDGUSPE+BwWlzQTo1AxLlPFuIYQQA0eK9xnYc6iez7YWoygKLWY7lfVmRqZEoNXI2yiEEGLgyAprZ2Dld0VU1JuxO9wkxQQDMt4thBBi4Enx7iWX2011owWA1VuKiY8KAmS8WwghxMCT/t5eqm2y4nIrZKdGEKzXUtNoQR+gISMh1NuhCSGEGGKkePdSZX17q3viiBh+esN4NGoV44ZFoVHLWyiEEGJgSbd5L1U2mAFIigliVHokv3/wAoL1Oi9HJYQQYiiS4t1LVfUdxTu6faJaTLjBm+EIIYQYwqTPt5cq680E6NREheu9HYoQQoghTop3L7jdClWNFhKjglGrVN4ORwghxBDXb8Xb7XbzzDPPsHDhQhYtWkRJSUmX51etWsUNN9zAggUL+Pe//91fYfSJ+hYrDqebpJggb4cihBBC9Fy86+rqzurAa9euxW63s2LFCh5//HFefPHFLs+/9NJLLF26lOXLl7N06VJaWlrO6jwDoXOmeefCLEIIIYQ39Vi877jjDh544AHWrFmD3W7v9YF37tzJjBkzAMjJySEvL6/L89nZ2RiNRux2O4qioBrE3dGemebRUryFEEJ4X4+zzb/88kt27NjBxx9/zB/+8AdmzpzJDTfcwPjx40/7OpPJREhIiOexRqPB6XSi1bafcuTIkSxYsACDwcCcOXMICzv9zT0iI4PQ9vE9s2Nje7fASqOp/UvLuKw4YmNDeth7cOhtbr7GX/MC/83NX/MC/83NX/MC/8mtV5eKTZkyhXHjxvHFF1/wyiuvsH79eqKionjmmWfIyck55WtCQkIwm82ex26321O4CwoK+Pbbb1m3bh1BQUH8/Oc/Z82aNcydO7fbGJqaLGeSV49iY0OpqzP2at8j5c1oNWrUblevX+NNZ5KbL/HXvMB/c/PXvMB/c/PXvMA3c+vuy0aPxXvr1q188sknbNmyhZkzZ/LKK68wadIkCgsLuf/++9mwYcMpXzdp0iS++eYbrr76avbs2UNWVpbnudDQUPR6PYGBgWg0GqKiomhtbT3L1PqXW1GoarCQEBUkq6kJIYQYFHos3n/961+56aabePbZZzEYji1Mkp2dzeLFi7t93Zw5c9i8eTO33noriqLwwgsvsHr1aiwWCwsXLmThwoXcfvvt6HQ60tLSuOGGG/omoz7W2NqGzeGSmeZCCCEGjR6L9xtvvMGnn36KwWCgpqaG9957jwceeACDwcDdd9/d7evUajXPPfdcl22ZmZmen2+77TZuu+22s498gMhMcyGEEINNj/3AP/vZz6itrQUgODgYt9vNE0880e+BDRb5RxsBmWkuhBBi8OixeFdWVrJkyRKgfRLakiVLKC0t7ffABoOt+dV8vaOMmHA9YzKivB2OEEIIAfSieKtUKgoLCz2Pi4qKPLPG/VlhaRNLPz+AIVDLozdPJEjv/zkLIYTwDT1WpCeffJLFixcTHx8PQFNTEy+99FK/B+ZNRoudv360D0WBn94wjmQZ7xZCCDGI9Fi8p0+fzjfffMPBgwfRarUMHz6cgICAgYjNawpLmzG3Obl2eoZ0lwshhBh0eizexcXFLFu2DIvFgqIouN1uysvLeffddwciPq8orTUBkJUa7uVIhBBCiJP1OOb92GOPERYWxoEDBxg9ejSVlZWMHDlyIGLzmrKa9hV4UuP8Yxk9IYQQ/qXHlrfD4eC//uu/cDqdjBkzhltuuYUFCxYMRGxeU1ZnIjwkgPBg/x4eEEII4Zt6bHkbDAbsdjsZGRnk5+ej1+sHIi6vMVkdNLbaSI3zjRuQCCGEGHp6LN7z58/nxz/+MbNmzWLZsmXcd999npnn/qisY7w7TbrMhRBCDFI9dptPmTKF66+/npCQEN555x327dvHRRddNBCxeUXneHdavLS8hRBCDE49tryXLFniuS93QkICc+bMISjIf2/S0TnTXLrNhRBCDFY9trxHjBjBX//6VyZOnNhlvHvq1Kn9Gpi3lNWaCNCqiY/03y8oQgghfFuPxbu5uZlt27axbds2zzaVSsW//vWvfg3MG5wuN5X1ZtITQlGrVd4ORwghhDilHov3O++8MxBxDAqV9WZcbkW6zIUQQgxqPRbvRYsWoVKd3Ar1x5b3sZnmUryFEEIMXj0W70ceecTzs9PpZN26dYSFhfVrUN5SWtMxWS1eLhMTQggxePVYvKdNm9bl8fTp07n55pt59NFH+y0obymrNaICUmLlLmJCCCEGrx6Ld2VlpednRVE4fPgwzc3N/RqUN7jcbkpqTMRFGtAHyL27hRBCDF49Vqk77rjD87NKpSIqKoqnn366X4PyhqKKVqw2J+ePjvN2KEIIIcRp9Vi8169fj8PhQKfT4XA4cDgcfrlIy96iBgAmjIjxciRCCCHE6fW4wtqaNWu48cYbAaiqqmLu3LmsXbu23wMbaLlF9ei0akanR3o7FCGEEOK0eizer732GkuXLgUgLS2Njz76iFdffbXfAxtI9c1WKurMjE6PJFCn8XY4QgghxGn1WLwdDgcxMce6kqOjo1EUpV+DGmi5HV3mE6XLXAghhA/occx78uTJPPbYY8ybNw+VSsVnn31GTk7OQMQ2YHKL6gGYmBnt5UiEEEKInvVYvH/961/zzjvvsGLFCrRaLVOnTuW2224biNgGRJvdSUFJEymxIUSF6Xt+gRBCCOFlPRZvh8OBXq/nb3/7GzU1Nbz33nu4XK6BiG1AHChuwulSmDhCWt1CCCF8Q49j3o8//ji1tbUABAcH43a7eeKJJ/o9sIGy70jHeHemjHcLIYTwDT0W78rKSpYsWQJASEgIS5YsobS0tN8DGyj1rW0AcicxIYQQPqPH4q1SqSgsLPQ8LioqQqv1n+VDzVYHWo2aAF2Pb4UQQggxKPRYhZ988kkWL15MfHw8KpWKxsZGXn755YGIbUCYrA5CDNpT3vZUCCGEGIx6bG5Onz6db775hmeffZZLL72UuLg47r///oGIbUC0F2+dt8MQQggheq3HlndZWRnvv/8+K1eupLW1lR//+Me8/vrrAxFbv3O63FhtLineQgghfEq3Le+vv/6ae++9l5tvvpnm5mZefvll4uLiePjhh4mKihrIGPuNuc0JIMVbCCGET+m25f3II48wd+5cVqxYQXp6OoDfjQubrA5AircQQgjf0m3xXrVqFR999BG33347ycnJXHPNNX61OAu0zzQHCJbiLYQQwod0222elZXFU089xXfffccDDzzAtm3bqK+v54EHHuC7774byBj7jbS8hRBC+KIeZ5trtVouv/xyXnvtNTZs2MAFF1zAH//4x4GIrd9J8RZCCOGLzmhlkqioKBYvXsyqVav6K54BJd3mQgghfNGQXlbMKC1vIYQQPmhIF2/pNhdCCOGLhnTxNkvxFkII4YOGdPE2WR2oVBCk958brQghhPB/Q754B+t1qP1s8RkhhBD+bUgXb7PVITPNhRBC+JwhW7wVRcHc5iTEIF3mQgghfMuQLd6WNicut0KIXlreQgghfMuQLd6tZjsgM82FEEL4niFbvI2W9uItY95CCCF8zZAt3p0t79AgKd5CCCF8y5At3tLyFkII4av6baq12+3m2WefpbCwkICAAH73u9+Rnp4OQF1dHY899phn3wMHDvD4449z22239Vc4JzF2jnnLhDUhhBA+pt+K99q1a7Hb7axYsYI9e/bw4osv8vrrrwMQGxvLO++8A8Du3bt55ZVXuOWWW/orlFNqtciENSGEEL6p37rNd+7cyYwZMwDIyckhLy/vpH0UReG3v/0tzz77LBqNpr9COSWjzDYXQohBy+Kw8ustL/Jd+RZvhzIo9VvL22QyERIS4nms0WhwOp1otcdOuX79ekaOHMnw4cN7PF5kZBBabd8VeKOl/aYkaSkRRIcb+uy4g0VsbKi3Q+gX/poX+G9u/poX+G9ugyGvPVWl1Lc1kteUz03nXdlnxx0MufWFfiveISEhmM1mz2O3292lcAOsWrWKO++8s1fHa2qy9Gl8rWYbADaLnTq7s0+P7W2xsaHU1Rm9HUaf89e8wH9z89e8wH9zGyx55VcUAXCksYya2hbUqnPvKO6L3NyKm5e2/4WsyBHcOPLac46pJ9192ei3bvNJkyaxYcMGAPbs2UNWVtZJ++Tn5zNp0qT+CuG0jGYHgToNOu2QnXAvhBCDVoWpCoA2Vxv11sZ+Ocfa0u94ZsuLWJ1tvX5NlbmGMlMleQ0F/RJTb/Vby3vOnDls3ryZW2+9FUVReOGFF1i9ejUWi4WFCxfS2NhIcHAwKi/d0avVYpd1zYUQYpDqLN4AZcYK4oJi+vwcefUHaGhrpKS1jFFRI3v1mpLWMgAa2hpxK+4+6RE4G/1WvdRqNc8991yXbZmZmZ6fo6Ki+PTTT/vr9D0yWuwkRAZ57fxCCCFOzeFyUGOpQ6fW4nA7KTNWMDl+Yp+fp9ZSB7R/Oeht8S7uKN5Ot5MWWyuR+og+j6s3hmSfscPpwmZ3SctbCCHO0pqja3lmy++xu+x9fuwqSw1uxc3E2HFAe3Hta1ZnGy124xkfv7PlDVBnbejzuHprSBZvk7V9gpqsriaEEGcntz6fhrYmqsw1Pe7774KVvPDDKyiK0qtjVxjbu8xHRAwjxhBNmami16/trc5WN/S+eNtddirN1Z7H9VK8B5bJ2n6ZmFzjLYYyt+L2dgjCC/ri9+5yuzxFu+a4ItidvfX5VJiqaGhr6tXxK8ztxTs5JInU0GTMDgtNtuazD/gUjo+71lrfq0lrZcZK3Iqb1JAkQFreA06KtxjqHG4nz33/MssLVno7FDGAVhR+zGPfPc0/894lr/4ALrfrrI5Ta63H6W7vweypeLfajRjtJgAqTJW9On6FsQoVKpKCE0gLSQagtI+7zjvj7izE5b04fklrKQCT43MAaXkPuM7iLd3mYqg60FBInbXB65e7iIGVW5ePw+1kZ20ur+9dyt/2vXVWLfEK47Ei3FPx7uwCByg/bgZ5dxRFocJURYwhCr02kNSw9uJ9ruPeLrerS9d7Z9ydhbg3x++crDYxdixatVaK90DTB7Sv1JYYLbPNxdC0szYXgGZbC2ZH3y6ANFg5XI6Txk3dirvPhg8URfHKUISiKDhcjh73a7a10GJvZULMWH4+5WFGRgxnf0MhG8q3nvE5jy/CtT0Vb/OxfSt7UbybbS2YnRaSO1rEqSHnXrzNDgv/d8vzvLPnWE9TraWOALWO8TFjACg19twrUNJaRrA2iFhDDDH6KOr66frz3hiSxXv88Gj+9esrGTcs2tuhCDHg7C47e+v3ex5X9OID1deVtJbxi82/4+PDn3XZ/kXxOn624Rk2V2475wlR35Zv5rHvnmZ96YY+n1x1Ou8d/JinNv2WVvvpVw7rnCWdEZZKRlga94z9EcG6ID4p+owac+0ZnbPzbyZaH0mtpf60X1rKO1reapX6pJb3qb50dB47JSQRgJCAYCIDI3os3lanlaa2ZpramrE4rF2e+6F6F0a7iU2l2z1fsmot9cQFxRIXFEOgJoAy0+mPb7KbqW9rJC0sBZVKRYwhGqvT6rUvv0OyeANEhum9HYIQXpHXUIDdZSfO0L7ohb8X7xpzLa/l/hOr00r+CcMEe+rysLns/LtgJf/Mf/ekD/0zsaduHw63k5WH/8Pre5dSY66lsa2JprbmfmuR59bls6nie9pcbeyr23/afTu7fNPDUgEIDwzltuwFONxO3j6wggZrE41tTRhtppNea3fZu+RQYaokIjCcjLA0HG4HTW0t3Z63wlRJgCaAzPAM6q0NtHVMDDvSUsJjG37Fzpo9J+zf/veY1FG8AdJCk2m1G2mxtZ7yHLtq9/LUxud4essLPL3lBZ7a9Jzny4qiKGyp/AGA5rZWKs3VNLW14HA7iA+KRa1SkxKSTI25FttpLnsrMR778gMQa2hv/Hmr61wudBZiCHC6nWjV7f/dOz8srx1+Bf/M/zflvZxEdC5cbhduFHTqU3/kKIqC3e0gUBPQp+dttrXw6p5/YHKYCdYFUWOpw+q0YtAasLnsVJqqSQpOQK/Vs6t2Ly22Vh6b/NAZn8etuCk1VhBjiCZGH0V+Q0GXLwqXJF/IwuwbuuzvdDsJOId8jXYTywtWolFpcCku9tbv56Lk87vdv8RTvFM8286LG8/U+Elsr9nFM1t/79k+MmI40xImo1Nr+aF6FwcaDzIr9SJuGjkfo91Ei93IuOhRxAfFAu1d0NGGyJPO6XA7qbbUkh6aQmpoMoeaj1BprmZ4eAbbq3fhVtysK93oGXeGk1veAKmhyeTW51PUUsykuAldzlHQeIi38pejU2s5L2YCTsXF7tq9fHz4Mx4970FKjGVUmqsxaA1YnVYONB4kObj92HEd8aeFJVPUcpQKUyXDwzNO+f6d+OUnpqN411kbPNsG0pBteQvh63q7HvOu2r38n2//Lx8d+g8mh5m8hgISguLIiR2PVq0dkJb3P/Pf5bff/wGH+9Q3AXqv8COe2vQczbbuW3BnY9mBD2iyNTNv+FVMT5yGgkJpa3v3aJmxAgWFUVEj+T/nPcioyJEUtRSf1dhqtbkWu8vOyIjh/DTnXhZm3cC0hElMS5hEZGAEGyu+p9LUfn2woii8mfcuT2954ay7XBVFYXnhRxgdJuZnXkVScAKFTYe6bTm6FTclreXEB8Vh0Ha9i+LC7Ou4PG2mJ95RMZkcaj7CuwUf8Nb+5exvLEStUrO58gfanG2ev5fkkCRP8e5u0lq1uRa34iY5JJHkjmJcbqxCURTP0E2JsYzyjvFml9vFkZYSDFo9UfpjXwY6F2v5rnxzl+OXtJbx//a9jQp4cMJd3D32Nu4bdwfjokdzqPkIeQ0H2FzR3uq+aeQ8oL3Yd8bbGX9qL2a0H246AhxfvKMAaXkLIXqhxWZke80ufqjeRYWpigfG3+n5YOvOzppcFBTWlW3gh5pdON1OJsdPRKPWkBQcT6W5BpfbhUZ9Zrfc7W3rscVmJLcuHwWFvXX5Jy1zmVuXz6bKbZ6fZ6ZM9zxnc9nRqbVntX60oigcbSkhPiiOK9MvZU9dHtD+gZ8dNYLijst+MsJS0ag1zEyZTkHTIbZUbmdhdvIZnav4uFatWqXmkpQLuYQLAdhXv5+/7X2LT4vW8JOJ97C1agd76vYBsL16N7NSLzrj3AqbDpNbl8eIiGHMTp2BxWHly5L1FDQeYmLs2JP2r7XU0+ZqY2LYyc8ZtAZuGHGN53FsbCgFpSXsrMnFoTiZEjeRXbV7+c/Rr9hZm0ubs/2OjMkhicQGtbc+uyvelZ5Cf6x4V5gqKTdV0mxrISIwnGZbC1uqtnNL6HVsqdpOk62Zi5Mv6HLfi6SQBMZEZbO/sZCjLaUMC0/DaDfx+t6l2F0O7ht3B1mRIzz7Xz/iavIbCvj48Gc021qI0kcyLWES31Vu5nDzESIDw4Hjinfo6SfFVZiqONhcRGb4MMIC2u/y1dltXmdpL96KomBz2dBrB2ZIVlreQniZw+3E2YvrbW0uO7/d9jIfH/6M6o4JRps7xvK6oygKh5uPEBEYzgUJUzzX206Oay+gSSGJON1Oaq31p4zrdNcBf3L4c57e8gItttNPlNpdtxeF9glcW06It7PrV6tq/+Kw77iJdPXWRn6x6TnWFK877fE7WZ1tXSaKNdtaaHPZSAqOR6VSecYqizvGLo91I6cBMDZ6FOEBoWyv2YW9F7O3j9d5/e+puk/HRY9mZMRw8hoOsLVqBx8e+hS9Rt/Rmj27iXKHm9tbgVekz0atUjM+ZjTQ9f3rGl/XLt+eRBuiuCLjUq4ZNof44DguSJyCChWbK3/o0q0dZzjWbX4qnUMyySFJJATHo1apqTBVe+K8LnMuYQGhngllnx39igBNAFdnzDnpWJenzQTa7wTm6Xmwm7gucy45ceO77JsYHM/0pKnUWOqwuexcmDgFtUrNhITRnkvlAM/NTuKDYtGpdRxsKjrl735dafsdMuekz/RsizJEoULlWahl1ZEveGrTc5js5pNe3x+keIsh6VwmJvUlRVH4/Q//w6vfL+1x32pzDVZnGxNixvLCxU+TFprMgcaDmBzdf1hUmWswOcxkRWayaMwt3D/+Tn406ibig+MASOm4HKfiFJfJvLbnTX65+Xfk1R845bGLWooxOywndWWeaGdNLipUJAbHU9B0yHN7R0VReK+j63de5lWkhiRxsKnIMxywpfIHbC47u2pye3xvtlXt5ImNz3b5clBtaf+Ck9CRa0RgOGEBoZ5CVtJaRogumOiO7lmNWsMFiVOxOts8LePutDnbcLqODQGUtJahVWs9Y6nHU6lUnpbtsgPvY3PZuSXrOibEjKXSXE2psbzH/E7U2UJM62gxpoelEqoLYV/9/lNOjitu7TrZ6kxF6iMYG51NSWsZeQ0H0Kl1xAbFoNcGEhEY7nmvT3Ssiz0BnVpLQlAcFeYq9tblo1FpGB8zmgsSp2B1Wvnf3Dcx2k1cnjaT8MCT72GdFZnZPvZdl8dnR78mty6PkRHDuSztklOe++phcwhQ61Ch4sLEqQDkJLRfFmZz2QkPCPO0kjVqDRclTaOxrYnVR77ocpymtma21+wmPiiOsdGjPNt1ai0RgeHUWxuoszSwrnQDYQFhGKTlLUT/yKs/wM83/prdtaf/gO6t0xXPnhgdJmosteyrKeixBdbZNTkqaiQhumAmxU3ErbjJ7egOPpVDHS20kRHDAciJHcf0pGme5z3jkCeMe1ebaznYXITJYeb1vUv58NCqk8arO8f6NlZs9XSlnqje3MiRlmJGRgxnTtosAL6v2g60X1q1py6PzPD2rt/xMWNwKS4ONB7E5XZ59qu21NLU1v3SmHn1B1hW8AFuxU1B06EuOQAkBMcD7UU0PSyVZlsL5cZKGtqaSA9L7dI92/khv7mjG9+tuD29FZ2MdhO/3vrf/M/3bwJgdzmoMFeTGpLc7dBDeliqp7djYsxYpiVM8vweOs91JsqMFUQEhhMaEALgaX2bHGZPoT5eSWsZWpWmywzuM9UZr9lhISk4wTOUERcUS7Ot5aTxds9iK/ooT5FMDknE7rJTZqpkZMRwDFqD5z0vM1YQFhDKZamnLsYqlYrL02aioLCmeC2BmgAWjb6l2yGViMBwFo/7EYtG3+K589eomEzPpMnOLvNO12XOJS4ohm/KNnGoqciz/ZvyTbgVN5enXXLSuWIN0bTYW1l5eDUuxcV1mVed8fDT2ZLiLYacH6p3AbCmeO05X4+7r34/T218js0VZ/4BDO1jkdB+DWlP1+nWnjDJZlJHMdh5mpZp54fQyIjMUz5/bByya/HuoxaQ/gAAIABJREFU7Fa8Iv1S4oPi+KZsE/858qXneauzzfOlxeK0srWj0J5oS9lOACbHT+S8uPHoNXq2Vu1gW9VOPjy0ivCAUO4ac2t78YltbxXtrdtPfkMBLXYjwbr2hZQONB485fGLmov5R94yNCoNAWpdlzHL6o61txOC4jzbOlueGyraFyY5sRs5Niia7MgRHG4+yvsHP+WZLS/yi02/7fIFaU3xOkwOMz+U76HSVE25qQK34u6xVXtT1nyuSp/N7aNvQqVSMTpqJJGBEeyo2XPSlx+34qbGUkeVuYYqcw1W57GeohZbKy12o2ectlPnYiMndp073E7KTZUkhyZ1O9u/N8ZFj/Z8WUg+7kvA8TPOj9dib/3/7d13eFzllfjx7xTNqI+61Xt3bxhj3MAGmxIwGBeBqSE4EEgIYdmFn50snUBMIAlJyG4WYgKbBNgQEkKowWCwYxv3XmTJlmT1NqpT7u+PmXs1I40syZItj3Q+z8MDU3TnfW0x5563nBerrYWksETtOc+fU/++44JjyHX/fl6ZsZBAo7nXNkyOHa+NlCzN+QbR7kVjvRkfU8iMhKnaY5PRRLb7RjYuxDt4mwwmbi5YDsD6/X/kZHM5J5rL2Fi2mXBTGNPjp/S4vjrnv7tmH2lhKdr/k+eCBG/RL2329jOug3w+sTvt7Ks7CLgC1oG6w338RO8cTgdvH/krCgr/KPnkjP58qlu75po9TyvypfsK2eigSDItaRyqP+pz3llRFA6757tjevmSCwkIJsJs8ao5rSgK2yp3EqA3cnnafP5t2r3odXqONhzX3qNm3VPjJhKgD+DTE5/77P9XpdvQ6/RMih2PyWBievxkGjoa+d3+PxBkDOKeSd/UthilhCYRYbawr/YAn5dvAmBZ7rWA7+Dd0NHIr3e9gkNx8M1xN5FuSaO6rVYLdKdaq9Ch07YDQVewVm/gfAVcNcP87ORG2uxtGPQGXj/wFs2dVqpaq/m87CttS9tHpZ/12ELUm3BTGFdnLSI0IARwZcszE6e7pgaqdnm9973ij3h007M8vvkn7n/WaX++6g1K9+CdH5VDgN7oWhzocVNaZi3HoTjOeMhcZdAbuDB+GgDJHgG5txXnZe7V9Ukh8dpzXsE7ulD77xtyr+HarCu0LPx0bbilcCVLc77R53t7o57b7XlTp8qwpHFZ2nxq2+t5astPeXrLC7Q7OpiXPMvnjY+6XQxgSfYVXqM4Z5sEb9Gn5k4r/2/jk7xX/OFwN2XQjjQU02ZvJzsiA3B9+Z6pjeX/oqq1hkCDmdr2erb3MU/qi+dCsb6OVqx0l3O0mMO156bETURBYXv1rh7vV+e7cyKyTvulkhyaQKPH4RHlLaeobK1iXHQBgcZAAo1mYoNiONVapQUFdZFOengKMxOm+ex/VWsNR+tLyI/MIdTkClgXJbq+cAP0RlZPuNXry1yn0zE+ppAWeyv7ag+SGpbM1LiJRJojOFh3xGsuV1EUfn/gTVrsrVyfczXjYgq0+V9129Gpliqig6IwGbrOMEgLc+1xtjlt7sc9A9rk2PFck7WY28YW8dTFa7gmazFWWwuvH3iLd46+j1NxclPBMpLC4tlauYNd1Xtd1zqD4DjTvRDskxMbtGmJ+vYGPir9J2GmUGYnzSQ1LJmGjkaONZYArpOtoGu+W2UymBgbnU9la5XXjeCRhuJe+zpQl6XN56qMy5jhkYXG9RK897n3uXtm3mrQTwpN8NoXnhgaz8K0ef0acs6KSGd+ysVnHChnJ83k6szLudAjI/d0RcYCrs26gtlJM5mdNJOFqfOYl3Kxz/eqhY7GxxSQE+l7dOtskeAt+nS44RjtjnaONh4f7qYMmjqkuDh9AbkRWRyoP3xG+3rb7e28V/whZoOJuyfegQ6dtgp2IDwz7wpr75m3Ws4xNjjGa95tStwEdOh8LupSVyTnRGactg1qDWl16Hyru4iLZ+GMhJA42uxtNLkDfI17e0xMUDSXpMxBh47PTn7pdd0d7jUFUzy2hqWGJbM8dwnfmXSndgPlSR36BVcGrA4vt9hbvf6eNpZvZl/tQQqicpmb5Npa5rndx9rZgtXW0iO7Cg4I1lYYxwRGaTcVngx6A5elzWfamEmYDCbmJc8iJyKTXTV72VG9m4zwVCbHjufq/AU4FAeHG44RbAzStg4NRFRgJBclXkBFS6V2c/zX4g+wOe1ck7mYFXlLuCrzcqDrd1ct49k984auvzN1KkVRFDZXbMOgM1AYnTfg9nUXHBDE4owFXtuhfA2bH2ko5rOTXxITGEVBVK72fLgpjJsLlnNj/tJBt+VMmQ0mFqVf2mO/u8qoN7IwbR4r8pawIm8J12Zf0WvxoLExBVyduYiVedefzSb7JMFb9OmYO2hXtfbcTuRPFEVhd80+goyB5ERksiCta+vJQH1U+hnNNisLU+eRFZHOxNhxnGgu43DD0b5/2ENVWw0mfQAGvYHy02TeDR1d5Rw9Wczh5ERkcrTxeI9FXYe0xWqnzwjUbOj94x/T2NHMtsqdmN1ZnEoNguo8spp5xwRFExscTXp4KsebSrXSl+DaiwwwzuM6AHOSZ/oM3OBaUWw2mDDpA5jmDkT57i9/dei8pq2Wt478lSBjEDfmL9UyMDWYlTaX91hp7iktzLU1rL+Zsl6nZ1XBMgINrrnYJdlXodPpmJ12gbbnt/vCt4G4LvtKogOj+LDkn3xRtonNFdtIDInX5mrVP5NdNa7h8BPNZYSZQrGYwntca1x0PiaDiW2VO1AUheNNrupiE2LHavPVQy0qMIIAfQD7aw9xoO4w7fYO1u/7AwA3F67oEfhmJEwdlopkZ0OA3sii9Eu8RsPOFQneok/HGlzDdY2dTaet/Xuu2By2Phd3+VLecora9noKo/Iw6A0URuWRGBLP11W7BrQ3s6q1ho9KN2AxhXGJe5uKugf1wwHcCCiKQnVbLbHBMSSGjaGi5VSvNbC7z3d7UjNbdZGZeu3D9UeJMFv6zAjHxxQyLjqfww3HeGzzs9S21zEhZqzXcLO6tUwNijUewRsgJzITp+LkqHto11Up6zgp4QkDChoBeiO3j72RO8bdpG25yY/KQYeO/XWHKGk6wc+2/4ZO93YrdRUxuFb+qgdMaIvV3CvNPaVbXIFjIHPA0UFRrJ5wKzcVLCMrIt3VVkMA85JnDfha3QUaA1lVsAyANw6+jYLCtdlXaCMsAXojBVF5VLfVcqyxhLr2elLCknzeLJgMJibEFFLTXkdp80lt69yshAt6vHeo6HV6luZcTZujnZ/v+C/Wff0SNe11LEidq/1ZiaEnwVuclmtbR9dwZfV5kH3//sCb/McXj/GTbS/xedmmXrcpdacOO05wD83qdDomx413B53j/bqGU3Gyfv8fsDltXJdztZZVZFhSybJksK/2YK9b0BRF8Sr/2djZpB0QkmJJpMPR2euWKDV4x/kI3pNjx6PX6b1WnZc2n8RqayE7IqPPjNA1/3wb1+dcrRWo6F4FTc1g1e1X1W21RJgtWoBXVwurq9tLmk/S6bRREJdz2s/2ZVxMAePcRUfAtaguNTyZY40lPLftF9S217M4/VKmj5ns9XOeB0yoZS59LUq6KGE612ZdwUWJvdcB9yUnMouZCdO8npufMpsl2VcyL9n3nGj/r53JJSmzAciNzKYwynuIW/2dVYfWU0N7rwCnbknbWP4vtlbtICowkryo7F7fPxQuTrqQH0y9h+jASMqsFSSGxHNl5mVn9TNHOwne4rRKmk7gVJxaCUxflbjOJafiZF/tQYw6A8WNJfzvwbf5xc7/7tdc866afeh1eq+5P/UQgmJ3xtiXj0o/41hjCVPjJmrDuqobcr+B2WDilb2vc8g9ZOzp66qdPLLxCW34V70Rig2OIdXiGrrubcV5921inkJNIeRH5lDafFKb2vjkxOcAXOBje4svOp2OS1Jm82/T7mVl3nVeQ+bgCoI6dJxqqcTmtNPQ0ei1gj3DkoZep9f2latBfGxcLkOhMCoXp+IkNCCE70z6JldlXu7zpiQ1LAkFRSuyEh/S88/LZDCxMG3eabck9ZfJEMCC1Lk+584H6uqsRSzJvpJVBTf06NvY6Hx06LR97L7mu1UF0XkEGQPZWL6ZTkcnFyVMP6PysgOVFp7Cv1/wPZZkX8nqCbcOalua6JsEb3Fa6grXKbGuk3yGO/Ouaq2mxd7K5LgJPHbRf1AYlcexxuN9rvSubKmipOkE2ZYMgt17h8E13KlD16/Mu8xawV+PfUC4KYxledf2eD0lLIlvjb8FgF/verVH5ay9ta4taupCLvVGKNadeQNUWF3DvU7FSW1bvUf7e8+8oStT/rpqF7Vt9XxdtYvEkPgeGVxfksMSuTjpwh5f9iaDiajACE61VlHXVoeC4rVNJtBoJi0shdLmk7Tb27UgXhg78MzblwWpc1ma8w0evuB+bauPL2pQa7G1YjGF97oo6XwUoDeyIHWu14EcqlBTiNdpV6cL3gF6o1bvXoeOC7uNFpxNQcZAFqTO7XP/tRg8Cd7itNTFajMSXBlctccJOp0O25CdAmVz2k9bRaurPa6biUxLGpGBEdyQew0GnYF3jv4du3urjcPpoK693uvn3nGXPJzb7RCIQGMgyaEJlDaf7PXEK3Btl/uvPetxKA5uzF+q7dXtLj8qh1vGrqTD0cn/7Hnda0RAvUHYX3fINd/tXrEdF9wVvNXM+89H3+OHXz2tbfOpbK3GYgrrtfTixNixGHUGtlXu4NOTn+NUnFyaOmdI952OCYmjqbNZG5LuPpeuznsfbjjG0cbjxIeMwRI4NAt5Ao2BzE+5uM/5c8+gluBjvtufTXAXNQk2BvkM8J6mxblGhQqj87zWBYiRQ4K36JVTcVLcWEp0YBSZlnR06LxWnP/vwbdZ++XTvVa/6q8yawVP/eunrPnyqT5LlqoBUM1C4oJjmJ10ITVttXxetonK1mqe3foz1nz5FF+Wu6p+HWkoZmf1HjIt6UyM6XmqUoYlHbvT3uuWsXZ7Oy/t/C1VrTUsTJ3nNR/ry5S4CUyKHUdVW42WXTd1NmuLvGrb66luq/XKvONCognQB1DRUklNWy3/PLERBYUPSz6l09FJfUdDr1k3uE6GKozOp7zlFJ+f/IoIs6XHsP5gqfPH6tqBmO7B21256uPSDXQ6Osl1Pz6X1AMmoGuR3UihbqNLDUvu86YsPyqHorzrWZ7bc4RIjAwSvEWv1CHqTEsaRr2RqMBIqt0BR1EU9tYewKE4eHn377TDHgZCURQ+O/klP976MypbqzDoDb3OF6uKG0sINJhJDO2q2rQ4fQGBhkD+VvwBT295gRPWcgL0Rl4/8CY7q/fy5yN/A2BJ9pU+v/SyLGlA1yiDp06Hjd/sXk9p80kuTJjGNVmL+9W3gmjv7U3qiEFMYJT2fHVrDWaDiXBTKHqdnoSQMZxqreLPR97DoTgIMQazp/aAVgTE13y3J3Xo3K44mJc8C+MQzzmqi9b2ufvUPfPOtKR7zXtnD0PwNugNJLsLv/harObPxgTHcse4m1ia+40+36vT6ZiVNEOGr0cwCd4CcG396b5NqWuIOh1wZblNnc2029s51VqlFcGwOWz8Yud/ayuR+8Nqa+Hl3b/jj4f+TKDBzOoJt/LtCbcBvueLAZo6rFS2VpMenuo1JxtqCuHytPm02dvRo+f2sUV8d/JqAvRGfrP7dxQ3lTIpdjyZ7iDdXYaPRWvFjaW8fuBNHt74GAfqDzM+ppCivOv7PQydH+kO3rVq8D4OwOXplwKwr/Yg1W01xAXFaNdMDInH7rSzvXo3qWHJFBW4Clm8feSvQN/Be3xMISZ9AIEGMxcnDWwldX+ow9Bq+dHumbdr3jtZe5wTee6DN0BquKsNnjd4I8WUuAkjbjpAnBlZDnieqWytJjYo+pysDlUdqj/CC9tfpij/emZ5bJ/pGqJ2Bb3YoBj2c4jqtlot0F2aOgcFhdcPvMUr+97goWn39RngDtcf45V9b9DQ0UhuRBa3jF1BhNkCwC1jV/LbPb/nha9/zcq865gW37Ud6FDNMa/2eLokdTYhAcHkReVoq6C/Of5mfrXrf9Ch45qsRb22JyowggizhaONx7VCLr/e/SrgOploTtJFLEq/dECnBUUHRTImOJZDDUexO+0UN5ag1+mZOmYiH5Z8yr66gzgVJ7Hual8ACaFdX8pLsq8kOyKDuOAYbaridMPm4KocddeEWzHoDGdloZZnJhtsDNIODfGUE5lFcVMp8cFxWgGTc21R+qWkhCWT5bHAS4iRRjLv88jxplIe3fQsfzn6ft9vHkJb3fuDD9cf83q++xC1WlayqrVGq96VHZHJrMQZWoWxvs4m3ld7kBd3vExTZzNXZy7i3sl3aoEbXJnFbWNXoqDwP/veYL37/GOAgzWu7UeZPgo/GPVGZiXN8Nq+NDY6j/unfJt7Jt5x2sCn0+nIsKTR3GnleNMJXj/wFkadgW9PuI3HLvoPvpG1yKtgSX8VROXS6ejkUP1RSptOkhyagNlgIt+97Qm6aiND19na46ILyI3MQq/Tex2POKYfw8D5UTlnLeMNDgjWFox1z7pV6n7v3HNc59lTuCnMVTP8HB4SIcS5JsH7PLK3xlXI/9MTn1PTVndOPtOpONmj1kz2WLDVamujsrWatPAUbRRAneOsaq3pUb1rlvskJrWiky/FjaX8ZvfvMOj0fGfiN1mUfonPEYapYybx79O/S2pYEpsqtvKLHf9Np8PGodpj6NCRHp7a7/5lWtJOu7VIpWZpL+9+lWablauzFjEupmBQIyBqTecPSj7Frji04fkCj/Z4Zt65kVkU5V3PTQU3aM/NiJ9KWEAoJn2A10EOw0XNvnur2pYflcOqgmVckbHwXDZLiFFHgvd5RF3oY1ccvHvs7GTfrbY2ry1ZJ5rLaHSXGq1srdayXDWD9qxBrGbeu2v2uU+rytSym4KoXCLMFrZW7vBZQvVUSyW/3Plb7IqDO8bd1GfFp7jgWB6Yeg+T4yZwtLGY3+59jSN1JSSGxve6XWow1KH4ps5msiwZWrWrwciOyMSgM2h/r+rCODWrhq4/U3BVCJuVNMNrO1SAIYC7J97OXRNuPadTKb1R51t7y7x1Ote+4rNVR1sI4TL83wYCcNXrLm4qJSk0gdSwJLZW7jijFdx9+d3+/+XRzc9pAXyXO+uODYpGQdGOU1TPKPas2RwdGIVep6ek2fWa5/CsXqdnZsJ02h0dPc4m3lG1m59se4kWeytF+Uu9To46HaPeyC2FK8iLzGZ3zX5sDptXoYqhlByaiNlgwmwwcXPhsiEJlIFGs9f8vNr2QGMgWe6td3FBp5/HBtcCrP6MHpwLie6V3H0tnhNCnF0SvM8Tx5tKsTvt5EVmsyT7SgD+78jfBnzE5Ok4FSeH64/R6ejkr8c+AFxZtFFv5FL3wRrq0Ll64+CZeRv0BqI9ikPkdNsKpJ5NrA6dt9nbeOPg2/xmz3psTjs3FSzrURu6LwF6I98af7N2dnH2WQreBr2B1RNu495J3+o1qzwT6tB5pDnCq1hGUf5Svj3xtiEpq3kuXZgwjVUFy4Z8D7kQYmBktfl5wnMBWG5kNmOj89lbe4CD9Ud8Zl2KonDSWkFyaEK/F+ZUtVbT7nAd4rH51DYmxo6lzFrB2Oh87YjGruBdSoTZ4rWYDFxztNVttVhM4cR6LLYC18lL+VE57K87xK92vcL+ukPYnXYSQ+K5fdyNZ7zFJdAYyHcm3cmx9qMUhvQvaz8TZ2ORVWF0Pn859n6P05XigmO8hsz9RYDeeE7LbQohfJPM+zxxuP4oOnRaEL3SveCnt7Omd9fs4+ktP+Wriq39/gx1KHx8TIFrNffe192PC7XKVCesZTR0NNLY2ezzzF01YOdEZvq8abjIvXBtd80+ogOjuCZzMQ9Ou3fQe1NDAoK5JPOiAW3XOh+khCXyrfG3aKMpQggxFCTzPg94znere2fTwlPIichkf90hyqwVJLnnGlUl7gVl/zq1jYsSp2vP17XXY2xz4uu+TB0KX5R+KTaHXTuhaLx7VXVyaCIlzSc44t4ylh7WM3irQTgv0veCs8mx47mlcAVjgmP7VcZxNJgY27MkqxBCDIZk3ucBdb67+/7cBe55aF/Zt3q+85GGYu1wkA5HJ89seZEffbKuR7U01+ecwKgzkBSayLXZVwCQFpaiDY2nhCXhVJx8WeGqCe4r874wYRq3Fa5kRvxUn33R6XRcED+FtPAUCdxCCHGWSPA+D6hbiXIivOdcC6PziA8Zw9bKHT1O3FLPd1ZQtNXdX5VvwWprocJaxSH3ecoqm8PmyuDDEgnQG0kJS+LeSXdyy9gV2nvUE5kO1h9Bh47U8J7HDgbojUyLn+x3w9dCCDGSSPAeQu32Dsqtpwb8c4frj3nNd6v0Oj0LUubgVJx8euIL7Xmn4qSqtYaowEh06NhWuROH08EnJzagw5Xtdi+WctJagUNxeG39yo/K8dry43mc4pjgWL86C1kIIUYTCd5D6P3jH/PUlp/261xqVautjWNNJV7z3Z6mxU/GYgrjy4ot2lB4fXsjNqeNjPBU8iKzOd5UyscnNlDbXs+spBkkhyews3oP1s4W7Tra1i8f89iqxJAxGHWujNrXkLkQQojzgwTvIVTRcgqn4tTmo7tzOB0ccx9+odpauR27064d59hdgN5IflQubfY2bahc/feY4Fjt5/5y9H106Lg0ZTaXZM7Crjj416lt2nV8FV3pzqA3aEU4JHgLIcT5S4L3EGroaHL/u9Hn65tObeUn217is7Ivtee+LP8Xep2eGfG9751VA64agCs9gvfE2HEYdAYUFCbGjiUuOJY56TMw6AxsrNii3SiUNJcSZAz0qqXtS4bFVTf8bFUyE0IIMXgSvIeQGrR7C95q6dG/F39Em72N0uaTnLCWMz6mEIu59+MT1Sy4pFvwjguJJSQgmMLoPKBrdXq4OZSJsWM51VLJ4YajtNpaqWqtIS0spc+yn1dmXMa9k+4kJSyxv90WQghxjsk+7zNUbj1FmClUO4DB5rRjtbnmmOt7Cd5q0LXaWviw5DNa7W0AXJQw3ef7VYmhCRh1Bi3zVofN1brYK/OuZ37yxWR41NGelTiDr6t28eL232h7s/szFB4SEHze1NEWQgjhm2TeZ6DF1sqPt77IHw7+n/Zck3vIHKChvffgHWYKJcJs4ZMTG9hy6msizBYtc+5NgN5IUlgiZdYKbA4bla3VRJgtBBrNAFjMYT1O6cqPyuHWwpVkWtIpb3GtgO9eolMIIYR/ksz7DBxpOIbNaae8pVJ7rsEzePvIvDscnTR0NJIXmc30MZN57cCfsGFnfsrF/TrBKj08hZKmExxrLKG+o4HcXiqceZoeP5np8ZOpaaujouUUhVGnv0kQQgjhHyTzPgOH3eVDa9vrtO1bngHbV/D2XCE+I2EqiSHx2jGa/aFu8dpSuV27Tn/FBEUxPqZQKp4JIcQIIZn3GTjU4KpeZnfaaepsJsJsodEjYFttLdgcNgIMAdpz2iKz4Fj0Oj33TLqD+vZGooOi+vWZ6orz7VW7ATlPWQghRjPJvAeoxdbqVUWtpq0O6Bo2jw+O83qs8tzeBRBhtmjbsvojNjiGIGMg7Y52wHUTIIQQYnSS4D1ARxqOoaBoh3nUasHblXmnh6e6H/uuRX6mGbNep/eqjiaZtxBCjF4SvAdIPUTkQvepWjXtXZm36zCPZKDndrHK1moC9EYiAyPO+LPVrV5GvZGoQVxHCCGEfztrwdvpdLJ27VqWL1/OqlWrKCkp8Xp9165dFBUVsXLlSu677z46OjrOVlOG1OH6YwTojUwdMwnoyrwbOxoJN4USHRgJeG8XUxSFqtZqYoNi+rWyvDdq8I4b5HWEEEL4t7MWAT766CM6Ozv5wx/+wAMPPMDTTz+tvaYoCmvWrOGpp57ijTfeYPbs2ZSVlZ2tpgyZFlsrZdYKMsLTiAuOQYeO2vY6FEWhobMJi9miDad7Zt6NnU10ODoHPdSdYUnFqDN4nf4lhBBi9Dlrq823bdvG7NmzAZg0aRJ79uzRXisuLiYiIoJXX32VQ4cOMXfuXDIzM89WU4bMkYZiFBSyIzMx6o1EmC3UtNXRYm/F7rQTYbYQEegK3p7bxSpbBjffrQo3hfFv0+/DYg4f1HWEEEL4t7MWvK1WK6Ghodpjg8GA3W7HaDRSX1/P9u3bWbNmDWlpaaxevZpx48Yxc+bMXq8XGRmM0WgY0jbGxvZeT9yXspMnAbggfRyxsWEkhMeyv/oIDrNrBXh8RDTpCWMIMARgdTRr19/e6Fp5nh2fOuDPPNM2D/ZzzlcjtV8wcvs2UvsFI7dvI7VfMHL6dtaCd2hoKC0tXedJO51OjEbXx0VERJCWlkZ2tqtK2OzZs9mzZ89pg3d9feuQti82Nozq6uZ+v9/hdLCpdDsB+gAinDFUVzcTbrCgoPB1yX4AzM5gamqsRJjCqW6p065/tNIV9IMcoQP6zDM10L75i5HaLxi5fRup/YKR27eR2i/wz771drNx1ua8p0yZwoYNGwDYsWMHubm52mspKSm0tLRoi9i2bt1KTs75fRjGjuo91LbXMSNhqlZ8JTrItTjtaEMxABHu4ewIs4XmTit2px3oucdbCCGEGIyzlnkvXLiQjRs3smLFChRF4cknn+Tdd9+ltbWV5cuX88QTT/DAAw+gKAqTJ09m3rx5Z6spg6YoCh+VfoYOHZemzNaejwmKBuBo43EAbbFahNm1jauxo4nooCjtQJIgY9C5bbgQQogR6awFb71ez6OPPur1XFZWlvbfM2fO5M033zxbHz+kDjcco7T5JJNix3lVNosOdJU2bep0DcOomXdkYNeK83BTGHXt9XKilxBCiCEjtc374aPSzwBYkDrX6/mYbnXJLe7MO9LcteLcamtBQSExJP4ctFQIIcRoIMG7D2XWCvYEaE6BAAALsUlEQVTWHiDLkk6GJc3rtXBTGAF6IzanHbPBRJAxEMCrdOqmU1vR6/TMS551ztsuhBBiZJLgfRq7qvfy2v4/AbAwbV6P13U6HdGBUZxqrdICNqDt9f705Bc0d1qZnTSTMSFx56TNQgghRj4J3j44FSdvHX6Xf57ciFFvZHnuEsbHFPp8b3SQK3hbPIJ3pHvBWnOnFbPBxBUZC85Ju4UQQowOErx9+Kp8C/88uZH44DhuH3cjSaEJvb5XXbQW4VH1LCQgGKPOgF1xsDB1HuGmkVEUQAghxPlBgnc3HY5O/lr8ASZ9APdOvtNrONwXddGa5/v0Oj0JIWNotrVwSeqcs9peIYQQo48E724+Lv2Mps5mFqdf2mfgBkgNcx0BmtRtNfk9k74JgNlgGvpGCiGEGNUkeHto6mzmw9LPCAsI7bEtrDc5kZn88MIHtYItqjBTaC8/IYQQQgyOHArt4b3ij+h0dHJFxkIC3du++iMuOFbO1xZCCHHOSMRxa7W18VX5v4gNimZW4gXD3RwhhBCiVxK83XZW78GuOJiZMB2DfmiPHhVCCCGGkgRvt21VOwGYOmbiMLdECCGEOD0J3riKqRysP0JaWEqPhWdCCCHE+UaCN7CjejdOxSlZtxBCCL8gwRvYVukaMp8SN2GYWyKEEEL0bdQH74aORo40FJNlSScyMGK4myOEEEL0adQH7+1Vu1FQmDpm0nA3RQghhOiXUR+899cdAmBS7LhhbokQQgjRP6M+eJdbT2ExhWHxOBVMCCGEOJ+N6uDdZm+nvqOBhG6HigghhBDns1EdvCtaKgFIDJXgLYQQwn+M7uBtPQUgmbcQQgi/MrqDt5Z5jxnmlgghhBD9N6qDd3mLK/OOD5bgLYQQwn+M+uAdHRhJoNE83E0RQggh+m3UBu+m9maaO60y3y2EEMLvjNrgfaKpApCV5kIIIfzP6A3ejeUAJITIfLcQQgj/MmqDd6k7eCfKsLkQQgg/M2qD98nGcvQ6PWOCY4e7KUIIIcSAjMrgrSgKJxrLiQ2KIcAQMNzNEUIIIQZkVAbvxs4mWmxtJMp8txBCCD80KoN3uVoWVVaaCyGE8EOjMnibDCaMeiN5kdnD3RQhhBBiwIzD3YDhkB2Rwe+u/yn1ta3D3RQhhBBiwEZl5g1g1BuGuwlCCCHEGRm1wVsIIYTwVxK8hRBCCD8jwVsIIYTwMxK8hRBCCD8jwVsIIYTwMxK8hRBCCD8jwVsIIYTwMxK8hRBCCD8jwVsIIYTwMxK8hRBCCD8jwVsIIYTwMzpFUZThboQQQggh+k8ybyGEEMLPSPAWQggh/IwEbyGEEMLPSPAWQggh/IwEbyGEEMLPSPAWQggh/MyoC95Op5O1a9eyfPlyVq1aRUlJyXA3aVBsNhsPPvggRUVFLF26lI8//piSkhJWrlxJUVERP/zhD3E6ncPdzDNWW1vL3LlzOXr06Ijq169//WuWL1/Oddddx5/+9KcR0TebzcYDDzzAihUrKCoqGjF/Zzt37mTVqlUAvfbnj3/8I9dddx3Lli3j008/Hc7m9ptnv/bv309RURGrVq3ijjvuoKamBvD/fqneffddli9frj32x371oIwy//jHP5SHHnpIURRF2b59u7J69ephbtHgvPnmm8rjjz+uKIqi1NXVKXPnzlXuuusuZdOmTYqiKMqaNWuUDz74YDibeMY6OzuVu+++W7nsssuUI0eOjJh+bdq0SbnrrrsUh8OhWK1W5cUXXxwRffvwww+V++67T1EURfniiy+U73znO37fr5dfflm56qqrlBtuuEFRFMVnf6qqqpSrrrpK6ejoUJqamrT/Pp9179eNN96o7Nu3T1EURXnjjTeUJ598ckT0S1EUZd++fcrNN9+sPeeP/fJl1GXe27ZtY/bs2QBMmjSJPXv2DHOLBmfRokV897vf1R4bDAb27t3LBRdcAMCcOXP48ssvh6t5g/LMM8+wYsUK4uLiAEZMv7744gtyc3O55557WL16NfPmzRsRfcvIyMDhcOB0OrFarRiNRr/vV2pqKj/72c+0x776s2vXLiZPnozJZCIsLIzU1FQOHDgwXE3ul+79WrduHQUFBQA4HA7MZvOI6Fd9fT3PPfccDz/8sPacP/bLl1EXvK1WK6Ghodpjg8GA3W4fxhYNTkhICKGhoVitVu677z6+973voSgKOp1Oe725uXmYWzlwb7/9NlFRUdqNFjAi+gWuL5Q9e/bwwgsv8J//+Z/84Ac/GBF9Cw4OpqysjMWLF7NmzRpWrVrl9/26/PLLMRqN2mNf/bFarYSFhWnvCQkJwWq1nvO2DkT3fqk3yF9//TWvvfYat956q9/3y+Fw8Mgjj/Dwww8TEhKivccf++WLse+3jCyhoaG0tLRoj51Op9cvsT+qqKjgnnvuoaioiKuvvppnn31We62lpYXw8PBhbN2Zeeutt9DpdHz11Vfs37+fhx56iLq6Ou11f+0XQEREBJmZmZhMJjIzMzGbzZw6dUp73V/79sorr3DxxRfzwAMPUFFRwS233ILNZtNe99d+edLru/IdtT/dv1NaWlq8goO/eO+99/jlL3/Jyy+/TFRUlN/3a+/evZSUlPCjH/2Ijo4Ojhw5whNPPMGFF17o1/1SjbrMe8qUKWzYsAGAHTt2kJubO8wtGpyamhpuv/12HnzwQZYuXQpAYWEhmzdvBmDDhg1MmzZtOJt4Rn7/+9/z2muvsX79egoKCnjmmWeYM2eO3/cLYOrUqXz++ecoikJlZSVtbW3MnDnT7/sWHh6ufQlaLBbsdvuI+F305Ks/EyZMYNu2bXR0dNDc3MzRo0f97nvlnXfe0f5/S0lJAfD7fk2YMIG//e1vrF+/nnXr1pGdnc0jjzzi9/1S+XfKeQYWLlzIxo0bWbFiBYqi8OSTTw53kwblV7/6FU1NTbz00ku89NJLADzyyCM8/vjjrFu3jszMTC6//PJhbuXQeOihh1izZo3f92v+/Pls2bKFpUuXoigKa9euJTk52e/7duutt/Lwww9TVFSEzWbj/vvvZ9y4cX7fL0++fgcNBgOrVq2iqKgIRVG4//77MZvNw93UfnM4HDzxxBMkJCRw7733AjB9+nTuu+8+v+5Xb2JjY0dEv+RUMSGEEMLPjLphcyGEEMLfSfAWQggh/IwEbyGEEMLPSPAWQggh/IwEbyGEEMLPjLqtYkKMVidPnmTRokVkZWV5Pb9s2TJuvPHGQV9/8+bN/PznP2f9+vWDvpYQ4vQkeAsxisTFxfHOO+8MdzOEEIMkwVsIwcyZM1m4cCHbt28nJCSE5557juTkZHbs2METTzxBR0cHkZGRPProo6SlpbF//37Wrl1Le3s7FouF5557DoC6ujruvPNOSktLycjI4MUXX8RkMg1z74QYeWTOW4hRpKqqimuuucbrn4MHD1JXV8fkyZN59913ufLKK3n88cfp7Ozk+9//PmvWrOEvf/kLK1as4Pvf/z4AP/jBD7j77rt59913ueKKK3j11VcBKC8vZ+3atfz973+npqbG704RE8JfSOYtxCjS27C52Wzm2muvBWDJkiWsW7eO48ePEx4ezoQJEwBYvHgxa9eupaysjOrqaubPnw9AUVER4Jrzzs/P12pjZ2VlUV9ffy66JcSoI8FbCIFer9eOunQ6nRgMBpxOZ4/3qdWU1fcCdHR0UFVVBeB1Qp9Op0OqLwtxdsiwuRCCtrY2PvnkE8B1lvqcOXPIzMykoaGBXbt2Aa4jIxMTE0lKSmLMmDF88cUXgOtEqhdeeGHY2i7EaCSZtxCjiDrn7Wn69OkAvP/++zz//PPExcXxzDPPYDKZeP7553nsscdoa2vDYrHw/PPPA/Dss8/yox/9iGeffZbIyEh+/OMfU1xcfM77I8RoJaeKCSHIy8vj4MGDw90MIUQ/ybC5EEII4Wck8xZCCCH8jGTeQgghhJ+R4C2EEEL4GQneQgghhJ+R4C2EEEL4GQneQgghhJ+R4C2EEEL4mf8PvF5eBzBYRJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdZ2CUZbrw8f/UZCYz6b03SIAgHQQMIsoiCjZUlBV1XXdli56zuuu6Zy27ru2ses6uuPraj4oFFFRQcVVEUXpJICEF0nvvdTLl/TDJmJBCAum5fl9InnnKdU+GuZ67PgqbzWZDCCGEEGOGcqQDEEIIIcTASPIWQgghxhhJ3kIIIcQYI8lbCCGEGGMkeQshhBBjjCRvIYQQYoyR5C3EGFNQUEBMTAy33HJLt9ceeOABYmJiqKqqGtA577rrLrZt29bnPgcPHmTVqlX93i6EGDqSvIUYg5ycnMjOzqawsNCxrampiWPHjo1gVEKI4aIe6QCEEAOnUqlYuXIlO3bsYMOGDQB8+eWXXHrppbz++uuO/TZv3szbb7+NUqnE29ubhx56iIiICEpLS3nggQcoKysjMDCQyspKxzGZmZk8/vjj1NTUYLFYWL9+Pddff32/4qqvr+evf/0raWlpKBQK4uPjuffee1Gr1Tz33HN89dVXaDQaPDw8ePLJJ/H19e11uxCid1LzFmKMuuaaa/jkk08cv3/88cdce+21jt/379/Pq6++yltvvcX27dtZtWoVv/nNb7DZbDz66KPMmDGDzz77jAcffJDs7GwAzGYz99xzD/fddx/btm1j06ZNvP766yQmJvYrpsceewx3d3d27NjB1q1bSU9P5/XXX6e4uJg333yTrVu3sm3bNhYvXsyJEyd63S6E6JvUvIUYo+Li4lCpVCQnJ+Pl5UVjYyOTJ092vP79999zxRVX4OnpCcB1113H448/TkFBAfv27eOPf/wjAGFhYSxYsACAnJwc8vLy+K//+i/HeVpaWkhJSSEqKuqsMe3Zs4f33nsPhUKBVqvlpptu4s033+TOO+8kNjaWa6+9liVLlrBkyRIWLlyI1WrtcbsQom+SvIUYw6666iq2b9+Op6cnV199dZfXrFZrt/1tNhtmsxmFQkHnxxqo1favAovFgtFo7FKjr6iowGg09qv2bbVaUSgUXX43m80olUo2bdpEUlIS+/fv54knniA+Pp7777+/1+1CiN5Js7kQY9jVV1/NF198weeff95txHd8fDyff/65Y+T51q1bcXd3JywsjPj4eDZv3gxAUVERBw8eBCAiIgJnZ2dH8i4uLmbVqlUkJyf3K56LLrqITZs2YbPZMJlMbNmyhUWLFpGWlsaqVauIiorirrvu4vbbbycpKanX7UKIvknNW4gxzM/Pj6ioKIxGI+7u7l1eW7x4Mbfffju33XYbVqsVT09PXnrpJZRKJY888gh/+tOfWLlyJf7+/sTGxgKg1Wp54YUXePzxx3n11Vcxm838x3/8B3PmzHEk+L48+OCDPPbYY6xevZq2tjbi4+PZsGEDWq2WlStXsmbNGvR6Pc7Ozjz44IPExsb2uF0I0TeFPBJUCCGEGFuk2VwIIYQYYyR5CyGEEGOMJG8hhBBijJHkLYQQQowxkryFEEKIMWbMTBUrL68f1PN5eOiprm4a1HOOFuO1bOO1XDB+yzZeywXjt2zjtVwwNsvm42PscfuErXmr1aqRDmHIjNeyjddywfgt23gtF4zfso3XcsH4KtuETd5CCCHEWCXJWwghhBhjJHkLIYQQY4wkbyGEEGKMkeQthBBCjDGSvIUQQogxRpK3EEIIMcaMmUVaRqONG/+X9PRUqqoqaWlpITAwCHd3Dx577L/7PO706XR++GEPP/vZL4YpUiGEEOOJJO/zcPfdvwPg8893kJubw69+dXe/jps0KYZJk2KGMjQhhBDj2JAm7+PHj/PMM8/w9ttvd9l+4sQJnnrqKWw2Gz4+Pjz99NM4OTmd17W2fJPB4bSyfu+vUimwWGx97jMv1pcbl0UPKI5jx47w4osb0Wg0XHXVtTg5ObFt2wfYbPZrPfbY38nKyuCTT7by178+yU03Xcv06TPIy8vF09OTxx77OyrV+FkFSAghxOAbsj7vV155hQcffJDW1tYu2202Gw899BBPPvkk7733HvHx8RQWFg5VGD2yYaPF3Ar0nbzPlclk4oUXXuXyy68kPz+Pp5/+J88//zKhoWEcOrS/y75FRYXceecGXnrpDWpqqklNTRmSmIQQQowfQ1bzDg0NZePGjdx///1dtmdnZ+Pu7s6bb77JqVOnuPjii4mMjDzv6924LLrfteTE8mReSXqLG2LWsDhowXlf+0yhoWGOnz08PHnssUfQ6/Xk5uYQF3dBl33d3Nzx8/MHwNfXD5Op682OEEIIcaYhS94rVqygoKCg2/bq6moSEhJ46KGHCAsLY8OGDcTFxbFw4cI+z+fhoR+0ReVjtWGQBIWthb0+sWUgjEZn9HotPj5G3N316HT2n+vr63njjZf59ttvAfjZz36GweCEu7seJycNPj5GVCqlIwYnJw3u7vpBiWkwzjEajddywfgt23gtF4zfso3XcsH4KduwD1hzd3cnLCyM6Gh7LTk+Pp7k5OSzJu/BfIyb1uaCTuNMWlnmoDxqtL6+haYmE+Xl9dTUNNHa2kZ5eT02m41p0y5g9eqr0el0GI1GsrPzMRg8HftYrTZHDK2tbdTUNJ13TD4+xkF/hOpoMF7LBeO3bOO1XDB+yzZeywVjs2y93WwMe/IOCQmhsbGR3NxcwsLCOHLkCNdff/2wxqBUKJnkGcGJ0lQa25pw0ejP63xXXLHa8fPs2XOZPXsuAAqFgr/97akej+nYZ/v2fzu2/fWvT55XHEIIISaGYVukZceOHWzevBmtVsvjjz/Offfdx5o1a/D392fp0qXDFYbDJK8IAHLq8of92kIIIcT5GNKad3BwMFu2bAFg9eofa6cLFy7kww8/HMpLn1VH8s6uzWWal8y5FkIIMXZM2OVRJ3mFA5BTlzeygQghhBADNGGTt9HJgK/Om5y6PKw260iHI4QQQvTbhE3eABFuYTSbWyhrKh/pUIQQQoh+m9DJO9w1FICsWmk6F0IIMXZM6AeTRLjZk3dOXS6LAucN+PhzfapYh+LiIrKyMlm8OH7A1xZCCDFxTejkHejij0apIfsca97n+lSxDkeOHKK4uEiStxBCiAEZN8l7W8anJJQl9Xt/lVKBxWrDZrNR1FjCn374G2pl17djlu90roteNeBYXnjhnyQlncBqtbJu3XouvngZH3zwPl9+uROlUsnMmbO5884NvPvuW5hMJuLiLmDRoosGfB0hhBAT07hJ3ufKWe1EQ5uZelMDrloDKuX5vSU//LCH8vJyXnzxNVpbW/jlL29n3rwFfP75dv74xweJiZnCRx99iFKpZN26WykuLpLELYQQYkDGTfK+LnrVgGrJnde43V98hE2pW7DYrNw94w4CXPzOOY6srAxSU1P47W9/CYDFYqGkpJgHH3yU9957m5KSYqZPn+F4vrcQQggxUBN6tHmHhQFzuTnmOhraGnkn9fxWfgsLC2fu3Pk8//zL/POfL3LJJZcREBDEjh0fcf/9f+b5518mJSWZlJRkFAqFJHEhhBADNm5q3ufroqAL2Vt0iPz6AixWCyrluT1+dMmSS0hIOMavf30nzc1NLF16KTqdjvDwCO68cz3u7h74+voRGzsVrVbLO++8yaRJMSxbdtkgl0gIIcR4Jcm7k2BDAHn1BZQ2lRNo8O/3cZ2fKqZQKPjP//x9t32uueZ6rrmm69PTYmOn8t572849YCGEEBOSNJt3EmgIAKCwoXiEIxFCCCF6J8m7k2BJ3kIIIcYASd6dBEnyFkIIMQZI8u5Er9Hj4eROYUPReZ/r+8ID/FB4YBCiEkIIIbqS5H2GIIM/taZ66k0N53wOm83GJ5mfsz3ri0GMTAghhLCT5H2GIEMgcH5N5w1tjTSbW2hsa6LNah6s0IQQQghAknc3Hf3eReeRvMuaKhw/17XWn3dMQgghRGeSvM/QkbwLBpC8SxpLsdqsjt/LmsodP9ea6gYvOCGEEAJJ3t346r3RKNWOmvfhkgSePPQPSjsl5M7SqzL428FnOVRyzLGtrPnHmndtqyRvIYQQg0uS9xmUCiUBLv4UN5aSV1/AO2kfUNBQxPbMngefZdXmAnC6JsuxrXOzuSRvIYQQg02Sdw+CDQGYbRZeSHydNqsZDyd3EsuTyKsr6LZvcWMJAPn1hY5t0mwuhBBiKEny7kHHMqn1bQ0sC4nnlik3ALAj69/d9i1uLHX822Y1Y7VZKW+uQK/WAVLzFkIIMfgkefcg1Bjs+PfqqJXEek5iskc0KVXpZNRkO/azWC2OWrbVZqW4oYTa1jrarGai3CMASd5CCCEGnyTvHkS6hfGzqTfz6xl3oFbaH7x2VeQKgC593+XNFZhtFrRKDWBvOu8Y2BZsCECv1lEjzeZCCCEGmSTvHigUCub6z8KoNTi2RbiFEeMRTWZtNjWttQAUtTeZz/CJAyCvodAxWM1H542bk6vUvIUQQgy6IU3ex48fZ/369b2+/tBDD/HMM88MZQiDaqpXDACnqjOBH/u7Z/tegEqhoqC+iPL2aWK+eh/ctK40m5sxWUwjE7AQQohxaciS9yuvvMKDDz5Ia2trj6+///77nDp1aqguPyRiPKIBSK/OAH5M3iHGIAJd/ChsKHJs89Xba94AtbLKmhBCiEE0ZMk7NDSUjRs39vhaQkICx48fZ+3atUN1+SER1N6P3bnm7axyxt3JjWBjEG1WM6drsjBoXHDR6H9M3tLvLYQQYhCph+rEK1asoKCg+7zosrIynn/+eZ5//nl27tzZ7/N5eOhRq1WDGSI+PsYBHxPnF8OhwkTanBopbyonyjMcX19XptRGsr/4MGarmUCPUHx8jATX+EIu2JxM53St8zHc1xsu47VcMH7LNl7LBeO3bOO1XDB+yjZkybs3X3zxBdXV1fzyl7+kvLyclpYWIiMjue666/o8rrq6aVDj8PExUl4+8ObsMJcwDpHIJ8m7sNiseGu9KS+vx0Ph7djHQ+NBeXk9KpMWgPyKMsp1w9d0fq5lG+3Ga7lg/JZtvJYLxm/Zxmu5YGyWrbebjWFP3rfeeiu33norANu2bSMrK+usiXs06ej3PlB8BIAAgx9gb1JXoMCGDV+9D0CnPm9pNhdCCDF4hm2q2I4dO9i8efNwXW7I+Ot9MWoNNJtbAAhwsSdvJ5UWv/ak7au318IleQshhBgKQ1rzDg4OZsuWLQCsXr262+tjqcbdQaFQMNk9iqNlx4EfkzdAqGswJU1l+Ot9AXDV2ps7JHkLIYQYTMPebD4exHhEc7TsOHq1Djetq2P76sgVTPGcTKDBHwC1Uo1B4yKjzYUQQgwqSd7nYHJ7v3eAix8KhcKx3dPZg/n+Hl32dXNypbK5aljjE0IIMb5J8j4HPnov1k6+hiBD4Fn3dXNypbChmBZzC85q52GITgghxHgnyfscLQle1K/93LUdC7XUS/IWQggxKOTBJENMRpwLIYQYbJK8h1hvyXtf0SHeTduKxWoZibCEEEKMYdJsPsTctN3XN0+uSOXdtK3YsBHlFs6CgDl9nuPtlC2cqslkvv9sFgXMw0vnOaQxCyGEGN2k5j3EOmreKZXp1JsaKG+q5P9S3kelVKFSqPgiZ1efte+a1loOlhylqqWaL3J28cj+/+bdtK20Wc3DVQQhhBCjjNS8h1igIYAQYxDp1Rn89cDfcVHraTY3c8uUG8muzWVv0UGOlh1nvv/sHo8/UpqIDRvXRl+Ji8aFb/L2sLfoICWNpfxi+q0YtYYu+7+bthVTeguXBCwhzDVkOIoohBBimEnNe4hplGr+MOe33DDpahQoqGipIj5oIQsD5rIibBlKhZKdOV9jtVl7PP5wSQJKhZILA+ayMGAuf5j7W2b7XkBmbQ7/ffg5KjrNIc+ty2dv0UEOFx7n70c28v9OvMHp6kxsNttwFVcIIcQwkJr3MFApVSwNWcw8/1mcrsliutcUALx0HiwMmMveokMcKU3sVvsuaiihoKGI6d5TMWhcANCqtNwx7af46335POdrPs74jDunrwfg24K9ANw0/SoO5Z0gqSKVpIpUvJ09WRg4n8tCl6BWyp9cCCHGOql5DyMXjZ6ZPnGolD8+l7yj9v1V7rfdasiHSxMAuiV1hULBFRHLCXcNJaE8iby6Ampb6zlaehx/vS/XTrmce2f/iv+ctYEF/nOoM9WzI+sLTlamD30hhRBCDDlJ3iPMS+fJDJ84ihpLyK7LdWy32qwcLknAWeVMXHtNvTOFQsFVkZcDsD3rC/YWHcBis3Bx8CIUCgUKhYJJHpHcOnUtP4+7BYDChqLhKZQQQoghJcl7FIgPvBCA7wsPOLZl1uRQ3VrDLN/paFWaHo+L8Ywm1mMSqVWn+CrvO5xVzsz37z7tLMgQANib4QeitLGM4sbSAR1zNiaLiYf3PcWHp7cP6nmFEGIikeQ9Ckz2iMJX582xshM0tjVhtVn5NPvfAMz3n9XnsVdF2WvfJouJhYFzcVY7ddvH3ckNndqZosb+J+8jJQk8cfgfPJfw8qAOeDtdk0VlSxU/FB6k2dw8aOcVQoiJRJL3KKBQKFgctACz1czB4iN8lfstGTXZzPSZziT3qD6PDXMNYbbvBaiVapYE9bzeukKhINDFn7KmCkyWNsd2i9XSbZS7zWbj06wveSPlPcxWM3Wmekqbys6/kO1S2vvd26xtHClNHLTzjhcZNdkyh18IcVaSvEeJC/3nolao+DrvOz7N/hJ3JzfWxa7p8sjR3tw6ZS1/ufB+fPXeve4TaAjAho2SJnszuNlq5pH9/83LSW91qVl/krmTnTlf4+XsySUhFwH2hNJZW6cbgIFKrTqFRqlBqVCyr+hQt9fbrGb2FOzjUMmxftX4kypSyKsrOOd4RpNT1Zn877EX2Zz+0UiHIoQY5SR5jxIGrQszfadTa6rHZrNx65S1uGj0/TpWo9Lg4eze5z6BLv7Aj/3e2bW5VLfWkFSR4uhrP12dydd53+Gj8+IPc3/L4sAFAGTU5DjOk1SRwu++e5BT1ZkDLSKVzVWUNpUT4xHNNK8Y8uoLKai3D6Kz2qwcKUngbweeZvOpj3kz5X3eP/VRn6vPVTZX89KJN9mY+MqoePBLXl0Bn2TuxNyPmrPVZqXZ3NJlW1JFCgAHio+QX194TjHUmxpobGs6p2PFualtrRu2tRRazK2j8nkIe4sOkliWNNJhDLo2q5mTlWmjcq0MSd6jyNLgi1CgYHnYUmI8owf13IGGrsk7reo0AAoUfJTxKXn1BbyVugWA26behFFrwF/vi4tGT2btjzXvvUUHsWHjYPHRAceQUnUKgKleMSwMmA/AvuLD1Jsa+Ffia7yR8h41rXUsDV5MkCGAHwoP8Hziq70mo/3Fh7Bho8nczHvpW0f8P9jHmZ/zZe7us743JouJfxx7iYf3PUlT24/9/imV6SgVSmzY2Hp6x4DLY7PZePbov3j84LNUtVSfUxnGmjareUSTWWrlKf5r72M9tiL1l81m69ffurGtiUf2j+xgT5Olje8L93dpfWtsa+K9tG28m76118WmBlOb1UxpYxn1poYhv97n2V/xwvHXSW3/7hpNJHmPIhFuoTwV/7BjCthgctS82wetpVafRqlQcnPsdZisbTx75F9UtVSzInwZEW5hgL2vPMotgqqWaqpbamg2N5Naaf8QJ1WmDPhLM7W9v3uK52TivGIxag0cKjnKk4f+l7Tq00z1iuHhC3/PDZOv5t7Zv2aG9zRO1WSy9fSObueyWC3sLz6Cs8qZaPcIkipSOVRy7Jzfn/NVb2pwtEZ8mbu71/fGbDXzStLbZNZm02RuJqHsBADljZWUNJUx1TOGOK9YTtdkcaLi5IBiqGiuory5klpTPf86/jpN47wG3mox8eiBp3n95Ltdtp+sTON/jr4wLC0QX+V9C8De4nNL3jabjScP/4M3zihDT46VHaehrdFx4z0S9hUd4v30jxwLQoH9/7UNG41tTd1ajIbixmpT6hYePfgMD/zwKPfs/hNbTn086NcAe+wHio8AkHeOLWFDSZL3KGPQuPSrn3ug9BodHk7uFDUU09TWRF5dARGuoSwOXMAc3xmYbRZCjEFcEX5Zl+Oi3MMBe793UkUqZpsFrVJDY1sTWbW53a5js9k4VZ1BUkUK6VUZlLRPNbNYLaRXZ+Ct88JX741KqeJC/7k0m1uob2vkmqgr+NUFP8Nb5wWAs9qJO6evx1fvzdGy4zSYGrtcJ6UqnZrWWub5z+LWKWtxUmn54PQnVLfUdNmvprW2yxKyQyWhLAkbNgwaFypaqjhadrzbPharhTdT3ielKp1J7pEoUHCwxF5LTyy2N5lP9Yrh2uhVKBVKPsr4bECD1zJqsgDw0XlR0ljKy0lvDfvgt1aLia2nd1DYUDzk19pTsI+qlmqOlydTZ6p3bN+ZvYvM2hySK1IH/Zqda8j5tUWkV2cA9qWJy5oqBny+suYKChuKOVFx8qx/q4PFxxzHtJzR5TJcOlrhjrXfdAIkd1r8KaXyxxrq/qLD3PvdgwOeotqXprYmEsuScNO6MtNnOnqNjn1Fh2gxtw7aNTqkVKU7PlfFA5ipM1wkeU8ggQZ/ak31HCs7gQ0bsZ6TALgp5jquiFjOL+LWd1n9DSDaPQKAzNocx3/Y1ZErALrVDFvMrbx28h3+mfAy/+/E//Fc4sv87eCzbEr9gNSqU7RYWpnqOdmx/6WhS1gcuID/mHUXy8OWolR0/TgqFUrigxZitprZX3y4y2s/FB4EYHHgArx0nlwXvYpmcwtf5Oxy7GO1WfnnsZd44tD/UNpUfs7vW4e+mjePtSfrX0y/FaVCyb9zdzua9Eoby/go4zP+vO9xjpWdIMotgl/PuIPJHlFk1uZQ3lRJQon9vZzqGYO/iy/xQQspb67ko4xP+x1fRvsX6x1xP2WmTxyna7LYkfXF+RR5wL7I2cU3+d/zwalPhvQ6rRYTX+d9B4ANGwnt/a0VzZWOxY46d/ecL6vNyqvJm3js4LPUtNYCsPPUbgBm+MQBcLS0+w3b2XTcALdZzeTX9z7wsqypvMsiTgWdbo4SypJ48fgbw9LSkl2bB0BefQEVzVVYbVZSqtLslQ4UpFTZE7nNZuOb/O8x2ywklA9eX3hieTJmm4WlwYv5xfT1LAlaSJvV7Lhuf/Wnm6Kj1q1A0esNSIu5lQ9PbadyGCoIZ5LkPYF0NJ3vzv8BgNj2RKrX6LgyYnmPzwkPMQShVWpIrUwnteoUgS7+xAcvwkml5URFiuM/QXlTJc8e/RcJZSeIcgvn2ugruSJiOSGGQPYXH+alpDcBe5N5B6PWwLrYNY4bhJ5c6D8HjVLDD0UHHcmwuqWGk5VphBlDCDEGArAocD6ezh4cKjnm+BJLqkilrLmCVouJ15I3dZkm1xOTpa3XPjSbzcZrJ9/hsYPPdtuntrWOjJpsotzCiXaPYK7fTEoaS/k8+yv+lfgajx58hq/zvsNqtXJJyEX8asbtaFVaFrQvqLO/+DDJpWn46rzx0dtbHq6OWkmgiz/ftY+874+Mmmx0ameCDYHcNvVmPJzc2VOwb1AH87VaTOwp2N/je1nRXMk3+d8D9vn8QzkL4IfCAzS0NbIoYD4KFI5ph4dLfpx+eOYsif6ytx5lUm9qcGz7JHMnCWUnKGkq48Xjb1DZXM2e3IN4OXvy09jrUSvVHClNcPx/MFnaeqwNmiymLr9ndRoMmtnp5zN1fAamesYAUNBptcTd+d+TXJnKhz10Lw2m6pYaqltr0LQ/HyGh7AQ5dfk0tjVxgfc0wl1DyKnLo9ncTH5D4Y9ddJWD1198uP3vPMdvBgAzfaYDDGiw3KGSY/zh+0c43ceg23pTAycqUggyBBDqGkxpU3mPXQCHSxPYXfADn2V/NZBiDApJ3hNIx6C1kqYydGpnwozBZz1GpVQR7hZGRUsVZquZWb7T0SjVTPWMoaK5kuLGUooaSnjm6PMUNZZwcfAi7pn1Sy4LvZgrI5bz+7m/5Sdhl2Cz2dAo1Uz26Hve+pn0Gj1z/WZS0VzJiRL7qM/d+T9gw8biwPmO/ZQKJUuCFmKytrG//Y55d3simeYVS2FDMR+e7l4bTK5I5b8PP8cfv/8rv/vuzzyX8HKPCfx4ebLjyzunLr/Lawnl9ibz2b72L5QVYZegQMHOnF2kVKUT7R7BHdPW8fhFD3L9pKvQqXWAvcamVWrYlb+HFnMrU7xiHOd0Umm5c/p6nFXOvJu29azN0PbugUqi3MJRKpRoVRpWhC+jzWrmq9xv+/FO98+XOd+w+dRH7Mrb0+21jzI+w2w1szBgHgC78rvv019tljaOlyfzTuqH3b5kTRYTX+V9i7PKiWuiryDaPYKs2hyqWqo5XJqARqkm0i2M0qbyLs3pHRrbmvg44/Neb2qOlh3nnwkv8dcDf+f7wgMcLD7K13nf4av35kL/uRQ0FPH3I89hsrRxcfAiXDR64rxiKWkqo7ChmIrmKh498DR/O/gMta0/Xn93/g/c+91DpFdlOLZl1eagVthbu3q72bDarBwqOYaTSsuVkcsBHLM0TBYTue2fx4MlRx0zFgai42blbFNAO1oJlgZfhFKhJKEsiZOVaQDEeccyxXMyVpuV9KoMR63VSaUlpy6vy8DMDharhe8LD/S7C6CmtZbT1ZlEuoU5KhpBhgC8nT1Jrkzt1xTWxOKTvJ26hWZzC2nVGb3ud7jkGFablYUB8why8cdis1DW3L1bJL19/EFCeRKtZ9yYDTVJ3hNIxzKpAJPdo7o1kfcm2i3c8fNs3wsAuMBnGgDf5H/PxsRXaGhrZO3ka7lx8jVdnlymVqq5Omol98+7m3tm3YWz2nnAcccH2ZeP/TT9a944+S678vdg0Lg47r47LAqcj0ap4buCfeTVFXC6JospnpP5Rdx6gg2B7C061KUW22Ju4e3ULRQ0FKHX6PBy9uB0TRbHzmj+bDG38kGnEb4pZzzg5VjpcRQomOVrrwX4u/ixOnIFFwbM5f65d/O72b9ijvUhcBUAACAASURBVN9MR42lg7PaiZm+0x1Ty6Z1St4Afnofbp26ljZrG68lv9PnyNrM9i/+qE6tGAsD5uLp7MH3RQeoaa3FZrNxtDSRb/P3ntPIfLPVzN72UdV7O7WEAJyqziCxPJlItzDWxa4h0MWfY2UnqGjsf3NiRXMVe4sO8nryOzzww994Oekt9hUf4qWkt6hornTs913BPupNDVwcvBiX9ps7sNeOS5vKiPOeyrT25wH0VJs9WHyEr/K+5f9S3u/2njabm9l6egcapRqbDd5P38ZbqZtxVjmzYfrtrItdw1TPGBraGnFSOzluVOa0x/B13h6eS3iJ6tYaalpreS15ExarhbSq0/YZBNjY1z64rbGtiZKmMqLcI/B29iSrNqfHv3FmTQ6VLdXM8rmAEEMQGqWagvYBVNm1eZhtFqZ5xaJWqHg3bSsNbV3Hh2TUZPN++kddWhI6+yz7K/6Z8NJZa48dzfZx3lOI8Ygmtz6fg8VHUSlUxHhMctx8nqhI4UhJIkatgUtC4rFhc4wN6OxAyRHeT9/m6P44m47uvrl+P646qVAomOEbR6vFRFp13wP5curyeHbfKyiwjykqaey6+FRa1Wm+zN3N/qLD/FB0CLVCxTz/WQScMVOng9VmdQxSNVlMwz5VTpL3BOKn93H0K3f0d/dHR0IIcPHD38UPgDivWJQKJfuLD1NnqueGSVezJHhhr+cINQYT2T6KfaDCXEMINQZzojSVo2XHiXQL5w9z7+52I+Ci0TPPbxaVLVWO0buXhFyERqXh53G34KTSsuXUx44+y13539PQ1sgV4ct55ML7uWfWXagVKj7O3NmlWfiLnF3UtNZycfBilApll+Rd3VJDZm0O0e4RuDm5OravCF/G+ik3EuYa0mfZOprONUo1k9wju70+w2ca8/xmU9pU1mezaketLbrTOdRKNZeHL8NsNbM98wteTX6b10++ywenPzlrH6HVZmV3/g+OLhaw9zfWtzWgUWqobq1xvA8Wq4UPTtlvbq6fdBVKhZJLQ5dgtVn5/NQ3fV4H7DWqF4+/wSP7n+LdtK0cLTuOTu3MZaEXszpyBc3mZl5N3kSbpY3vCvbxSeZOdGody0LjAZjpOx2lQuloOp/nN8vRFdMxiK+zjpaTU9UZfFewr8trn2V9RZ2pnhVhy3j4wt8zx3eG/TG8cevwc/FFpVTx87ifMsd3ButnXIdeY29FifOagrPKicOlx6hsqWZVxApm+V5AZm02b6du4fXkd1AplBg1Bk6Un6TVYiK7vSYb6RZOlHsETebmHp8l8EORfR2GBQGzUSlVBLoEUNRYitlq5nR7+eKDLuTKiJ9QZ6rn1aS3ya7Nw2az8XXed/wz4SW+L9zf4xSzhLIkduZ83X6dg33WHrNqc1EqlIQagx03qtWtNUxyj8RZ7USYMRi9WsehkmM0mpuY7zebOK9YgB6nWp0ot7cSJLfX3s/mSEkiSoXSUYHoMKuj6bw8uddjq1tqePH4G5gsJn4e91N0amfHYFr4cUzDJ5k72ZT2AaVNZY5HMXd0N545aK2gvohGc5Pj/+1wz3aRhztPIGqlGn+9L0WNJcQMMHnP9Ilz1C7A3pw92T2KtOrTXBN1BUtDFg9FyA6Xh1/K22mbuTR4CT8Ju6TXVoOlIYvZV3yIsuYK/PQ+jj52X70310Wv4r30bbyXtpVbptzIN3l7MGoMjpXkvHWeXBISz1d53/JN/vdcHr6MtKrT7Mrfg6ezB9dEraSooZiMmmzqTQ0YtQb2tQ+k6/zeDMRkjyjCXEOI8g5Fq9L2uM/CgLkcLj3G4dJjTPKwf1Fk1+bxVur7rIr4CXP8ZpJRk41GqSHUGNTl2Av95/LvnN2OUe1hriHk1RWwLeMzYj0m9fg+tphbeTt1s+PL0FVrYI7fTPYU7Adg/ZQbef3kO3xfeIA47yl8mbubosYSFgXMd9yszPWbyfbML/g66wfifS/CoLU/j95ms5FQnkRtax1uTq40tjXxSeZOms3NRLmFM8dvJjEeUfjpfR2zLiqbq9lXfIi/H9lIUWMJRq2B38z4ueMZ9waNC1M8J3OyMg29Wsc0rxhs2D/vPTVF59TloVPrUCmUfJL5OVM8J+Pv4ktBfRHfFuzFV+fNZaEXo1FpuCPup1ht1i6DKZ3VztwR91N8fIyUl9ubxbUqDTN9pnOg5AiXh1/KyohLaTG3UNxY6ni077rYNVS11PBFzi6SK1Id/daRbmF4OLtxsOQomTXZXVrIdmbv4khpIv4ufo4bs2BjALn1+RQ3lnG6JhMF9imdUz1jSK/OIK36NM8cfR53JzdqWmtx0xrRafQcKU1kceACR9dVYUMxb6VuRqvSMt1rCkfLjnOo5BjB/su7vWcmSxsF9UX2MTAqDTO843hf8RFWm5Vp3vYErVKqiPGc5Jj+eGHAXPxdfNGpdaRWncJmszn+piaLifT2mnJ+faHj89CZ2Wpmc/pHtFpMGLUGcuvzmeoZg1Fr6LJfmGsIblpXkspTsMRYun2mLVYLryW/Q0NbI3fMXssM9zi+yv2O3Pp8LFb7/hXNVTSbm5nsEc18v1k0m5uZ3d6yF+CYZtv1xqqjpn9R4AIsNivp1RlUt9ScdcGswTKkNe/jx4+zfv36bts//fRTbrjhBm666SYefvhhrNahn9gv7FaEXcLy0KX46npfSvVMGqWaX0y/tdsd77rY6/n1jJ+zPGzpIEfZ3Qyfafzftf/DyojL+mzuDzIEOO6EO/rmOiwOXECsxySSK9P4V+KrtFhaWRG+rMvDXFaEX4JB48KXud/wj2P/j42Jr2C1Wblx8tVoVVqmesVgw0Zq1SnaLG18X7AfnVrHPL++HyDTG6VCyf1z72bDvFt63WeSRyTuTm4cKzvh6Nf7JPNzypoqeDNlMwllSRQ1lhDhGtqlywLsX6jXRV+JQePCNVFX8Ps5v2FhwDxKGksdNx6dVbVU8z/HXmhvAg9Hq9LyXvo2kitSyazNJtZjEnP8ZhDuGsrJyjSSK1LZmbMLN60r10Zf6TiPWqnm0tAltJhbeT7xFZramrDZbGzN2MFryZv48PR2XkvexPvp27DYLNwUcy2/m/0rLg5ehL+LX5fpkjdOvpoQQyBFjSV467y4b/ZvCDnjJmVO+3iDWe3r/GuUasJdQyhsKO7yAJx6UwOVLdVEuoVxU8x1tFnN/Ov4azxz5F88l/gyNmzcOPkaNJ2e5HfmLIjerJm0intm/pJVET8B7En+l3Hr8XL24LLQix3TMsHer55Vm4MCBRFuoUS7dbQU/Hiz8VXut3ya/W+8nD34zYw7HHEEG+xlz6nLJacun2BjIHqNDpVSxW9n3sk9M3/JdO+p1LbWMdk9igfm/ye3TrkRBQq2nPrYPm2zKoMXjr+OyWLitilrWTNpNSqFim8LfuxSya8vcvRz59UXYLFZHK1nBq0LMR72haQ6ateAYzZJqDGYQIM/SoWSWI9oqlqqu/QZp1Wdps1qxqixJ+KTPdS+kytS2Vd8mKNlxx3zyuf7z+62n1KhZIZPHI3mJkdLRGefZO4kuy6XuX4zWRF9MQD+Lr5YbVbK22PquJGa5hXDwsB5LAtdgruTG2C/eXXR6Ck6Y9xJx9iFyZ7RXOg/Bxs2DpckdLv+UBmymvcrr7zC9u3b0el0Xba3tLTwj3/8gx07dqDT6bj33nvZvXs3l1566VCFIjqZ6z+LuZxbojmTl84DL53HoJyrP/o7//2GyVdzsOQoCwPmdjt+XewaHj/0P+Q3FOHp7MFF7f3pHXRqHasif8L76R9xuiaLqV4xrAy/lMj2fv+pnjF8krmTlMp0zFYL9W0NLA9d2uPT3AaLUqFkrt9Mvs77juTKNAwaPadrstofNlPOa8mbgK793Z3N9J3OzPZmToBVkSs4UpbIZ1lfMtdvJrr27oc6Uz3PJbxMeXMl8UELuWHSVRwqOcamtA8cswU6ukYuCrqQnLo8Xkp6E6vNys2xPzYhd7gk5CJqrNV8k7WXjYmvEGwIZF/xYfxd/Lgi/FLqTA20mFuZ5z8L7x5mOnTQqDTcdcHt7C8+zEVBF+KqNXbbZ67fTFotpi43mNHukWTUZJNZk0Oct70PPKfOPtUp3DWEWb7TuTBgLgeKj1DTWotR48KKsGVM8Zrc7fz9odfou62M6Ofiy18XPuD47AYa/Al08edkZRqK9t91ah3OKmeMGgOZtTnYbDb+nfsNO7L+jbuTG/fMugtP5x//n3XMsPi+8ABmq7lLd4tCoSDGM5oYz2ia2prQqXUoFApctUYWBc5nb9FB/vfYi2TX5aFUKFkTvcrx2ZjtewGHSxNIKk0jr6yUTWkfAHDX9NsczfkRbqGOa/009npKm8rx1fs4ts3wieNgyVGWhy51bJviNZmE8iRSK0/h175vUvsc/Guir+Dt1C2crExjUacBqICjG+S3M+7ESa2l1WIi1qPnFsPZvtPZU7iPPQX7unQJHi8/ya78Pfjpfbg55jrH38HfxReA4sYy/F38HIvLBBsCu52748FOGTXZmCwmtCotJksbGbX2VhJXrZFZvhew5fQnHCg5yvKwpUOyVseZhix5h4aGsnHjRu6///4u27VaLe+//74jqZvNZpychu6LT0w8QYYArote1eNrXjpP1kxazbtpW7kq8vJug8jAXkNXKdQEt08TOfPcblpXUqtOUdhQjFKh5OLgnp/mNpjm+8/m67zvOFxyjGaLfQrSutjru/Tv9zXlrjM3JyM/CV3Kp9lf8l7aVq6NvhInlRPPJ75KeXMlK8KWOR41e2HAXFKq0jlWdgJ3Jzfi2geCzfG9gK2nd9Bsbmae32yme0/tdh2lQskv566judnE/uLD5NUXEmwI5O6Zv3A0o/eXh7M7V0R0b87toFKquo25+LHfO7tT8rb3d4e72pPQLbE3sCZ6Nc5qp37XsAfqzC/yOX4z2JFlf+Rvl9UM3cNJLE/mpaQ3SapIwcPJnXtm/aLbjU2QIQAFCscMhJ7GSoD9ZqKzq6IuJ7Esiey6PPxd/Lj1jDEZS0MWc7g0gRcPvU1lczU6tQ6ztY3Xkjc5Fk+K7DR41cPZvVsTsYtGz+9m/6rLto6uq9SqUywNWYzVZiWpMgWDxoX5/rP5ImcXaVWnMVvNjpajFnMLyZWp+Ol9ifWcdNZkGO0eSaRbOMcrTpJVm0ukWxi1rXW8k/oBGqV9zEvnMTL+envy7hi01jF6/8wWnQ4BLv6crsmipLGMUNdgsmpzMFvNjtYHvUbHDO9pHC07Tq2pzlFrH0pDlrxXrFhBQUH3eZ5KpRJvb3uT7dtvv01TUxOLF5+9v9TDQ49a3b/R0f3l49P9Dn68GK9lG4xyXeNzGctiFuDq3Pu5rvZd1utrs4Pi2J29j4a2Ri4KncfkkL4HpfVXX2Xz8TESlh7EiUr73PoZ/lOZH20f8a92hiNFJ1gQPR1tp+bevtzocQUJlSc4WnacxPIkPPUelDdWcllUPHfMub7Ll+Xd7rex8cAbLA6dh7/fj1/Wa6Zdzr68o2xYeDNGJ0NPlwHgP+JvxyvRlZLGCn49f/2AE/e5MrpPQ3lcSW5jnuO9LTpp/5KeEzmlUxyuvZzh7M7l87jceZEjec8MjnWc44KgWBLLk0mqSCHaM5z7L9qAu67nJBDo6kdhXQkKFCyInt6v99QHI39c8iuyqvO4LCq+22fFx2cak7LCOV2Vg4/ekz9d/FtK6st4eu9LFDWW4KlzP6fPug9Ggoz+pFefptxWjFalpd7UwNLwhfj5ujEv+AI+P72bCkqZ7mNvgt+Tk0Kb1cySiHn4+vbv73P7nOt5+Jtn+Dzv3/zlknt57Ye3aTQ3ccfstcyM+LE1xcfHyDRdJJyAakslPj5GCpuK8dZ7Eh7o1+O5Y2rD2VO4j3plDT4+U/iq2N6CsyDiAsff766F60gtn0d0UNDYrnn3xWq18vTTT5Odnc3GjRv7VdDq6sFdPajzYJPxZryWbbDLVV5/bueKdIlkN/ZRyot8LxyUmPpTtlneM8ittTfvXRa01LH/TLeZzHSbSW1VC9D/ZTP/MPseDpcksCvvO0oay5jtewFXh15JRUX3KUU/n3IrQJcYF3otZKHXQlrqbLTQc+w+PkYqKxpZGWxfla+51kpzL/sOhXDXUE5XZpOcm4WPzovTldn46r0HJY5z/Tyq0BFqDCavvgAfpb/jHGFO9jn6s3ymc8uUG2lrUFLe0PP5A3T+FNaVEGQIGFBZvPDDy8Ov18/KdVFXkeCRyFL/JTi1GgjTGrh58nW8m76VSNfwc/6sXxWxkpeT3uKJ754nun3Q5STjJMrL64nURwG72Zt5DH+lvea7O8M+wn6KcWq/r+mFL9O9p5BUnsoz373CsdIkYjyimeU2y3GOjr+ZzaZBo9SQW1VERkEhtS11XOA9rddrGW32m9b0khymukzjWEEyKoUKb/w7HaMg2nlyj/9/zkdvN4gjkrwffvhhtFotL7zwAkqlzFYTY0usxyS0Ki3hxpCzTgUbTPP8Z/Fp9pdMam8iPF8apZpFgfO4MGAOxY2lBLj4DVnT8Ui5JOQismpz+DJ3N8tDl9JsbumxiX+43TLlBgobirs0ifu7+PLMkkdx6mXWQWfBhkCOlCY6Zh8MllBjMHMip3RJYouDFhBg8MNnAINczxTnPYVfTF/PK0lvk1KZjlqhcvRfR3tEolVpOVmZxppJq2loayS16hQhxiBHH3l/XRW5kuSKNA6XJuCscuaWKTf0+JlWKpT4630oaSojr31Z2mBj9/7uDgHtU2SzanJ4Oekt8uoLifWYNKRjXc5m2JL3jh07aGpqIi4ujg8//JC5c+dy2223AXDrrbeyfHnv/VlCjCZ6jY4/z/8denX/nrc+WNyd3Pjz/Hu7TZU5X0qFssv0pPFkpk8cfnpfDpUcw9vZnig7+rtHUpAhoMf3vD+JG2CW73SOlyc7FokZaoNxszjdeyq/mL6eV5PeZppXrCPxaZRqYj0mcaLiJC+deBNvnSdWm9UxMn8gAg3+LAiYw4HiI1w/+aouA/3O5O/iR35DEcfL7c8VCOlhsFoHvUaHu5Mb2e0DHie5R/LTKdcPOL7BNKTJOzg4mC1b7M+IXr16tWN7Wlr/JuULMVp1DOAZbr76c6/9TERKhZIVYZfwVupmdrY/tCZ8GFtLhoq3zovfz/3tSIcxYNO9p/KXhX90LBHc4eqoldSZ6rs87OjMFRT7a+3ka1kcOP+sNxwdI84T2x+c0ttgtQ6xnpM4Wnqcq6NWcnHwohFvpZJFWoQQ49pcv5l8lv0llS3VqJXqcdvKMFb0tIiJv4svv5/zG05VZ7Irfw8+Oq8+a8190ao0/Wop6FgtstncgotGf9YR4jfHXMdNk6/tsgbASJLkLYQY11RKFcvDLuH99G2EGAK7LWQjRofOc9SHQ8d0MbA/PfFsA6dH2+dmdEUjhBBD4MKAuZyqznA8e1sIH50XKoUKi81CkHHstcZI8hZCjHsapZqfx/W+BK2YeFRKFT56b0oaSwkx9N3fPRqNr3khQgghRD8FtT905MwH+owFEzJ5l1Q18ecX91I6yAu/CCGEGDuuilrJz+Nuwc/F9+w7jzITstk8p7iOExkVzIjyws9jeOfqCiGEGB28dZ59PhRnNJuQNW+j3r4QQn1T7w+eF0IIIUarCZq87fP06hvbRjgSIYQQYuAmaPJur3k3S81bCCHE2DMhk7dB117zbpKatxBCiLFnQiZvjVqJ3lktfd5CCCHGpAmZvAHcXJyk5i2EEGJMmrDJ29WgpaG5DZvNNtKhCCGEEAMyYZO3m4sTFquNplbzSIcihBBCDMjETd6Gjrne0nQuhBBibJnAydsJkIVahBBCjD0TOHlLzVsIIcTYNGGTt6uLveZdJzVvIYQQY8yETd5S8xZCCDFWTdzk7SJ93kIIIcamCZu8Xdtr3g1S8xZCCDHGTNjkLaPNhRBCjFUTNnk7aVQ4aVXS5y2EEGLMmbDJG8Co01DfLMlbCCHE2DKxk7deS32TSdY3F0IIMaZM8OStwWyx0WKyjHQoQgghRL8NafI+fvw469ev77b9m2++Yc2aNaxdu5YtW7YMZQh9Muo1gCzUIoQQYmxRD9WJX3nlFbZv345Op+uyva2tjSeffJIPP/wQnU7HzTffzCWXXIKPj89QhdIro/7HhVr8PIb98kIIIcQ5GbKad2hoKBs3buy2PTMzk9DQUNzc3NBqtcyZM4cjR44MVRh96qh5y3QxIYQQY8mQ1bxXrFhBQUFBt+0NDQ0YjUbH7y4uLjQ0NJz1fB4eetRq1aDGGOjrav9BpcLHx9j3zmPMeCtPh/FaLhi/ZRuv5YLxW7bxWi4YP2UbsuTdG4PBQGNjo+P3xsbGLsm8N9XVTYMah4+PESz2gWpFpXWUl9cP6vlHko+PcVyVp8N4LReM37KN13LB+C3beC0XjM2y9XazMeyjzaOiosjNzaWmpgaTycSRI0eYNWvWcIcBgKuLPJxECCHE2DNsNe8dO3bQ1NTE2rVreeCBB/j5z3+OzWZjzZo1+Pn5DVcYXRh1HX3ekryFEEKMHUOavIODgx1TwVavXu3YvmzZMpYtWzaUl+4Xx2jzZhmwJoQQYuyY0Iu0OGlVaNVKqXkLIYQYUyZ08gb7dDGZKiaEEGIsmfDJ26DXUt/UJuubCyGEGDMmfPI26jW0ma20tsn65kIIIcaGCZ+8fdzty7cWVjSeZU8hhBBidJjwyXtysDsAp/JrRjgSIYQQon8keYe0J+88Sd5CCCHGhgmfvD2MTvi66zhVUIvVKoPWhBBCjH4TPnmDvfbd3GqmoPzsD0gRQgghRpokbzo1nUu/txBCiDFAkjcwOVSStxBCiLFDkjfg4+aMh9GJU/k1sliLEEKIUU+SN6BQKJgU7EZdUxul1c0jHY4QQgjRJ0ne7WKk31sIIcQYIcm7XcegtXSZ7y2EEGKUk+TdLsDbBRdnNZmFtSMdihBCCNGnfifvsrIyAI4cOcI777xDS0vLkAU1EpQKBaF+RspqmmluNY90OEIIIUSv+pW8H3nkEf7xj3+QkZHBfffdx8mTJ3nwwQeHOrZhF+pnAJDFWoQQQoxq/UreSUlJPP744+zcuZPrr7+eJ554guzs7KGObdiF+hoByCuV5C2EEGL06lfytlgsWK1Wdu3axZIlS2hubqa5efxNqQppr3nnl9WPcCRCCCFE7/qVvK+55houuugigoKCmDFjBmvWrGHt2rVDHduw8/fUo1YppeYthBBiVFP3Z6ef/exn3HbbbSiV9lz/zjvv4OHhMaSBjQS1SkmQjwsF5Y1YrFZUShmML4QQYvTpV3bavXs3zz77LI2NjaxcuZLLL7+cbdu2DXVsIyLU14DZYqWksmmkQxFCCCF61K/k/fzzz7N69Wo+//xzLrjgAr755hs2bdo01LGNiFA/GbQmhBBidOt3u3BsbCzffvsty5Ytw8XFhba2tqGMa8SE+NoHreXJoDUhhBCjVL+St7e3N3/7299ISkoiPj6ep556isDAwKGObUQ4krfUvIUQQoxS/Urezz77LNOnT2fTpk3o9XpCQkJ49tlnhzq2EaFzUuPrriO/rEEeDyqEEGJU6tdocxcXFxobG3nmmWcwm80sWLAAvV7f5zFWq5W//OUvpKeno9VqeeyxxwgLC3O8vn37dt544w2USiVr1qxh3bp151eSQRTiZ+BoejnV9a14ujqPdDhCCCFEF/1K3n//+9/Jzc1lzZo12Gw2tm3bRn5+fp9LpH799deYTCY2b95MYmIiTz31FC+++GKXc3766afo9XquvPJKrrzyStzc3M6/RIMg1NeevPPKGiR5CyGEGHX6lbz37t3Lxx9/7JjnvXTpUlavXt3nMUePHiU+Ph6AmTNnkpyc3OX1mJgY6uvrUavV2Gw2FArFucQ/JELaR5xnFdUyM9p7hKMRQgghuupX8rZYLJjNZrRareN3lUrV5zENDQ0YDAbH7yqVCrPZjFptv+SkSZNYs2YNOp2O5cuX4+rq2uf5PDz0qNV9X3OgfHyMPW5fZHDmtU9T+P5EMbetisPZqV9v06jSW9nGuvFaLhi/ZRuv5YLxW7bxWi4YP2XrV1ZavXo1t956K1deeSUAn332GatWrerzGIPBQGNjo+N3q9XqSNxpaWl8++237Nq1C71ezx/+8Ad27tzJypUrez1fdfXgLpri42OkvLz36WCXzglm+94cPvgqncsXhA7qtYfa2co2Vo3XcsH4Ldt4LReM37KN13LB2Cxbbzcb/RptvmHDBn79619TVFREYWEhGzZsoKSkpM9jZs+ezZ49ewBITExk8uTJjteMRiPOzs44OTmhUqnw9PSkrq6uv2UZFsvnhaBzUvHFwVxaTZaRDkcIIYRw6Hd78JIlS1iyZInj93vvvZe//OUvve6/fPly9u7dy0033YTNZuOJJ55gx44dNDU1sXbtWtauXcu6devQaDSEhoZy7bXXnldBBpuLs4bL5oSwY18OuxMKx1ztWwghxPh1zp25Z5sDrVQqefTRR7tsi4qKcvx88803c/PNN5/r5YfF8nkhfHUkny8O5nLJ7CCcNIPb5y6EEEKci3N+bNZoGh0+VAw6DZfMDqKuqY3krKqRDkcIIYQAzlLzXr9+fY9J2maz0draOmRBjSZxEV7sPJBHZlEtc2J8RjocIYQQou/kfffddw9XHKNWRIARhQIyCmtHOhQhhBACOEvynj9//nDFMWo5a9WE+BjIKa7HbLGiVp1zT4MQQggxKCQT9UNUsBtmi1WeNCaEEGJUkOTdD9GB9jXXM6XpXAghxCggybsfooLsS7dKv7cQQojRQJJ3P/i46zDqNWQWSfIWQggx8iR594NCoSA6yI2qulaq6lpGOhwhhBATnCTvfooKau/3Lhpda7ALIYSYeCR591NUoL3fWwatCSGEGGmSvPspPMAVlVIhyVsIIcSIk+TdT04aLvMdsgAAIABJREFUFeH+RrKK6vgusXCkwxFCCDGBSfIegFt+EoOLTsObX6Sz82DuSIcjhBBigpLkPQBh/kYe+OlsPIxOfLA7ky8P5Y10SEIIISYgSd4DFOjtwp9+Ohu9k5ovDuVhPctzzYUQQojBJsn7HHi765g92YeaBpMMYBNCCDHsJHmfo7mxvgAcTisb4UiEEEJMNJK8z9HUcA/0TmqOppdL07kQQohhJcn7HKlVSmZN9qa6vpWsQll1TQghxPCR5H0e5sbYm86PpEvTuRBCiOEjyfs8TIvwROek5kh6mTSdCyGEGDaSvM+DWqVk1iRvqupayZYHlgghhBgmkrzP07z2UecHU0pHOBIhhBAThSTv8zQtwhODTsPB1FLMFutIhyOEEGICkOR9ntQqJQum+lHf1EZydtVIhyOEEGICGLLkbbVaefjhh1m7di3r168nN7frgzxOnDjBunXruPnmm7nnnntobW0dqlCG3KI4fwD2J5eMcCRCCCEmgiFL3l9//TUmk4nNmzdz33338dRTTzles9lsPPTQQzz55JO89957xMfHU1g4dh+zGe5vJMBLT8LpCppa2kY6HCGEEOPckCXvo0ePEh8fD8DMmTNJTk52vJadnY27uztvvvkmt9xyCzU1NURGRg5VKENOoVCwKM4fs8XKkfTykQ5HCCHEOKceqhM3NDRgMBgcv6tUKsxmM2q1murqahISEnjooYcICwtjw4YNxMXFsXDhwl7P5+GhR61WDWqMPj7GQTvXFfFRbP0ui4OpZay+OBqtZnBjHajBLNtoMl7Lxf9v797jo6rv/I+/5j6TmWSSyZWQeyAQDNeAigiiLYqKVYEaNvxgbd1aV7t2FV1dXdBSoVpYrNW1q499dKu4W7DqVrFeKpUWiaDcAgbCNSQQcr9nMslcz++PyGgkXIQMkzn5PB+PPGDmTM583swwn/me2xf1ZlNrLlBvNrXmAvVkC1nzttlsdHV1BW8HAgH0+t6ni42NJTMzkxEjRgAwffp0ysrKztq8W1tdA1pfYmI0jY2dA7Y+DTA6I5byyhbmPfouFpOOyaOS+MFN+QP2HOdroLMNFmrNBerNptZcoN5sas0FkZntTF82QrbZfNKkSWzevBmA0tJS8vLygsvS09Pp6uoKHsS2Y8cORo4cGapSLpmi60Yy9bIULsuKw6DX8cneWo7XR9YbRQghxOAXspH3rFmzKCkpYcGCBSiKwsqVK9mwYQMul4uioiJWrFjBkiVLUBSFiRMnMnPmzFCVcslkpkTzo1vGALDnSBPPvbGXjTuq+eHNl370LYQQQr1C1ry1Wi3Lly/vc19ubm7w71OnTuWNN94I1dOH3djceJLjLGzbX8/8mbnEWI3hLkkIIYRKyEVaQkSr0fDdyen4/AH+Whq5p8EJIYQYfKR5h9C0sSlYTHo27Topl04VQggxYKR5h5DZqGf6uGG0d3ko+aI23OUIIYRQCWneIfbdwjQMei1rPzzEpl3V4S5HCCGECkjzDrGEWAsPL5iI1aJn7Z8P8fuNhwkoSrjLEkIIEcGkeV8CI9Ls/NviyQyLj+KjHSdkAhMhhBAXRZr3JZIYa+GB749HA/x1txx9LoQQ4sJJ876EEmItjM2N52hNh1x5TQghxAWT5n2JzZwwHIC/ldaEuRIhhBCRSpr3JTY210FctIlP99XR7faFuxwhhBARSJr3JabTarlmfCpuj5/PyuvDXY4QQogIJM07DKaPT0Wr0ciV14QQQlwQad5hEBdtYvLoRE40OPn3daV0ujzhLkkIIUQEkeYdJnfeOJrCvEQOnmjj56/soLrBGe6ShBBCRAhp3mFiNur5x9sL+N60LJrae1i1bjcNra5wlyWEECICSPMOI61Gw23Tc1h0fR6dLi/P/mEvzm5vuMsSQggxyEnzHgSunZTG7CsyqG9x8cKbe/H6/OEuSQghxCAmzXuQmD8zlymjkzhU3c47JZXhLkcIIcQgJs17kNBqNNx1cz6xNiMf7ThBR5ccgS6EEKJ/0rwHEaNBx5yrsvB4A7y3rSrc5QghhBikpHkPMtPHpRIfY2LT7pO0droBOFbbQUVNR5grE0IIMVjow12A6Mug13LLtGx+9/4B1v3lMD5/gN2Hm9DrtDxzz1Tiok3hLlEIIUSYych7ELqqIIXEWDPbDzSw+3ATCXYzPn+ADz8/Hu7ShBBCDALSvAchvU7L4htGMzojlvtuL2DFj67EEWPir7tPyoFsQgghpHkPVpdlO/iX4kkUjkrCoNdy4xWZeHwy+hZCCCHNO2LMGD8Mu83Ix7tOykQmQggxxEnzjhAGvY4br8jE7fXzygcHaXO6w12SEEKIMAlZ8w4EAixbtoyioiIWLVpEVVX/5y0vXbqU1atXh6oMVblmQioZSTZ2HWrk0Ze28tbmiuDpZEIIIYaOkDXvjRs34vF4WL9+PUuWLOHpp58+7THr1q3j0KFDoSpBdUwGHUvvnMzi2aOwGPW8+2klD/1HCU+/tpMte2tRFCXcJQohhLgEQnae986dO5k+fToAEyZMoKysrM/y3bt3s2fPHoqKiqioqAhVGaqj02qZOWE4U8ekUFJWy+flDRw+0cah6nY8Pj/XTUoLd4lCCCFCLGTN2+l0YrPZgrd1Oh0+nw+9Xk9DQwMvvPACL7zwAu+///55rS8uLgq9XjegNSYmRg/o+i61ouGxFN2QT02Tk395/hPW/eUw4/KSSUyM/GxnotZcoN5sas0F6s2m1lygnmwha942m42urq7g7UAggF7f+3QffPABra2t3H333TQ2NtLT00NOTg5z58494/paW10DWl9iYjSNjZ0Dus5wMQB3zxnD6vWlrPjdZzy/5Fp8bvXNC66m1+yb1JpNrblAvdnUmgsiM9uZvmyEbJ/3pEmT2Lx5MwClpaXk5eUFly1evJi33nqLtWvXcvfddzNnzpyzNm5xbvlZDubPzKXd6eGZtTvwBwLhLkkIIUSIhGzkPWvWLEpKSliwYAGKorBy5Uo2bNiAy+WiqKgoVE87pM2+PIOKkx3sPNTIhpJKbpueE+6ShBBChEDImrdWq2X58uV97svNzT3tcTLiHjgajYY7bxrN8UYnGz6tJD8zjlEZceEuSwghxACTWcVUxmo28PDCyTz6H1t46Z19LJyVh8+v4PH58fkCeHwBkuOimDAyIdylCiGEuEDSvFUoP9vB7TOyefNvFfzH/5X1+5hHF04iLz32ElcmhBBiIEjzVqkbr8wkwW6h3enGoNdi0Osw6LV0e3ys/eAg//3+AZb/cAqGAT79TgghROhJ81YprUbDFWOS+11W09TFxh3VvFNSybxrTj8OQQghxOAmE5MMQXNn5JBgN/P+tuNU1UXWOY9CCCGkeQ9JZqOexbNHEVAUfvXGHg5Xt4W7JCGEEN+CNO8hqiA7ngXfGUlHl4df/u9uNu44IRObCCFEhJB93kPY9VPSSU+y8dLbZfzvxsP8aWsVBdkOxo9IYNKoRLQaTbhLFEII0Q8ZeQ9x+ZlxPPGDy5k2NgVFUSgpq+PFP5bxi7U7OV4v+8OFEGIwkpG3IC7axF03jyGgKJyod/Letiq2H2hg+e92MOeqTG69OhuNjMKFEGLQkJG3CNJqNGSmRPOPtxXw4B3jccSYeKekkg0lleEuTQghxNdI8xb9KsiJ57FFhSTYzfxxyzE27T4Z7pKEEEJ8SZq3OKNYm4klRROIiTLw2ocH2bavLtwlCSGEQJq3OIdkRxQP3DEBs0nHyxv28962quApZV5fAFePL8wVCiHE0CMHrIlzykyJ5l8XFvLsH/bwxl+PcqLBic8XoKyyBYCHF0wkJzUmzFUKIcTQISNvcV7Skmz82+LJpCfZ+Gx/PTsPNRITZcDj9fOrP+yhvsUV7hKFEGLIkJG3OG9x0SYeXTiJ/ZUtpCZYSXFE8bc9Nbz6wUH+fX0pDxZNwGYxYNBpMRlltjIhhAgVad7iW7GY9BSOSgrenjlhOG2dbt4pqeSxl7cBoNH0Tn5y89SsMFUphBDqJs1bXLRbr84mymzg6Ml2AgGFIzXtvPW3CjKSoxmbEx/u8oQQQnWkeYuLptFouH5KOkxJB6CyroOVa3fy8jv7eOIHU0iwW8JcoRBCqIs0bzHgslJiKJ6Vx6sfHOTZ1/fgiDZR0+zCoNNyVUEKV48bhiPGHO4yhRAiYknzFiFxzfhUjp5sp+SLOmqbXThiTLR3efjjlmO8XXKMyaOSmDsjh2RHVLhLFUKIiCPNW4SERqPhBzfmc/2UDBLsZiwmPd1uH9sPNLBp10m2H2hg58FGZowfxuwrM0mKlU3rQghxvqR5i5DRajWkJ9mCty0mPTPGpzJ93DB2Hmzkzc0V/LW0hr+V1lCQE8/YHAd1LS5ONDhJS7Kx4LqRGPRyKQIhhPgmad7iktNoNEwencTEvAQ+L+8diX9R0cwXFc3BxxyubqehxcV9c8diNvZ9mza0unC5fWSlyFXdhBBDkzRvETY6rZapl6Uw9bIUjtd3crzeyfBEK8lxFv7r3XJKjzTx7+tKuXV6Nka9jsO1nWzYfJSyY72XZf3etCy+d3U2WplrXAgxxEjzFoNCRnI0GcnRwdv33l7Af79XztZ99axZv6fPY0em2Wn98sIwJxqc/MOcMVhM8lYWQgwdIfvECwQCPPnkkxw8eBCj0chTTz1FZmZmcPm7777LK6+8gk6nIy8vjyeffBKtVvZvil56nZa75oxhbE48Te09eHx+rFYTY9JjSU+y4ez28ps/lrH7cBOr15XySPFEjAa5JKsQYmgIWbfcuHEjHo+H9evXs2TJEp5++ungsp6eHn71q1/x6quvsm7dOpxOJ5s2bQpVKSJCaTUarrwshTlXZTF3Ri7/b3Z+8AA4m8XAg0XjufKyZI7VdvDb98qDU5UGFIVut0xVKoRQr5CNvHfu3Mn06dMBmDBhAmVlZcFlRqORdevWYbH0nh7k8/kwmUyhKkWolE6r5Qc35tPU1sPn5Q2kOKJIjLXw/mfHqWnqYmSanWljhzFldJJsVhdCqErIPtGcTic221enCel0Onw+H3q9Hq1WS0JCAgBr167F5XIxbdq0s64vLi4KvX5gN4smJkaf+0ERSq3Z+sv1xI+m8uBzf+OdkkoAdFoNI9LsHK5u53B1O3/85BgPLSxkfF7iJa722xlKr5laqDWbWnOBerKFrHnbbDa6urqCtwOBAHq9vs/tVatWcezYMZ5//nk05zhiuLV1YOeLTkyMprGxc0DXOVioNdvZcv3k9rH89r1yRqbZuWFKBvF2M03t3Xyyp5b3tlWx9OVPue3qbG6+KqvP0emtnW6O13eSEh9Fot2CVhueI9eH4msW6dSaTa25IDKznenLRsia96RJk9i0aRM33XQTpaWl5OXl9Vm+bNkyjEYjL774ohyoJi5aepKNJ+6c0ue+BLuF22fkMC43nt+8Xcb/fXKM7QcamTU5jYKceD7acYKNO6rx+QMAGPVa8jPjuGlqJiPTYsMRQwghzotGOXWUzwA7dbT5oUOHUBSFlStXsn//flwuFwUFBcybN4/JkycHR9yLFy9m1qxZZ1zfQH9bisRvYOdLrdkuJleny8P/bjzM9vIGAl97yztiTFxVkEJzew/H652cbOrdWjQyzY7daqS1043bG2BsroMrx6SQlmg951aiCyGvWeRRaza15oLIzHamkXfImvdAk+Z9/tSabSBytXT0sGn3SQ5UtVI4KonvFA7H8LVjKQ6daOPdrZWUVfReCEan1aDVavD6ekfnwxOsXDEmmSvHJJMwgNdjl9cs8qg1m1pzQWRmu+SbzYUYjBwxZuZdk3vG5XnpsTyYPoGWjh50Wg3RViM+X4C9R5vZtr+evUebeGtzBW9trmBYfBRZKTFkD4vm8jHJxEQZL2ESIcRQJs1biH58fb5xo0HH5NFJTB6dhKvHy86DjXxeXs+Rmg5qm+vYuq+ONzdXcMOUdG64PENOSxNChJx8ygjxLUSZDUwfn8r08akEFIX6FhdfHG3mvW1VvFNSyUc7TjA6I44xWQ7i7Wa6ur243D4ykmyMTI8NHumuKAoBRUEnB2sKIS6ANG8hLpBWo2FYvJVh8VZmTEjlox3VfLKnht2Hm9h9uOm0x8dFmyjIdtDU3kNVXSc+f4BxIxKYMjqJ6+wyn7kQ4vxJ8xZiAJiNem65Kotbrsqisa2b8qpWurq9WC0GjAYt+ytb2XmwkU/21gKQ7IgCRWHHgQZ2HGjg1Q8O9M6wVpBCVV0nW/fVUd/i4juFacy+IqPPQXVCCCFHm6uQWrNFei6vL0BNUxeJsRaizHoURaG6sYvPy+vZuq+Olg538LEawGzS0e32k2A3853CNKzm3i8C0VFGHDEm4mymQT8ZS6S/Zmej1mxqzQWRmU2ONhcizAx6LZkpX/1H1Gg0pCfZSE+y8Q+3j2Pj1mPsOtRIWpKNK/KTsZj0bPhyP/r6j4/0u85kRxRZKdGMGG7nijHJ2CyGPssDisKR6nZONjopHJ0kR8QLoRIy8lYhtWZTay44e7bGtm6O1Xbg9vrxeAO0d3lo7eyhqa2H4w3O4AxqBr2WK8Ykk5sag7PbS0unm9LDTbR29o7oLSY9t07L4rrCNPS6S3Og3FB9zSKZWnNBZGaTkbcQESox1kLiGS4IE1AUGtu62X2oiU27q9myt5YtX+5Xh96GffW4YSTGWvjws+Os+/gI726tItlhIdFuISc1hsJRScRFm3D1+NhzpInGtm6uuCyZ5Lio056vzemmutHJ6Iy4S/YFQAhxOhl5q5Bas6k1FwxMtoCiUF7ZSpvTTXSUkegoA2mJNgz63ibb6fLwTkkle4400dLhDl4mVgOkJdmobe7C5//qvkl5iRTkOHD1+Gjv8nDgeCvH650AjMuN5x9vK8B0jn3u8ppFHrXmgsjMJiNvIVROq9FwWbbjjMujo4wsnJXHwll5+AMBmtt7+KKihe0HGjh8oo3hiTYmj04kPsbMxp3V7DzUyM5DjcHf12k1jMmKC15xbvW63fx0/vjgfvYej48j1e0cb3ByWZajz/59IcTAkuYtxBCk02pJioviO4VRfKcwDZ8/0Gcz+FUFKRyubqehtRtblIFoi4HUBCsWkx6fP8Bv3ytn2756/vWlrVgtBhRFoaXDjT/QO3J/k6NcNTaF+d8dxae7q9m6r55Ol4fc4Xby0uxMzEs8464AIcS5SfMWQpy2/1qj0ZCXHkte+ulTo+p1Wv5hzhjiY8x8WlaHx+sHIDMlmlEZsQxzWPnz9hOUfFFHyRd1QO+oPTrKwK5Djew61Mjrm45yeX4S1xWm0djWTVlFMycbu9DpNOh1WmwWAymOKBLjLHT3+Gho68bp8jIizc643HhSHFFoNBoURaG9y0Nds4um9h5ibUZSE6zERZtCMvubEIOF7PNWIbVmU2suUF+2QEBhyxe1HD7ZQc6waKaMTsJq1tPc0cP+ylY27jhBdWNXn98xGrQoCvh8Ac71oWQ26ggEFLz+AP19gkWZ9BTkOJg4MpFxufEhud682l6zU9SaCyIzm+zzFkJcMlqthhnjU5n33VF9PiwT7BZmjLcwfdww9h5tZvuBBoYnWCnIiQ/Ola4oCp0uL3UtLhrbuoky6UmMs2A26iivbGXv0WbqW13odVr0ei32KCMp8VHE2820dbqpbXZRUdPO5+UNfF7egFGv5fL8ZGaMT6XD5WH7gQYOnWgjNT6KvPRYEuwWqpucVDd0oddpyEmNISfVTlZK9Ldq+odOtFFR08E1E1JlchoRcjLyViG1ZlNrLlBvtnDlUhSFEw1Odh9u4tOyWhrbevost1kMOLu9Z12HBkhNtDJiuJ3L85MZldE7sYyrx8u+ylasVhMGjYIGDe9urWTv0Wag9xr2i64fRU5qDCVf1LJ1Xz09Hh86nRazUce4nHimFqSQ4jj9VLzBQK3vRYjMbGcaeUvzViG1ZlNrLlBvtsGQK6Ao7D/Wwmf764mxGpmSn0RmcjSdLi+Hq9toc3oYnmAlLcmG1xegoqadipoOKmo6OFbXgccbACDBbiYx1sKhE23BA/O+blR6LNnDYvhoxwn8AQWtRkNAUTDotcREGfD6FVw9Pnz+3vUlO6KIthiwmPTYbUaS4yw4Ynq3Hpxs6sLj9XPLtGzSk2yX9N9rMLxmoRKJ2WSzuRBiSNJqNBTkxFOQE9/n/hirkcJRSac9vnBUUvB+fyDAkep2tnxRy/YDDTS195CVEs3EkQkkJ0ZTVdOG0+VlyugkLst2oNFomDY2hXUfH8HZ7WVaQe9kM1Zz7+l0bo+fXYcb2bqvjmM1HTS0uvrdZ39K6ZFmvj8zl8mjk/h4V+9FeCwmPRNGJJCXHktlXQdfVDRzsqkLo16HyaAlIzmam6dmkZMaA0BXj5djNR109fjodvvQajWkOKJIie/98iAH9kUmGXmrkFqzqTUXqDebmnL1eHx4vAFirL3Xhx+IbIqi4Pb6ae1009DaTXNHD7E2E6kJVuqaXfz3++V0ur7avG816/H6A8GtAdB7JH9qghV/QKHb7QteDndMVhweb4CjNe1n/YJwah0JdjOpCVbyshwMi7UwYnjvZXY/Latj+4EGAgGFGKsRu9WI3WoixmogyvzVtfSjzHoSYy04ok20OT2cbHLS6fKSlxZLTmoMWm34vyRE4vtRRt5CCHERzEY95gGe10Wj0WA26hkWr2dYvLXPshRHFMtTr+DVDw7Q3N7DtZOGc+VlKWiAA8dbOXKyg8xkG2OyHMED5BRF4cDxNjaUHGN/ZSsaDeSm2snPjCPWZsRs0uPzBahrcVHb7KLH4yOg9M5419Dq6jMXvQaCR/0bDVrMRj0Nre3nPBOgPzaLgayUaLRaDZovc2s0vacdjs2JZ8roJExGHQ2tLrbtq6ehrTv4u3HRpt4tBY4okh1RwYsCdfV4qW5w4vYGMBm0mIw6TIbeH+OXf+p1GtVuWZCRtwqpNZtac4F6s6k1Fwz+bHUtLmwWw2kzzZ3JqXPm23v87Nxfy5HqdvS63sluCkclYjbqCQQUOru9tDvddHR5cLl9webodHlobOuhqaOHWGvv+fZRZj37K1vZe7SJNqfnjM9tNupIcURRWXfuf0+bpXdq3K9PoXsmWo0Gu81IZnI0Gck2fArsq2imtrmLWKuJJIeFWJsJvz+A169gMmhxRJtxxJhwxJhxRJswG/WcaHRSVddJp8uD1WzAFmUgLcHKiDQ7Bn1op+WVkbcQQgwh3/Zodo1GQ6zNxMjsaDIT+v9drVbz5Wbz898EcXl+Moqi4PEGUFBQFHp/UHC6vGzdV8eWL2qpquskPzOOqwpSyEuPRaOBgAIt7T3Utbi++ml24fb5Kch2kJ5kI8qsx+0N4PH6cZ/68fi/vB2gqb2b0iNNlB7p3aKg1/Xu8+/o8lBW0fKt/o2+yajXkpMag9cXoKXTjUGnZdmdk/vsTggVad5CCCFCSqPRYDKePkK1mg3cNj2H703Lxu3193t+fFKshdGZcRf1/G1ONycanKQNs2MzaIOT9bh6fHS6PMFrBvR4fLR0uGnp6KGl001rRw9dPT6GJ1jJGhZNXLSZrm4vHS4PFTUd7Kts4cDxNnTa3i8+wxOtIR+JnyLNWwghRFhptZqQXtgm1mYi1mY6bVdHlFlPlPmr57Vbjf1Ohdufy/OTgd4DGY0GHdpLvG9dmrcQQghxgczG8LRR7bkfIoQQQojBRJq3EEIIEWFC1rwDgQDLli2jqKiIRYsWUVVV1Wf5xx9/zLx58ygqKuL1118PVRlCCCGE6oSseW/cuBGPx8P69etZsmQJTz/9dHCZ1+vlF7/4Bb/97W9Zu3Yt69evp7GxMVSlCCGEEKoSsua9c+dOpk+fDsCECRMoKysLLjt69CgZGRnY7XaMRiOFhYXs2LEjVKUIIYQQqhKyw+ScTic221ez4eh0Onw+H3q9HqfTSXT0V1eNsVqtOJ3Os64vLi4K/QCfP3emK9eogVqzqTUXqDebWnOBerOpNReoJ1vImrfNZqOrqyt4OxAIoNfr+13W1dXVp5n3p7XVNaD1DfZLG14MtWZTay5Qbza15gL1ZlNrLojMbGf6shGyzeaTJk1i8+bNAJSWlpKXlxdclpubS1VVFW1tbXg8Hnbs2MHEiRNDVYoQQgihKiEbec+aNYuSkhIWLFiAoiisXLmSDRs24HK5KCoq4tFHH+Wuu+5CURTmzZtHcnJyqEoRQgghVCVkzVur1bJ8+fI+9+Xm5gb/ft1113HdddeF6umFEEII1ZKLtAghhBARJmLm8xZCCCFELxl5CyGEEBFGmrcQQggRYaR5CyGEEBFGmrcQQggRYaR5CyGEEBFGmrcQQggRYYZc8z7XPOORxuv18vDDD1NcXMz8+fP5y1/+QlVVFX/3d39HcXExTzzxBIFAINxlXrDm5mauueYajh49qqpcL730EkVFRcydO5c//OEPqsjm9XpZsmQJCxYsoLi4WDWv2Z49e1i0aBHAGfO8/vrrzJ07lzvuuINNmzaFs9zz9vVc5eXlFBcXs2jRIu666y6ampqAyM91yoYNGygqKgrejsRcp1GGmA8//FB55JFHFEVRlN27dyv33HNPmCu6OG+88Yby1FNPKYqiKC0tLco111yj/PjHP1a2bdumKIqiLF26VPnzn/8czhIvmMfjUe69917l+uuvV44cOaKaXNu2bVN+/OMfK36/X3E6ncqvf/1rVWT76KOPlPvvv19RFEXZsmWL8pOf/CTic7388svKnDlzlO9///uKoij95mloaFDmzJmjuN1upaOjI/j3weybuRYuXKjs379fURRF+f3vf6+sXLlSFbkURVH279+vLF68OHhfJObqz5AbeZ9tnvFINHv2bH76058Gb+t0Ovbt28fll18OwIwZM/j000/DVd5FeeaZZ1iwYAFJSUkAqsm1ZcsW8vLyuO+++7jnnnuYOXOmKrJlZ2fj9/sJBAKuvzojAAAFiklEQVQ4nU70en3E58rIyOD5558P3u4vz969e5k4cSJGo5Ho6GgyMjI4cOBAuEo+L9/MtWbNGvLz8wHw+/2YTCZV5GptbWX16tU89thjwfsiMVd/hlzzPtM845HKarVis9lwOp3cf//9/PM//zOKoqDRaILLOzsjawo8gLfeeguHwxH8ogWoIhf0fqCUlZXx3HPP8bOf/YyHHnpIFdmioqI4efIkN954I0uXLmXRokURn+uGG24ITmUM/b8HnU5nnymNrVYrTqfzktf6bXwz16kvyLt27eK1117jzjvvjPhcfr+fxx9/nMceewyr1Rp8TCTm6k/IJiYZrM42z3ikqq2t5b777qO4uJhbbrmFVatWBZd1dXURExMTxuouzJtvvolGo2Hr1q2Ul5fzyCOP0NLSElweqbkAYmNjycnJwWg0kpOTg8lkoq6uLrg8UrP97ne/4+qrr2bJkiXU1tby93//93i93uDySM31dVrtV+OdU3m++ZnS1dXVpzlEivfee4/f/OY3vPzyyzgcjojPtW/fPqqqqnjyySdxu90cOXKEFStWcOWVV0Z0rlOG3Mj7bPOMR6KmpiZ++MMf8vDDDzN//nwAxowZw2effQbA5s2bmTx5cjhLvCD/8z//w2uvvcbatWvJz8/nmWeeYcaMGRGfC6CwsJBPPvkERVGor6+nu7ubqVOnRny2mJiY4Ieg3W7H5/Op4r34df3lGTduHDt37sTtdtPZ2cnRo0cj7nPl7bffDv5/S09PB4j4XOPGjeNPf/oTa9euZc2aNYwYMYLHH3884nOdEtlDzgvQ3zzjkew///M/6ejo4MUXX+TFF18E4PHHH+epp55izZo15OTkcMMNN4S5yoHxyCOPsHTp0ojPde2117J9+3bmz5+PoigsW7aMtLS0iM9255138thjj1FcXIzX6+WBBx6goKAg4nN9XX/vQZ1Ox6JFiyguLkZRFB544AFMJlO4Sz1vfr+fFStWMGzYMP7pn/4JgClTpnD//fdHdK4zSUxMVEUumVVMCCGEiDBDbrO5EEIIEemkeQshhBARRpq3EEIIEWGkeQshhBARRpq3EEIIEWGG3KliQgxV1dXVzJ49m9zc3D7333HHHSxcuPCi1//ZZ5/xwgsvsHbt2otelxDi7KR5CzGEJCUl8fbbb4e7DCHERZLmLYRg6tSpzJo1i927d2O1Wlm9ejVpaWmUlpayYsUK3G43cXFxLF++nMzMTMrLy1m2bBk9PT3Y7XZWr14NQEtLCz/60Y84fvw42dnZ/PrXv8ZoNIY5nRDqI/u8hRhCGhoauPXWW/v8HDx4kJaWFiZOnMiGDRu4+eabeeqpp/B4PDz44IMsXbqUd955hwULFvDggw8C8NBDD3HvvfeyYcMGbrrpJl555RUAampqWLZsGe+//z5NTU0RN4uYEJFCRt5CDCFn2mxuMpm47bbbALj99ttZs2YNlZWVxMTEMG7cOABuvPFGli1bxsmTJ2lsbOTaa68FoLi4GOjd5z169OjgtbFzc3NpbW29FLGEGHKkeQsh0Gq1wakuA4EAOp2OQCBw2uNOXU351GMB3G43DQ0NAH1m6NNoNMjVl4UIDdlsLoSgu7ubjz/+GOidS33GjBnk5OTQ1tbG3r17gd4pI1NTUxk+fDjJycls2bIF6J2R6rnnngtb7UIMRTLyFmIIObXP++umTJkCwAcffMCzzz5LUlISzzzzDEajkWeffZaf//zndHd3Y7fbefbZZwFYtWoVTz75JKtWrSIuLo5f/vKXHDt27JLnEWKoklnFhBCMGjWKgwcPhrsMIcR5ks3mQgghRISRkbcQQggRYWTkLYQQQkQYad5CCCFEhJHmLYQQQkQYad5CCCFEhJHmLYQQQkQYad5CCCFEhPn/nB8S66il0r8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Trains for 5 epochs.\n",
    "if args.train_using_builtin_fit_method:\n",
    "    model.trainable = True\n",
    "    model.compile(optimizer=optimizers.SGD(lr=0.001/10, momentum = 0.9), \n",
    "                  loss=[categorical_crossentropy], \n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model.load_weights(filepath=weights_path+'/model_transfer_epoch_50.hdf5')\n",
    "    \n",
    "    #%%\n",
    "    save_path=weights_path+'/model_fine_tune.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(save_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only = True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    #%%\n",
    "    history = model.fit(train_gen, epochs=150, verbose=1, callbacks=callbacks_list, validation_data=test_gen, shuffle=True)\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    #plt.style.available\n",
    "    #['fivethirtyeight',\n",
    "     #'seaborn-pastel',\n",
    "     #'seaborn-whitegrid',\n",
    "     #'ggplot',\n",
    "     #'grayscale']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(fname='model_accuracy_'+db+'.png')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(fname='model_loss_'+db+'.png')\n",
    "    \n",
    "    #%% stop execution\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(filepath=weights_path+\"/model_fine_tune_epoch_150.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "papermill": {
     "duration": 7.678019,
     "end_time": "2020-12-02T09:00:50.963669",
     "exception": false,
     "start_time": "2020-12-02T09:00:43.285650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 43s 239ms/step\n",
      "[[17  3  6 ...  0  0  0]\n",
      " [ 1 19  1 ...  0  0  0]\n",
      " [ 3  1 19 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 22  0  0]\n",
      " [ 0  0  0 ...  2 25  0]\n",
      " [ 0  0  0 ...  0  0 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        30\n",
      "           1       0.61      0.63      0.62        30\n",
      "           2       0.59      0.68      0.63        28\n",
      "           3       0.67      0.87      0.75        30\n",
      "           4       0.86      0.86      0.86        14\n",
      "           5       0.90      0.82      0.86        11\n",
      "           6       0.91      0.91      0.91        23\n",
      "           7       0.60      0.67      0.63        18\n",
      "           8       0.38      0.52      0.43        29\n",
      "           9       0.87      0.87      0.87        30\n",
      "          10       0.56      0.50      0.53        30\n",
      "          11       1.00      0.81      0.89        26\n",
      "          12       0.79      0.77      0.78        30\n",
      "          13       0.75      0.90      0.82        30\n",
      "          14       0.83      0.89      0.86        28\n",
      "          15       1.00      0.82      0.90        28\n",
      "          16       0.73      0.81      0.77        27\n",
      "          17       1.00      1.00      1.00        15\n",
      "          18       0.79      0.79      0.79        29\n",
      "          19       0.64      0.62      0.63        29\n",
      "          20       0.93      0.83      0.88        30\n",
      "          21       0.78      0.69      0.73        26\n",
      "          22       0.68      0.52      0.59        29\n",
      "          23       0.90      0.82      0.86        22\n",
      "          24       0.59      0.57      0.58        30\n",
      "          25       0.81      0.73      0.77        30\n",
      "          26       0.60      0.30      0.40        30\n",
      "          27       0.81      0.86      0.83        29\n",
      "          28       0.25      0.27      0.26        30\n",
      "          29       0.31      0.37      0.34        30\n",
      "          30       0.73      0.63      0.68        30\n",
      "          31       0.86      0.52      0.65        23\n",
      "          32       0.51      0.62      0.56        29\n",
      "          33       0.84      0.72      0.78        29\n",
      "          34       0.90      0.93      0.92        30\n",
      "          35       0.76      0.87      0.81        30\n",
      "          36       0.35      0.45      0.39        29\n",
      "          37       0.64      0.53      0.58        30\n",
      "          38       0.27      0.38      0.31        29\n",
      "          39       0.62      0.53      0.57        30\n",
      "          40       0.71      0.73      0.72        30\n",
      "          41       0.88      0.73      0.80        30\n",
      "          42       0.53      0.28      0.36        29\n",
      "          43       0.81      0.70      0.75        30\n",
      "          44       0.72      0.60      0.65        30\n",
      "          45       0.71      0.80      0.75        30\n",
      "          46       0.89      0.80      0.84        30\n",
      "          47       0.97      0.93      0.95        30\n",
      "          48       0.50      0.43      0.46        30\n",
      "          49       0.73      0.80      0.76        30\n",
      "          50       0.83      0.83      0.83        30\n",
      "          51       0.96      0.87      0.91        30\n",
      "          52       0.82      0.93      0.87        30\n",
      "          53       0.77      0.77      0.77        30\n",
      "          54       0.93      0.83      0.88        30\n",
      "          55       0.79      0.87      0.83        30\n",
      "          56       0.93      0.87      0.90        30\n",
      "          57       0.72      0.75      0.74        28\n",
      "          58       0.37      0.23      0.29        30\n",
      "          59       0.38      0.48      0.42        29\n",
      "          60       0.88      0.97      0.92        30\n",
      "          61       0.38      0.33      0.36        30\n",
      "          62       0.90      0.90      0.90        30\n",
      "          63       0.48      0.53      0.51        30\n",
      "          64       0.40      0.30      0.34        20\n",
      "          65       0.38      0.60      0.47        30\n",
      "          66       0.64      0.53      0.58        30\n",
      "          67       0.70      0.77      0.73        30\n",
      "          68       0.71      0.67      0.69        30\n",
      "          69       0.91      1.00      0.95        30\n",
      "          70       0.53      0.53      0.53        30\n",
      "          71       0.48      0.47      0.47        30\n",
      "          72       0.76      0.87      0.81        30\n",
      "          73       0.88      0.77      0.82        30\n",
      "          74       0.96      0.89      0.92        27\n",
      "          75       0.76      0.87      0.81        30\n",
      "          76       0.86      0.63      0.73        30\n",
      "          77       0.51      0.69      0.59        29\n",
      "          78       0.62      0.70      0.66        30\n",
      "          79       0.90      0.60      0.72        30\n",
      "          80       0.80      0.80      0.80        30\n",
      "          81       0.72      0.70      0.71        30\n",
      "          82       1.00      0.83      0.91        30\n",
      "          83       0.82      0.78      0.80        23\n",
      "          84       0.90      0.87      0.88        30\n",
      "          85       0.61      0.73      0.67        30\n",
      "          86       0.86      0.80      0.83        30\n",
      "          87       0.89      0.83      0.86        30\n",
      "          88       0.89      0.80      0.84        30\n",
      "          89       0.90      0.87      0.88        30\n",
      "          90       0.57      0.53      0.55        30\n",
      "          91       0.70      0.77      0.73        30\n",
      "          92       0.93      0.83      0.88        30\n",
      "          93       0.90      0.93      0.92        30\n",
      "          94       0.78      0.83      0.81        30\n",
      "          95       0.70      0.70      0.70        30\n",
      "          96       0.71      0.76      0.73        29\n",
      "          97       0.92      0.73      0.81        30\n",
      "          98       0.81      0.73      0.77        30\n",
      "          99       0.90      0.87      0.88        30\n",
      "         100       0.83      1.00      0.91        20\n",
      "         101       0.35      0.30      0.32        30\n",
      "         102       0.57      0.77      0.66        30\n",
      "         103       0.41      0.57      0.48        30\n",
      "         104       0.56      0.74      0.64        19\n",
      "         105       1.00      0.87      0.93        30\n",
      "         106       0.43      0.43      0.43        30\n",
      "         107       0.94      0.57      0.71        30\n",
      "         108       0.93      0.87      0.90        30\n",
      "         109       0.72      0.87      0.79        30\n",
      "         110       0.69      0.67      0.68        30\n",
      "         111       0.63      0.57      0.60        30\n",
      "         112       0.56      0.75      0.64        20\n",
      "         113       0.83      0.63      0.72        30\n",
      "         114       0.37      0.45      0.41        29\n",
      "         115       0.30      0.37      0.33        30\n",
      "         116       0.48      0.45      0.46        29\n",
      "         117       0.63      0.40      0.49        30\n",
      "         118       0.55      0.41      0.47        29\n",
      "         119       0.86      0.63      0.73        30\n",
      "         120       0.44      0.37      0.40        30\n",
      "         121       0.71      0.73      0.72        30\n",
      "         122       0.83      0.50      0.62        30\n",
      "         123       0.62      0.69      0.66        29\n",
      "         124       0.58      0.66      0.61        29\n",
      "         125       0.58      0.47      0.52        30\n",
      "         126       0.53      0.60      0.56        30\n",
      "         127       0.56      0.67      0.61        30\n",
      "         128       0.69      0.60      0.64        30\n",
      "         129       0.57      0.57      0.57        30\n",
      "         130       0.31      0.33      0.32        30\n",
      "         131       0.71      0.90      0.79        30\n",
      "         132       0.54      0.87      0.67        30\n",
      "         133       0.90      0.93      0.92        30\n",
      "         134       0.39      0.37      0.38        30\n",
      "         135       0.83      0.63      0.72        30\n",
      "         136       0.61      0.67      0.63        30\n",
      "         137       0.81      0.73      0.77        30\n",
      "         138       0.78      0.83      0.81        30\n",
      "         139       0.84      0.87      0.85        30\n",
      "         140       0.59      0.66      0.62        29\n",
      "         141       0.47      0.47      0.47        30\n",
      "         142       0.45      0.50      0.48        30\n",
      "         143       0.35      0.40      0.38        30\n",
      "         144       0.40      0.33      0.36        30\n",
      "         145       0.44      0.37      0.40        30\n",
      "         146       0.62      0.77      0.69        30\n",
      "         147       0.77      0.80      0.79        30\n",
      "         148       0.71      0.76      0.73        29\n",
      "         149       0.70      0.63      0.67        30\n",
      "         150       0.68      0.90      0.78        21\n",
      "         151       0.63      0.63      0.63        30\n",
      "         152       0.64      0.55      0.59        29\n",
      "         153       0.57      0.70      0.63        30\n",
      "         154       0.63      0.63      0.63        30\n",
      "         155       0.49      0.67      0.56        30\n",
      "         156       0.67      0.48      0.56        29\n",
      "         157       0.93      0.83      0.88        30\n",
      "         158       0.80      0.93      0.86        30\n",
      "         159       0.84      0.93      0.89        29\n",
      "         160       0.56      0.73      0.64        30\n",
      "         161       0.86      0.60      0.71        30\n",
      "         162       0.73      0.63      0.68        30\n",
      "         163       0.96      0.83      0.89        30\n",
      "         164       0.88      0.77      0.82        30\n",
      "         165       0.88      0.76      0.81        29\n",
      "         166       0.59      0.63      0.61        30\n",
      "         167       0.81      0.86      0.83        29\n",
      "         168       0.79      0.76      0.77        29\n",
      "         169       0.64      0.60      0.62        30\n",
      "         170       0.78      0.70      0.74        30\n",
      "         171       0.67      0.73      0.70        30\n",
      "         172       0.44      0.50      0.47        30\n",
      "         173       0.54      0.63      0.58        30\n",
      "         174       0.51      0.60      0.55        30\n",
      "         175       0.71      0.83      0.77        30\n",
      "         176       0.96      0.87      0.91        30\n",
      "         177       0.77      0.65      0.71        26\n",
      "         178       0.52      0.41      0.46        29\n",
      "         179       0.60      0.70      0.65        30\n",
      "         180       0.96      0.83      0.89        29\n",
      "         181       0.61      0.83      0.70        30\n",
      "         182       0.59      0.73      0.66        30\n",
      "         183       0.83      0.63      0.72        30\n",
      "         184       0.88      0.77      0.82        30\n",
      "         185       0.74      0.93      0.82        30\n",
      "         186       0.84      0.80      0.82        20\n",
      "         187       0.87      0.90      0.89        30\n",
      "         188       0.81      0.97      0.88        30\n",
      "         189       0.89      0.86      0.88        29\n",
      "         190       0.96      0.80      0.87        30\n",
      "         191       0.85      0.93      0.89        30\n",
      "         192       0.54      0.50      0.52        30\n",
      "         193       0.70      0.70      0.70        30\n",
      "         194       0.78      0.60      0.68        30\n",
      "         195       0.38      0.43      0.41        30\n",
      "         196       0.71      0.57      0.63        30\n",
      "         197       0.73      0.73      0.73        30\n",
      "         198       0.76      0.83      0.79        30\n",
      "         199       0.93      0.93      0.93        30\n",
      "\n",
      "    accuracy                           0.69      5794\n",
      "   macro avg       0.70      0.69      0.69      5794\n",
      "weighted avg       0.70      0.69      0.69      5794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on unseen data\n",
    "if args.dataset == 'CUB200' and official_split:\n",
    "    #load best weights\n",
    "    #model.load_weights(filepath=weights_path+'/model_fine_tune.134-1.2162.hdf5')\n",
    "    model.load_weights(filepath=weights_path+'/model_fine_tune_epoch_150.hdf5')\n",
    "    \n",
    "    #model.evaluate(actual_test_gen,verbose=1)\n",
    "     \n",
    "    pred_probs= model.predict(actual_test_gen,verbose=1)\n",
    "    \n",
    "    pred_classes = np.argmax(pred_probs,1)\n",
    "    #actual_classes = np.argmax(test_gen.classes,1)\n",
    "    actual_classes = actual_test_gen.classes\n",
    "    print(confusion_matrix(actual_classes,pred_classes))\n",
    "    print(classification_report(actual_classes,pred_classes)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(actual_test_gen,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-02T09:01:05.954641Z",
     "iopub.status.busy": "2020-12-02T09:01:05.951393Z",
     "iopub.status.idle": "2020-12-02T09:01:05.958062Z",
     "shell.execute_reply": "2020-12-02T09:01:05.957242Z"
    },
    "papermill": {
     "duration": 7.555548,
     "end_time": "2020-12-02T09:01:05.958208",
     "exception": false,
     "start_time": "2020-12-02T09:00:58.402660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('./trained_weights'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "duration": 11667.954103,
   "end_time": "2020-12-02T09:01:14.885863",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-02T05:46:46.931760",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
